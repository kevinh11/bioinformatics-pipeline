{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2699dea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnrrd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ecat, imagestats, mriutils, orientations, streamlines, viewers\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nifti1 \u001b[38;5;28;01mas\u001b[39;00m ni1\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\imagestats.py:13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Functions for computing image statistics\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimageclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spatial_axes_first\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_nonzero_voxels\u001b[39m(img):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Count number of non-zero voxels\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\imageclasses.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyzeImage\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrikhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AFNIImage\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Image\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfreesurfer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MGHImage\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgifti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GiftiImage\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\cifti2\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# vi: set ft=python sts=4 ts=4 sw=4 et:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"CIFTI-2 format IO\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m.. currentmodule:: nibabel.cifti2\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m   cifti2_axes\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     CIFTI_BRAIN_STRUCTURES,\n\u001b[0;32m     22\u001b[0m     CIFTI_MODEL_TYPES,\n\u001b[0;32m     23\u001b[0m     Cifti2BrainModel,\n\u001b[0;32m     24\u001b[0m     Cifti2Header,\n\u001b[0;32m     25\u001b[0m     Cifti2HeaderError,\n\u001b[0;32m     26\u001b[0m     Cifti2Image,\n\u001b[0;32m     27\u001b[0m     Cifti2Label,\n\u001b[0;32m     28\u001b[0m     Cifti2LabelTable,\n\u001b[0;32m     29\u001b[0m     Cifti2Matrix,\n\u001b[0;32m     30\u001b[0m     Cifti2MatrixIndicesMap,\n\u001b[0;32m     31\u001b[0m     Cifti2MetaData,\n\u001b[0;32m     32\u001b[0m     Cifti2NamedMap,\n\u001b[0;32m     33\u001b[0m     Cifti2Parcel,\n\u001b[0;32m     34\u001b[0m     Cifti2Surface,\n\u001b[0;32m     35\u001b[0m     Cifti2TransformationMatrixVoxelIndicesIJKtoXYZ,\n\u001b[0;32m     36\u001b[0m     Cifti2VertexIndices,\n\u001b[0;32m     37\u001b[0m     Cifti2Vertices,\n\u001b[0;32m     38\u001b[0m     Cifti2Volume,\n\u001b[0;32m     39\u001b[0m     Cifti2VoxelIndicesIJK,\n\u001b[0;32m     40\u001b[0m     load,\n\u001b[0;32m     41\u001b[0m     save,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axis, BrainModelAxis, LabelAxis, ParcelsAxis, ScalarAxis, SeriesAxis\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_cifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Extension\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\cifti2\\cifti2.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataobj_images\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataobjImage\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilebasedimages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileBasedHeader, SerializableImage\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti1Extensions\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti2Header, Nifti2Image\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Recoder, make_dt_codes\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\nifti1.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m     DicomDataset \u001b[38;5;241m=\u001b[39m pdcm\u001b[38;5;241m.\u001b[39mDataset\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     pdcm, have_dicom, _ \u001b[38;5;241m=\u001b[39m \u001b[43moptional_package\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpydicom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m have_dicom:\n\u001b[0;32m     50\u001b[0m         DicomDataset \u001b[38;5;241m=\u001b[39m pdcm\u001b[38;5;241m.\u001b[39mDataset\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\optpkg.py:104\u001b[0m, in \u001b[0;36moptional_package\u001b[1;34m(name, trip_msg, min_version)\u001b[0m\n\u001b[0;32m    102\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfromlist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc_:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Could fail due to some ImportError or for some other reason\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# e.g. h5py might have been checking file system to support UTF-8\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# etc.  We should not blow if they blow\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     exc \u001b[38;5;241m=\u001b[39m exc_  \u001b[38;5;66;03m# So it is accessible outside of the code block\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\__init__.py:31\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2018 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"pydicom package -- easily handle DICOM files.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m   See Quick Start below.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataelem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataElement\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, FileDataset, FileMetaDataset\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\dataelem.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, TYPE_CHECKING, NamedTuple\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config  \u001b[38;5;66;03m# don't import datetime_conversion directly\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatadict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     dictionary_has_tag,\n\u001b[0;32m     21\u001b[0m     dictionary_description,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     repeater_has_tag,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\config.py:407\u001b[0m\n\u001b[0;32m    398\u001b[0m show_file_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03mIf ``True`` (default), the 'str' and 'repr' methods\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03mof :class:`~pydicom.dataset.Dataset` begin with a separate section\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m.. versionadded:: 2.0\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrle_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrle_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpillow_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpillow_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixel_data_handlers\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn_and_log\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     apply_color_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_color_lut,\n\u001b[0;32m      8\u001b[0m     apply_modality_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_modality_lut,\n\u001b[0;32m      9\u001b[0m     apply_voi_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_voi_lut,\n\u001b[0;32m     10\u001b[0m     apply_voi \u001b[38;5;28;01mas\u001b[39;00m _apply_voi,\n\u001b[0;32m     11\u001b[0m     apply_windowing \u001b[38;5;28;01mas\u001b[39;00m _apply_windowing,\n\u001b[0;32m     12\u001b[0m     convert_color_space \u001b[38;5;28;01mas\u001b[39;00m _convert_color_space,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     expand_ybr422 \u001b[38;5;28;01mas\u001b[39;00m _expand_ybr422,\n\u001b[0;32m     16\u001b[0m     pack_bits \u001b[38;5;28;01mas\u001b[39;00m _pack_bits,\n\u001b[0;32m     17\u001b[0m     unpack_bits \u001b[38;5;28;01mas\u001b[39;00m _unpack_bits,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     21\u001b[0m _DEPRECATED \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_color_lut\u001b[39m\u001b[38;5;124m\"\u001b[39m: _apply_color_lut,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_modality_lut\u001b[39m\u001b[38;5;124m\"\u001b[39m: _apply_modality_lut,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munpack_bits\u001b[39m\u001b[38;5;124m\"\u001b[39m: _unpack_bits,\n\u001b[0;32m     31\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2024 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_decoder\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_encoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     apply_color_lut,\n\u001b[0;32m      7\u001b[0m     apply_icc_profile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     create_icc_transform,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     as_pixel_options,\n\u001b[0;32m     19\u001b[0m     compress,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     unpack_bits,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\encoders\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2024 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     RLELosslessEncoder,\n\u001b[0;32m      5\u001b[0m     JPEGLSLosslessEncoder,\n\u001b[0;32m      6\u001b[0m     JPEGLSNearLosslessEncoder,\n\u001b[0;32m      7\u001b[0m     JPEG2000LosslessEncoder,\n\u001b[0;32m      8\u001b[0m     JPEG2000Encoder,\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\encoders\\base.py:798\u001b[0m\n\u001b[0;32m    793\u001b[0m JPEG2000LosslessEncoder\u001b[38;5;241m.\u001b[39madd_plugin(\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpylibjpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpydicom.pixels.encoders.pylibjpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_encode_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    795\u001b[0m )\n\u001b[0;32m    797\u001b[0m JPEG2000Encoder \u001b[38;5;241m=\u001b[39m Encoder(JPEG2000)\n\u001b[1;32m--> 798\u001b[0m \u001b[43mJPEG2000Encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_plugin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpylibjpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpydicom.pixels.encoders.pylibjpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_encode_frame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Available pixel data encoders\u001b[39;00m\n\u001b[0;32m    804\u001b[0m _PIXEL_DATA_ENCODERS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;66;03m# UID: (encoder, 'versionadded')\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     RLELossless: (RLELosslessEncoder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    810\u001b[0m     JPEG2000: (JPEG2000Encoder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    811\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\common.py:86\u001b[0m, in \u001b[0;36mCoderBase.add_plugin\u001b[1;34m(self, label, import_path)\u001b[0m\n\u001b[0;32m     83\u001b[0m module \u001b[38;5;241m=\u001b[39m import_module(import_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# `is_available(UID)` is required for plugins\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUID\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_available[label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, import_path[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# `DE/ENCODER_DEPENDENCIES[UID]` is required for plugins\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\encoders\\pylibjpeg.py:32\u001b[0m, in \u001b[0;36mis_available\u001b[1;34m(uid)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_available\u001b[39m(uid: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return ``True`` if a pixel data encoder for `uid` is available for use,\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    ``False`` otherwise.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_passes_version_check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpylibjpeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m _OPENJPEG_SYNTAXES:\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\utils.py:1274\u001b[0m, in \u001b[0;36m_passes_version_check\u001b[1;34m(package_name, minimum_version)\u001b[0m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if `package_name` is available and its version is greater or\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03mequal to `minimum_version`\u001b[39;00m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1274\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__version__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m minimum_version\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1532\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1506\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1605\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nrrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_dir = os.path.join(os.path.dirname(os.getcwd()),\"data\", \"clinical_df_cleaned.csv\")\n",
    "clin_df= pd.read_csv(clin_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db8a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TCIA_ID</th>\n",
       "      <th>Interval_BL</th>\n",
       "      <th>age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>fhx_can</th>\n",
       "      <th>fhx_livc</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>Pathology_Moderately differentiated</th>\n",
       "      <th>Pathology_NOT STATED</th>\n",
       "      <th>Pathology_No biopsy</th>\n",
       "      <th>Pathology_Poorly differentiated</th>\n",
       "      <th>Pathology_Well differentiated</th>\n",
       "      <th>HCV</th>\n",
       "      <th>HBV</th>\n",
       "      <th>Child Pugh A</th>\n",
       "      <th>multinodular</th>\n",
       "      <th>T_involvment &gt; 50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HCC_001</td>\n",
       "      <td>-18</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HCC_002</td>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HCC_003</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HCC_004</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HCC_005</td>\n",
       "      <td>435</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>100</td>\n",
       "      <td>HCC_101</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>101</td>\n",
       "      <td>HCC_102</td>\n",
       "      <td>27</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>102</td>\n",
       "      <td>HCC_103</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>103</td>\n",
       "      <td>HCC_104</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>104</td>\n",
       "      <td>HCC_105</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  TCIA_ID  Interval_BL  age  Sex  Smoking  Alcohol  fhx_can  \\\n",
       "0            0  HCC_001          -18   71    0        1        1        0   \n",
       "1            1  HCC_002           84   72    0        1        1        0   \n",
       "2            2  HCC_003            3   53    0        1        1        1   \n",
       "3            3  HCC_004            1   80    0        0        0        1   \n",
       "4            4  HCC_005          435   73    0        1        1        1   \n",
       "..         ...      ...          ...  ...  ...      ...      ...      ...   \n",
       "94         100  HCC_101           -6   57    0        1        1        0   \n",
       "95         101  HCC_102           27   86    0        0        0        1   \n",
       "96         102  HCC_103            1   79    0        1        1        1   \n",
       "97         103  HCC_104            9   66    0        1        1        1   \n",
       "98         104  HCC_105            6   58    0        1        1        0   \n",
       "\n",
       "    fhx_livc  Diabetes  ...  Pathology_Moderately differentiated  \\\n",
       "0          0         0  ...                                  1.0   \n",
       "1          0         0  ...                                  0.0   \n",
       "2          0         0  ...                                  1.0   \n",
       "3          0         0  ...                                  1.0   \n",
       "4          0         0  ...                                  1.0   \n",
       "..       ...       ...  ...                                  ...   \n",
       "94         0         0  ...                                  0.0   \n",
       "95         0         0  ...                                  1.0   \n",
       "96         0         1  ...                                  0.0   \n",
       "97         0         1  ...                                  0.0   \n",
       "98         0         1  ...                                  0.0   \n",
       "\n",
       "    Pathology_NOT STATED  Pathology_No biopsy  \\\n",
       "0                    0.0                  0.0   \n",
       "1                    0.0                  0.0   \n",
       "2                    0.0                  0.0   \n",
       "3                    0.0                  0.0   \n",
       "4                    0.0                  0.0   \n",
       "..                   ...                  ...   \n",
       "94                   0.0                  0.0   \n",
       "95                   0.0                  0.0   \n",
       "96                   0.0                  0.0   \n",
       "97                   0.0                  0.0   \n",
       "98                   0.0                  0.0   \n",
       "\n",
       "    Pathology_Poorly differentiated  Pathology_Well differentiated  HCV  HBV  \\\n",
       "0                               0.0                            0.0    1    0   \n",
       "1                               0.0                            1.0    0    1   \n",
       "2                               0.0                            0.0    1    0   \n",
       "3                               0.0                            0.0    0    0   \n",
       "4                               0.0                            0.0    0    0   \n",
       "..                              ...                            ...  ...  ...   \n",
       "94                              0.0                            1.0    1    1   \n",
       "95                              0.0                            0.0    1    0   \n",
       "96                              0.0                            1.0    0    0   \n",
       "97                              0.0                            1.0    1    0   \n",
       "98                              0.0                            1.0    0    0   \n",
       "\n",
       "    Child Pugh A  multinodular  T_involvment > 50%  \n",
       "0              1             1                   0  \n",
       "1              1             1                   0  \n",
       "2              1             1                   1  \n",
       "3              1             1                   0  \n",
       "4              1             1                   0  \n",
       "..           ...           ...                 ...  \n",
       "94             1             0                   0  \n",
       "95             1             0                   0  \n",
       "96             1             1                   0  \n",
       "97             1             1                   0  \n",
       "98             1             1                   0  \n",
       "\n",
       "[95 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = pd.read_csv(\"radiomics_df_veryfinal.csv\")\n",
    "\n",
    "drop_cases = (set(clin_df[\"TCIA_ID\"].unique())).difference(set(test2[\"TCIA_ID\"].unique()))\n",
    "clin_df = clin_df[clin_df[\"TCIA_ID\"].isin(drop_cases) == False]\n",
    "\n",
    "clin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "def is_train_test_difference_significant(x_train, x_test, alpha=0.05):\n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    \n",
    "    # Flatten arrays if they are multi-dimensional\n",
    "    if x_train.ndim > 1:\n",
    "        x_train = x_train.ravel()\n",
    "    if x_test.ndim > 1:\n",
    "        x_test = x_test.ravel()\n",
    "    \n",
    "    # Perform the Wilcoxon rank-sum test\n",
    "    stat, p_value = ranksums(x_train, x_test)\n",
    "    \n",
    "    # Check if the difference is statistically significant\n",
    "    is_significant = p_value < alpha\n",
    "\n",
    "    \n",
    "    \n",
    "    return is_significant, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047afca",
   "metadata": {},
   "source": [
    "A/B Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab7af9",
   "metadata": {},
   "source": [
    "combine endpoint and radiomic for initial test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_patients(\n",
    "    dataframe: pd.DataFrame, \n",
    "    identifier: str, \n",
    "    endpoint: str, \n",
    "    test_ratio: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    # Get unique patient IDs\n",
    "    unique_patients = dataframe[identifier].unique()\n",
    "    \n",
    "    # Split patients into train and test\n",
    "    train_patients, test_patients = train_test_split(\n",
    "        unique_patients, \n",
    "        test_size=test_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=dataframe[endpoint]  # Optional: maintain class balance\n",
    "    )\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = dataframe[identifier].isin(train_patients)\n",
    "    test_mask = dataframe[identifier].isin(test_patients)\n",
    "    \n",
    "    # Split the data\n",
    "    x_train = dataframe[train_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    x_test = dataframe[test_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    y_train = dataframe[train_mask][endpoint]\n",
    "    y_test = dataframe[test_mask][endpoint]\n",
    "\n",
    " \n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute common classification metrics for binary classification.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "    - Accuracy\n",
    "    - F1 Score\n",
    "    - Precision\n",
    "    - Sensitivity (Recall)\n",
    "    - Specificity\n",
    "    - Confusion Matrix\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Sensitivity (Recall)': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Confusion Matrix': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7b0d5",
   "metadata": {},
   "source": [
    "Standardization of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MissingValueColumnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        self.keep_features_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        missing_frac = X.isna().mean()\n",
    "        self.keep_features_ = missing_frac[missing_frac <= self.threshold].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        # Only keep columns selected during fit\n",
    "        return X[self.keep_features_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e01e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CleanFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_thresh=0.3, variance_thresh=1e-6):\n",
    "        self.missing_thresh = missing_thresh\n",
    "        self.variance_thresh = variance_thresh\n",
    "        self.keep_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # print(\"from clean feature selector : X\")\n",
    "        # print(X)\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        # Drop by missing %\n",
    "        keep_missing = X.isna().mean() < self.missing_thresh\n",
    "        X2 = X.loc[:, keep_missing]\n",
    "\n",
    "        # Drop by variance\n",
    "        var = X2.var()\n",
    "        keep_var = var > self.variance_thresh\n",
    "\n",
    "        self.keep_cols_ = X2.columns[keep_var].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        print(X[self.keep_cols_])\n",
    "        return X[self.keep_cols_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "\n",
    "class FeatureFilterer:\n",
    "    def __init__(self, responder_idx, nonresponder_idx, filter_method_name = \"mannwhitney\", p_val_threshold=0.05):\n",
    "        self.p_val_threshold = p_val_threshold\n",
    "        self.responder_idx = responder_idx\n",
    "        self.nonresponder_idx = nonresponder_idx\n",
    "        self.filter_method_name = filter_method_name\n",
    "        self.kept_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # X is already a DataFrame here if CleanFeatureSelector keeps it that way\n",
    "        X_df = pd.DataFrame(X, index=self.responder_idx.union(self.nonresponder_idx))\n",
    "\n",
    "        x_responder = X_df.loc[self.responder_idx]\n",
    "        x_nonresponder = X_df.loc[self.nonresponder_idx]\n",
    "\n",
    "        self.significant_features = []\n",
    "\n",
    "        statistical_test = None\n",
    "\n",
    "        if self.filter_method_name == \"mannwhitney\":\n",
    "            statistical_test = mannwhitneyu\n",
    "\n",
    "        elif self.filter_method_name == \"kruskal\":\n",
    "            statistical_test = kruskal\n",
    "        \n",
    "        elif self.filter_method_name == \"wilcoxon\":\n",
    "            statistical_test = kruskal\n",
    "\n",
    "        for col in X_df.columns:\n",
    "            try:\n",
    "                _, p = statistical_test(\n",
    "                    x_responder[col].dropna(),\n",
    "                    x_nonresponder[col].dropna()\n",
    "                )\n",
    "                if p < self.p_val_threshold:\n",
    "                    self.significant_features.append(col)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        print(self.significant_features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        return X_df[self.significant_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # record input column names\n",
    "        self.columns_ = X.columns if hasattr(X, \"columns\") else self.columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.columns_, index=getattr(X, \"index\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Interval_BL  age  Sex  Smoking  Alcohol  fhx_can  fhx_livc  Diabetes  \\\n",
      "0           -18   71    0        1        1        0         0         0   \n",
      "2             3   53    0        1        1        1         0         0   \n",
      "4           435   73    0        1        1        1         0         0   \n",
      "6          -148   71    0        1        1        1         0         0   \n",
      "7             0   68    0        0        1        1         0         0   \n",
      "8            -8   59    0        0        0        1         0         0   \n",
      "9            -1   46    1        1        1        0         0         0   \n",
      "12            1   61    0        0        0        1         0         1   \n",
      "13           -7   68    1        1        1        0         0         0   \n",
      "15            1   49    0        0        1        0         0         1   \n",
      "16           -2   79    1        0        0        0         0         1   \n",
      "17            2   50    0        1        1        1         0         1   \n",
      "19            0   72    0        1        1        0         0         0   \n",
      "20            2   64    0        1        1        1         0         0   \n",
      "22           96   70    0        1        1        1         0         0   \n",
      "24          225   73    1        0        0        0         0         0   \n",
      "25           35   75    1        0        0        1         0         1   \n",
      "27           34   49    0        0        1        0         0         0   \n",
      "28            1   77    0        0        0        1         0         1   \n",
      "32            1   80    1        1        0        1         0         1   \n",
      "35            3   82    0        1        1        1         0         1   \n",
      "36           -8   71    0        1        1        1         1         1   \n",
      "37           50   76    1        0        0        1         0         0   \n",
      "38          -13   76    1        1        1        1         0         0   \n",
      "39          -15   49    0        1        1        0         0         0   \n",
      "42           28   57    0        0        0        1         1         1   \n",
      "43           27   64    0        1        1        1         0         0   \n",
      "44            0   88    0        1        1        0         0         1   \n",
      "47           28   76    1        0        0        0         0         0   \n",
      "48            5   63    1        1        1        1         0         1   \n",
      "50          -21   54    1        1        1        1         0         0   \n",
      "51           -3   80    0        0        1        1         0         0   \n",
      "54           11   81    0        0        0        1         0         0   \n",
      "55            0   83    0        0        1        0         0         0   \n",
      "57            7   40    0        1        1        1         0         0   \n",
      "60           33   54    0        1        1        0         0         0   \n",
      "62           -7   60    1        0        0        1         0         0   \n",
      "63            7   59    1        0        1        1         0         0   \n",
      "66           10   77    0        1        1        1         0         0   \n",
      "67           40   68    1        0        0        1         0         1   \n",
      "70           19   67    1        1        0        1         0         0   \n",
      "71            0   93    1        0        0        1         1         0   \n",
      "72           13   81    0        1        0        0         0         0   \n",
      "73           21   72    0        1        1        1         0         1   \n",
      "75           14   76    0        1        0        0         0         1   \n",
      "76           11   73    1        1        1        0         0         0   \n",
      "79            1   72    1        1        1        1         0         1   \n",
      "82            5   81    1        1        0        0         0         1   \n",
      "84           13   63    1        0        0        1         1         1   \n",
      "86           -5   70    1        1        1        1         0         0   \n",
      "87           14   75    0        0        0        0         0         0   \n",
      "90            8   59    0        1        1        0         0         0   \n",
      "92           28   74    0        1        0        0         0         0   \n",
      "93            2   50    0        0        0        0         0         0   \n",
      "94           -6   57    0        1        1        0         0         0   \n",
      "95           27   86    0        0        0        1         0         0   \n",
      "96            1   79    0        1        1        1         0         1   \n",
      "\n",
      "    Personal history of cancer  Evidence_of_cirh  ...  lesion_size_missing  \\\n",
      "0                            1                 1  ...                    0   \n",
      "2                            0                 1  ...                    1   \n",
      "4                            0                 1  ...                    1   \n",
      "6                            0                 1  ...                    0   \n",
      "7                            0                 1  ...                    1   \n",
      "8                            0                 0  ...                    1   \n",
      "9                            0                 1  ...                    1   \n",
      "12                           1                 1  ...                    0   \n",
      "13                           0                 1  ...                    1   \n",
      "15                           0                 1  ...                    1   \n",
      "16                           0                 1  ...                    0   \n",
      "17                           0                 1  ...                    1   \n",
      "19                           0                 1  ...                    1   \n",
      "20                           0                 1  ...                    1   \n",
      "22                           0                 1  ...                    0   \n",
      "24                           0                 1  ...                    1   \n",
      "25                           0                 0  ...                    1   \n",
      "27                           0                 0  ...                    1   \n",
      "28                           1                 1  ...                    0   \n",
      "32                           0                 0  ...                    0   \n",
      "35                           1                 1  ...                    0   \n",
      "36                           0                 1  ...                    0   \n",
      "37                           0                 1  ...                    0   \n",
      "38                           0                 0  ...                    1   \n",
      "39                           0                 1  ...                    0   \n",
      "42                           0                 1  ...                    0   \n",
      "43                           1                 0  ...                    1   \n",
      "44                           1                 0  ...                    1   \n",
      "47                           0                 0  ...                    0   \n",
      "48                           0                 1  ...                    0   \n",
      "50                           0                 1  ...                    1   \n",
      "51                           0                 1  ...                    0   \n",
      "54                           1                 0  ...                    1   \n",
      "55                           0                 1  ...                    0   \n",
      "57                           0                 1  ...                    1   \n",
      "60                           0                 1  ...                    1   \n",
      "62                           0                 1  ...                    0   \n",
      "63                           0                 1  ...                    1   \n",
      "66                           0                 0  ...                    1   \n",
      "67                           1                 0  ...                    0   \n",
      "70                           0                 0  ...                    1   \n",
      "71                           0                 1  ...                    0   \n",
      "72                           0                 1  ...                    1   \n",
      "73                           1                 1  ...                    0   \n",
      "75                           0                 0  ...                    1   \n",
      "76                           0                 0  ...                    1   \n",
      "79                           1                 1  ...                    1   \n",
      "82                           0                 0  ...                    1   \n",
      "84                           1                 1  ...                    1   \n",
      "86                           0                 1  ...                    0   \n",
      "87                           0                 0  ...                    0   \n",
      "90                           0                 1  ...                    0   \n",
      "92                           0                 0  ...                    0   \n",
      "93                           0                 1  ...                    0   \n",
      "94                           0                 1  ...                    0   \n",
      "95                           0                 1  ...                    1   \n",
      "96                           0                 1  ...                    0   \n",
      "\n",
      "    Pathology_Moderately differentiated  Pathology_NOT STATED  \\\n",
      "0                                   1.0                   0.0   \n",
      "2                                   1.0                   0.0   \n",
      "4                                   1.0                   0.0   \n",
      "6                                   0.0                   0.0   \n",
      "7                                   0.0                   0.0   \n",
      "8                                   0.0                   0.0   \n",
      "9                                   1.0                   0.0   \n",
      "12                                  1.0                   0.0   \n",
      "13                                  0.0                   0.0   \n",
      "15                                  1.0                   0.0   \n",
      "16                                  0.0                   0.0   \n",
      "17                                  0.0                   1.0   \n",
      "19                                  0.0                   0.0   \n",
      "20                                  1.0                   0.0   \n",
      "22                                  0.0                   0.0   \n",
      "24                                  0.0                   0.0   \n",
      "25                                  0.0                   0.0   \n",
      "27                                  1.0                   0.0   \n",
      "28                                  1.0                   0.0   \n",
      "32                                  0.0                   0.0   \n",
      "35                                  0.0                   0.0   \n",
      "36                                  0.0                   1.0   \n",
      "37                                  0.0                   0.0   \n",
      "38                                  0.0                   1.0   \n",
      "39                                  1.0                   0.0   \n",
      "42                                  0.0                   0.0   \n",
      "43                                  1.0                   0.0   \n",
      "44                                  1.0                   0.0   \n",
      "47                                  0.0                   0.0   \n",
      "48                                  0.0                   0.0   \n",
      "50                                  0.0                   1.0   \n",
      "51                                  0.0                   0.0   \n",
      "54                                  0.0                   0.0   \n",
      "55                                  0.0                   0.0   \n",
      "57                                  1.0                   0.0   \n",
      "60                                  0.0                   0.0   \n",
      "62                                  1.0                   0.0   \n",
      "63                                  1.0                   0.0   \n",
      "66                                  0.0                   0.0   \n",
      "67                                  0.0                   0.0   \n",
      "70                                  0.0                   0.0   \n",
      "71                                  1.0                   0.0   \n",
      "72                                  0.0                   0.0   \n",
      "73                                  0.0                   0.0   \n",
      "75                                  0.0                   1.0   \n",
      "76                                  0.0                   0.0   \n",
      "79                                  1.0                   0.0   \n",
      "82                                  1.0                   0.0   \n",
      "84                                  0.0                   0.0   \n",
      "86                                  0.0                   0.0   \n",
      "87                                  0.0                   1.0   \n",
      "90                                  1.0                   0.0   \n",
      "92                                  0.0                   0.0   \n",
      "93                                  0.0                   0.0   \n",
      "94                                  0.0                   0.0   \n",
      "95                                  1.0                   0.0   \n",
      "96                                  0.0                   0.0   \n",
      "\n",
      "    Pathology_Poorly differentiated  Pathology_Well differentiated  HCV  HBV  \\\n",
      "0                               0.0                            0.0    1    0   \n",
      "2                               0.0                            0.0    1    0   \n",
      "4                               0.0                            0.0    0    0   \n",
      "6                               0.0                            1.0    1    0   \n",
      "7                               0.0                            1.0    0    0   \n",
      "8                               1.0                            0.0    0    0   \n",
      "9                               0.0                            0.0    1    0   \n",
      "12                              0.0                            0.0    0    0   \n",
      "13                              0.0                            1.0    0    0   \n",
      "15                              0.0                            0.0    1    0   \n",
      "16                              0.0                            1.0    0    0   \n",
      "17                              0.0                            0.0    1    0   \n",
      "19                              1.0                            0.0    0    0   \n",
      "20                              0.0                            0.0    0    1   \n",
      "22                              1.0                            0.0    1    1   \n",
      "24                              0.0                            1.0    1    1   \n",
      "25                              1.0                            0.0    0    0   \n",
      "27                              0.0                            0.0    0    1   \n",
      "28                              0.0                            0.0    0    0   \n",
      "32                              1.0                            0.0    1    0   \n",
      "35                              0.0                            1.0    0    0   \n",
      "36                              0.0                            0.0    0    0   \n",
      "37                              0.0                            1.0    1    0   \n",
      "38                              0.0                            0.0    0    0   \n",
      "39                              0.0                            0.0    1    0   \n",
      "42                              0.0                            1.0    0    1   \n",
      "43                              0.0                            0.0    0    0   \n",
      "44                              0.0                            0.0    0    0   \n",
      "47                              0.0                            1.0    0    0   \n",
      "48                              0.0                            1.0    0    0   \n",
      "50                              0.0                            0.0    1    1   \n",
      "51                              0.0                            1.0    1    0   \n",
      "54                              0.0                            1.0    0    0   \n",
      "55                              0.0                            1.0    0    0   \n",
      "57                              0.0                            0.0    0    1   \n",
      "60                              1.0                            0.0    1    0   \n",
      "62                              0.0                            0.0    1    0   \n",
      "63                              0.0                            0.0    0    0   \n",
      "66                              0.0                            1.0    0    0   \n",
      "67                              0.0                            1.0    0    0   \n",
      "70                              1.0                            0.0    0    0   \n",
      "71                              0.0                            0.0    1    1   \n",
      "72                              0.0                            1.0    1    0   \n",
      "73                              0.0                            1.0    1    0   \n",
      "75                              0.0                            0.0    0    0   \n",
      "76                              1.0                            0.0    0    0   \n",
      "79                              0.0                            0.0    0    0   \n",
      "82                              0.0                            0.0    0    0   \n",
      "84                              0.0                            1.0    0    1   \n",
      "86                              1.0                            0.0    0    0   \n",
      "87                              0.0                            0.0    1    0   \n",
      "90                              0.0                            0.0    1    0   \n",
      "92                              0.0                            1.0    0    0   \n",
      "93                              1.0                            0.0    0    1   \n",
      "94                              0.0                            1.0    1    1   \n",
      "95                              0.0                            0.0    1    0   \n",
      "96                              0.0                            1.0    0    0   \n",
      "\n",
      "    Child Pugh A  multinodular  T_involvment > 50%  \n",
      "0              1             1                   0  \n",
      "2              1             1                   1  \n",
      "4              1             1                   0  \n",
      "6              1             1                   0  \n",
      "7              0             0                   1  \n",
      "8              0             1                   1  \n",
      "9              0             0                   0  \n",
      "12             1             1                   0  \n",
      "13             0             0                   0  \n",
      "15             0             0                   0  \n",
      "16             0             1                   0  \n",
      "17             0             1                   0  \n",
      "19             0             0                   0  \n",
      "20             1             1                   0  \n",
      "22             0             1                   0  \n",
      "24             1             0                   0  \n",
      "25             1             0                   0  \n",
      "27             1             0                   1  \n",
      "28             0             1                   1  \n",
      "32             1             1                   0  \n",
      "35             0             1                   1  \n",
      "36             1             1                   0  \n",
      "37             1             0                   0  \n",
      "38             1             0                   0  \n",
      "39             1             1                   1  \n",
      "42             1             1                   0  \n",
      "43             1             1                   1  \n",
      "44             1             0                   0  \n",
      "47             1             1                   0  \n",
      "48             1             1                   0  \n",
      "50             1             1                   0  \n",
      "51             1             0                   0  \n",
      "54             1             0                   1  \n",
      "55             1             0                   0  \n",
      "57             1             0                   0  \n",
      "60             1             1                   0  \n",
      "62             1             0                   1  \n",
      "63             1             0                   0  \n",
      "66             1             0                   1  \n",
      "67             1             1                   0  \n",
      "70             1             1                   1  \n",
      "71             1             1                   0  \n",
      "72             1             0                   0  \n",
      "73             1             0                   0  \n",
      "75             1             0                   0  \n",
      "76             1             0                   0  \n",
      "79             1             0                   0  \n",
      "82             0             0                   0  \n",
      "84             1             0                   0  \n",
      "86             1             1                   0  \n",
      "87             1             1                   0  \n",
      "90             1             1                   0  \n",
      "92             1             1                   1  \n",
      "93             1             0                   0  \n",
      "94             1             0                   0  \n",
      "95             1             0                   0  \n",
      "96             1             1                   0  \n",
      "\n",
      "[57 rows x 28 columns]\n",
      "    Interval_BL  age  Sex  Smoking  Alcohol  fhx_can  fhx_livc  Diabetes  \\\n",
      "1            84   72    0        1        1        0         0         0   \n",
      "3             1   80    0        0        0        1         0         0   \n",
      "5            33   54    0        0        0        1         0         0   \n",
      "10           -3   77    0        1        1        0         0         0   \n",
      "11            3   73    0        0        0        1         0         1   \n",
      "14           29   85    1        0        0        1         0         1   \n",
      "18            2   78    0        1        0        1         0         0   \n",
      "21           19   58    0        1        0        0         0         0   \n",
      "23           15   70    0        1        0        0         0         1   \n",
      "26           67   58    0        1        1        1         0         0   \n",
      "29           33   64    0        0        0        1         0         0   \n",
      "30           39   72    1        0        0        1         0         1   \n",
      "31            8   82    1        0        0        1         0         1   \n",
      "33           32   75    0        1        1        0         0         1   \n",
      "34           32   68    1        0        0        0         0         0   \n",
      "40          105   73    0        0        1        1         0         1   \n",
      "41            3   72    0        1        0        0         0         0   \n",
      "45           13   50    0        0        0        0         0         0   \n",
      "46           15   65    1        0        0        1         0         0   \n",
      "49            5   56    0        1        1        0         0         0   \n",
      "52           -1   81    0        1        0        1         0         1   \n",
      "53            4   68    1        1        0        1         0         1   \n",
      "56          868   64    1        0        0        1         0         1   \n",
      "58         -193   71    1        1        0        1         1         0   \n",
      "59           -2   70    1        0        1        0         0         0   \n",
      "64           69   68    0        1        1        0         1         0   \n",
      "65            3   54    0        0        1        1         0         1   \n",
      "68           28   69    1        0        0        1         0         0   \n",
      "69           37   56    1        0        0        0         1         0   \n",
      "74            6   76    0        1        1        0         0         1   \n",
      "78         -303   81    1        0        1        1         0         0   \n",
      "80           13   86    0        1        1        0         0         0   \n",
      "81          -15   66    0        1        0        1         0         1   \n",
      "85            0   72    0        1        1        1         0         0   \n",
      "89            4   63    0        1        0        0         0         0   \n",
      "91            0   55    1        1        1        1         0         0   \n",
      "97            9   66    0        1        1        1         0         1   \n",
      "98            6   58    0        1        1        0         0         1   \n",
      "\n",
      "    Personal history of cancer  Evidence_of_cirh  ...  lesion_size_missing  \\\n",
      "1                            0                 1  ...                    1   \n",
      "3                            0                 1  ...                    0   \n",
      "5                            1                 1  ...                    1   \n",
      "10                           0                 1  ...                    0   \n",
      "11                           1                 1  ...                    0   \n",
      "14                           0                 1  ...                    0   \n",
      "18                           1                 1  ...                    1   \n",
      "21                           0                 1  ...                    0   \n",
      "23                           0                 0  ...                    0   \n",
      "26                           0                 1  ...                    0   \n",
      "29                           0                 1  ...                    0   \n",
      "30                           0                 0  ...                    1   \n",
      "31                           0                 0  ...                    1   \n",
      "33                           1                 0  ...                    1   \n",
      "34                           0                 0  ...                    0   \n",
      "40                           0                 1  ...                    0   \n",
      "41                           0                 1  ...                    0   \n",
      "45                           0                 1  ...                    0   \n",
      "46                           0                 0  ...                    1   \n",
      "49                           0                 1  ...                    1   \n",
      "52                           0                 1  ...                    1   \n",
      "53                           1                 0  ...                    0   \n",
      "56                           0                 1  ...                    1   \n",
      "58                           0                 1  ...                    0   \n",
      "59                           0                 1  ...                    1   \n",
      "64                           0                 1  ...                    1   \n",
      "65                           0                 1  ...                    0   \n",
      "68                           0                 1  ...                    1   \n",
      "69                           0                 1  ...                    1   \n",
      "74                           0                 0  ...                    0   \n",
      "78                           1                 1  ...                    1   \n",
      "80                           0                 1  ...                    0   \n",
      "81                           0                 1  ...                    1   \n",
      "85                           1                 1  ...                    0   \n",
      "89                           0                 0  ...                    1   \n",
      "91                           1                 1  ...                    1   \n",
      "97                           0                 1  ...                    0   \n",
      "98                           0                 1  ...                    0   \n",
      "\n",
      "    Pathology_Moderately differentiated  Pathology_NOT STATED  \\\n",
      "1                                   0.0                   0.0   \n",
      "3                                   1.0                   0.0   \n",
      "5                                   0.0                   0.0   \n",
      "10                                  0.0                   0.0   \n",
      "11                                  0.0                   1.0   \n",
      "14                                  0.0                   1.0   \n",
      "18                                  1.0                   0.0   \n",
      "21                                  0.0                   0.0   \n",
      "23                                  0.0                   0.0   \n",
      "26                                  1.0                   0.0   \n",
      "29                                  0.0                   0.0   \n",
      "30                                  0.0                   1.0   \n",
      "31                                  0.0                   0.0   \n",
      "33                                  0.0                   0.0   \n",
      "34                                  0.0                   0.0   \n",
      "40                                  0.0                   1.0   \n",
      "41                                  1.0                   0.0   \n",
      "45                                  0.0                   1.0   \n",
      "46                                  1.0                   0.0   \n",
      "49                                  0.0                   0.0   \n",
      "52                                  0.0                   0.0   \n",
      "53                                  1.0                   0.0   \n",
      "56                                  0.0                   0.0   \n",
      "58                                  1.0                   0.0   \n",
      "59                                  0.0                   1.0   \n",
      "64                                  0.0                   0.0   \n",
      "65                                  0.0                   0.0   \n",
      "68                                  1.0                   0.0   \n",
      "69                                  0.0                   0.0   \n",
      "74                                  0.0                   0.0   \n",
      "78                                  1.0                   0.0   \n",
      "80                                  1.0                   0.0   \n",
      "81                                  0.0                   0.0   \n",
      "85                                  0.0                   0.0   \n",
      "89                                  0.0                   0.0   \n",
      "91                                  0.0                   0.0   \n",
      "97                                  0.0                   0.0   \n",
      "98                                  0.0                   0.0   \n",
      "\n",
      "    Pathology_Poorly differentiated  Pathology_Well differentiated  HCV  HBV  \\\n",
      "1                               0.0                            1.0    0    1   \n",
      "3                               0.0                            0.0    0    0   \n",
      "5                               0.0                            1.0    1    1   \n",
      "10                              0.0                            1.0    1    1   \n",
      "11                              0.0                            0.0    0    0   \n",
      "14                              0.0                            0.0    0    0   \n",
      "18                              0.0                            0.0    1    0   \n",
      "21                              0.0                            1.0    0    1   \n",
      "23                              0.0                            1.0    0    0   \n",
      "26                              0.0                            0.0    0    0   \n",
      "29                              0.0                            0.0    0    0   \n",
      "30                              0.0                            0.0    0    0   \n",
      "31                              0.0                            1.0    0    0   \n",
      "33                              0.0                            1.0    0    0   \n",
      "34                              0.0                            0.0    1    1   \n",
      "40                              0.0                            0.0    0    0   \n",
      "41                              0.0                            0.0    1    1   \n",
      "45                              0.0                            0.0    1    0   \n",
      "46                              0.0                            0.0    0    0   \n",
      "49                              0.0                            1.0    1    0   \n",
      "52                              0.0                            1.0    0    0   \n",
      "53                              0.0                            0.0    0    0   \n",
      "56                              0.0                            1.0    0    0   \n",
      "58                              0.0                            0.0    1    0   \n",
      "59                              0.0                            0.0    1    1   \n",
      "64                              1.0                            0.0    0    0   \n",
      "65                              0.0                            0.0    1    1   \n",
      "68                              0.0                            0.0    0    0   \n",
      "69                              1.0                            0.0    1    0   \n",
      "74                              0.0                            1.0    0    0   \n",
      "78                              0.0                            0.0    0    0   \n",
      "80                              0.0                            0.0    1    0   \n",
      "81                              0.0                            1.0    0    0   \n",
      "85                              0.0                            0.0    0    0   \n",
      "89                              0.0                            0.0    1    1   \n",
      "91                              0.0                            0.0    1    1   \n",
      "97                              0.0                            1.0    1    0   \n",
      "98                              0.0                            1.0    0    0   \n",
      "\n",
      "    Child Pugh A  multinodular  T_involvment > 50%  \n",
      "1              1             1                   0  \n",
      "3              1             1                   0  \n",
      "5              0             0                   0  \n",
      "10             1             1                   0  \n",
      "11             1             1                   0  \n",
      "14             0             0                   0  \n",
      "18             1             0                   0  \n",
      "21             1             1                   0  \n",
      "23             1             1                   1  \n",
      "26             0             1                   0  \n",
      "29             0             1                   0  \n",
      "30             1             0                   0  \n",
      "31             1             0                   0  \n",
      "33             1             0                   1  \n",
      "34             1             1                   0  \n",
      "40             1             0                   0  \n",
      "41             1             1                   0  \n",
      "45             1             1                   0  \n",
      "46             1             0                   0  \n",
      "49             0             1                   1  \n",
      "52             1             1                   0  \n",
      "53             1             1                   1  \n",
      "56             1             0                   0  \n",
      "58             1             0                   0  \n",
      "59             1             1                   0  \n",
      "64             1             0                   0  \n",
      "65             1             0                   0  \n",
      "68             1             1                   0  \n",
      "69             1             0                   0  \n",
      "74             1             0                   0  \n",
      "78             0             0                   0  \n",
      "80             1             0                   0  \n",
      "81             1             1                   1  \n",
      "85             1             0                   0  \n",
      "89             1             0                   0  \n",
      "91             1             0                   0  \n",
      "97             1             1                   0  \n",
      "98             1             1                   0  \n",
      "\n",
      "[38 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# combined = pd.merge(\n",
    "#     left=radiomics_df,\n",
    "#     right= clin_df.drop(columns=['TNM']),\n",
    "#     on=\"Subject ID\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clin_df = clin_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split_patients(\n",
    "    clin_df, \n",
    "    identifier=\"TCIA_ID\",\n",
    "    endpoint=\"responder\", \n",
    "    test_ratio=0.4\n",
    ")\n",
    "x_tr_index = x_tr.index\n",
    "x_ts_index = x_ts.index\n",
    "x_cols = x_tr.columns\n",
    "    \n",
    "responder_indices = y_tr[y_tr == 1].index\n",
    "nonresponder_indices = y_tr[y_tr == 0].index\n",
    "\n",
    "preprocess_pipe = Pipeline(steps=[\n",
    "    (\"screening\", CleanFeatureSelector()),\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# Transform\n",
    "x_tr_transformed = preprocess_pipe.fit_transform(x_tr)\n",
    "x_ts_transformed = preprocess_pipe.transform(x_ts)\n",
    "\n",
    "# Extract kept columns\n",
    "# screening_cols = preprocess_pipe.named_steps['filter'].keep_cols\n",
    "kept_cols = preprocess_pipe.named_steps['screening'].keep_cols_\n",
    "\n",
    "# Final aligned DataFrames\n",
    "x_tr = pd.DataFrame(x_tr_transformed, index=x_tr.index, columns=kept_cols)\n",
    "x_ts = pd.DataFrame(x_ts_transformed, index=x_ts.index, columns=kept_cols)\n",
    "\n",
    "# Now you can use x_tr_final and x_ts_final with their original indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_filter_pipe = Pipeline(steps=[\n",
    "#     (\"filter\", FeatureFilterer(\n",
    "#         p_val_threshold=0.2,\n",
    "#         responder_idx=responder_indices,\n",
    "#         nonresponder_idx=nonresponder_indices\n",
    "#     )),\n",
    "    \n",
    "# ])\n",
    "\n",
    "# # Fit on TRAINING DATA ONLY (after screening + impute)\n",
    "# x_tr_filtered = feature_filter_pipe.fit_transform(x_tr)\n",
    "# x_ts_filtered = feature_filter_pipe.transform(x_ts)\n",
    "\n",
    "# # Extract feature names\n",
    "# kept_cols = feature_filter_pipe.named_steps[\"filter\"].significant_features\n",
    "\n",
    "# # Rebuild DataFrame\n",
    "# x_tr = pd.DataFrame(x_tr_filtered, index=x_tr.index, columns=kept_cols)\n",
    "# x_ts = pd.DataFrame(x_ts_filtered, index=x_ts.index, columns=kept_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548d3f5",
   "metadata": {},
   "source": [
    "Univariate Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MRMRSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, K=50, show_progress=False):\n",
    "        self.K = K\n",
    "        self.show_progress = show_progress\n",
    "        self.selected_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # mrmr_classif automatically uses mutual information and redundancy internally\n",
    "        self.selected_features_ = mrmr_classif(\n",
    "            X, y,\n",
    "            K=self.K,\n",
    "            show_progress=self.show_progress\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure that the data type supports column selection\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X[self.selected_features_]\n",
    "        else:\n",
    "            raise TypeError(\"MRMRSelector expects a pandas DataFrame as input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7afa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(\n",
    "#     max_iter= 5000, \n",
    "#     # C=10.0,\n",
    "#     class_weight= \"balanced\"\n",
    "# )\n",
    "\n",
    "lr_param_grid = {\n",
    "  \"C\" :[0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0],\n",
    "  \"penalty\" : [\"l1\", \"l2\"],\n",
    "  \"solver\" : [\"liblinear\", \"saga\"],\n",
    "  \"class_weight\" : [\n",
    "    \"balanced\",\n",
    "    {0:1, 1:2},\n",
    "    {0:1, 1:5},\n",
    "    {0:1, 1:10}\n",
    "  \n",
    "  ]\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum()]  # handles class imbalance\n",
    "}\n",
    "\n",
    "linear_svc_params = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5, 10, 20, 100],\n",
    "    'penalty': ['l1'],\n",
    "    'loss': ['squared_hinge'],\n",
    "    'dual': [False],\n",
    "    'class_weight': [None, 'balanced', {0:1, 1: 2}, {0:1, 1: 5}, {0:1, 1: 10}, {0:1, 1: 20}],\n",
    "    'max_iter': [5000, 10000, 20000],\n",
    "    'random_state': [42],\n",
    "    'intercept_scaling': [0.5, 1.0, 2.0, 3.0]\n",
    "}\n",
    "# Support Vector Machine\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'class_weight': ['balanced', {0:1, 1: 5}, {0: 1, 1: 10}]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (64,), (128,), (64, 64), (128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': np.logspace(2, 4, 10),\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 500, 1000],\n",
    "    'early_stopping': [True],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "adaboost_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'estimator': [\n",
    "        DecisionTreeClassifier(max_depth=1), # Decision Stump\n",
    "        DecisionTreeClassifier(max_depth=2), \n",
    "        DecisionTreeClassifier(max_depth=3)\n",
    "    ],\n",
    "    # AdaBoost does not have a native 'scale_pos_weight' parameter. \n",
    "    # Imbalance is typically handled by adjusting the class_weight of the base estimator \n",
    "    # or relying on the boosting mechanism itself.\n",
    "    # The DecisionTreeClassifier base estimator must handle the class_weight.\n",
    "    # Note: GridSearchCV handles the combination.\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 40], # Main complexity parameter for leaf-wise growth\n",
    "    'max_depth': [-1, 5, 8], # -1 means no limit\n",
    "    'min_child_samples': [20, 50, 100],\n",
    "    'subsample': [0.7, 0.9, 1.0], # Row subsampling\n",
    "    'colsample_bytree': [0.7, 0.9, 1.0], # Feature subsampling\n",
    "    'reg_alpha': [0, 0.1, 0.5], # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 0.5], # L2 regularization\n",
    "    # Handling Imbalance\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum(), 5, 10, 20]\n",
    "}\n",
    "\n",
    "# voting_grid = {\n",
    "#     \"estimators\": [\n",
    "#         (\"lr\", LogisticRegression(max_iter=5000, class_weight=\"balanced\")),\n",
    "#         (\"xgb\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")),\n",
    "#         (\"svc\", SVC(probability=True)),\n",
    "#         (\"et\", ExtraTreesClassifier()),\n",
    "#         (\"rf\", RandomForestClassifier()),\n",
    "#         (\"mlp\", MLPClassifier())\n",
    "#     ]\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_wrapper(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sampling_method=None,\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    if sampling_method is None:\n",
    "        return x_train, y_train\n",
    "\n",
    "    try:\n",
    "        # Get column names before sampling\n",
    "        feature_columns = x_train.columns\n",
    "        target_name = y_train.name if hasattr(y_train, 'name') else 'target'\n",
    "        \n",
    "        # Apply sampling\n",
    "        sampler = sampling_method\n",
    "        x_tr_res, y_tr_res = sampler.fit_resample(x_train, y_train)\n",
    "        \n",
    "        # Convert back to DataFrame/Series with original column names\n",
    "        x_tr_res = pd.DataFrame(x_tr_res, columns=feature_columns)\n",
    "        y_tr_res = pd.Series(y_tr_res, name=target_name)\n",
    "        \n",
    "        return x_tr_res, y_tr_res\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sampling: {str(e)}\")\n",
    "        return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedKFold, RepeatedStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "    \n",
    "\n",
    "def sparse_pca_wrapper(x_train, y_train, estimator, param_grid=None,\n",
    "                      n_components=None, min_feats=5, alpha=1, ridge_alpha=0.01, \n",
    "                      max_iter=1000, n_runs=5, cv=5, scoring='accuracy', \n",
    "                      random_state=42, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Sparse PCA wrapper with integrated cross-validation for feature selection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : array-like of shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : array-like of shape (n_samples,)\n",
    "        Target values\n",
    "    estimator : estimator object\n",
    "        A scikit-learn estimator that implements 'fit' and 'predict'\n",
    "    param_grid : dict, optional\n",
    "        Dictionary with parameters names as keys and lists of parameter settings to try\n",
    "    n_components : int, default=None\n",
    "        Number of sparse components to extract\n",
    "    alpha : float, default=1\n",
    "        Sparsity controlling parameter\n",
    "    ridge_alpha : float, default=0.01\n",
    "        Amount of ridge shrinkage\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    n_runs : int, default=5\n",
    "        Number of runs for stability analysis\n",
    "    cv : int, cross-validation generator, default=5\n",
    "        Determines cross-validation splitting strategy\n",
    "    scoring : str, callable, default='accuracy'\n",
    "        Scoring metric\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict:\n",
    "        Dictionary containing:\n",
    "        - 'best_estimator': Best model from GridSearchCV\n",
    "        - 'best_params': Best parameters from GridSearchCV\n",
    "        - 'cv_results': Cross-validation results\n",
    "        - 'feature_importances': Feature importances if available\n",
    "        - 'stability_scores': Stability of feature selection across runs\n",
    "    \"\"\"\n",
    "  \n",
    "    if param_grid is None:\n",
    "        param_grid = {}  # Default empty param grid\n",
    "    \n",
    "    # Store results from all runs\n",
    "    all_importances = []\n",
    "    best_estimators = []\n",
    "    \n",
    "    # Run multiple times for stability analysis\n",
    "    for run in tqdm(range(n_runs), desc=\"Running stability analysis\"):\n",
    "        # Set random state for this run\n",
    "        current_seed = random_state + run if random_state is not None else None\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=current_seed)\n",
    "        \n",
    "        # Set up GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv_splitter,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        \n",
    "        # Store best estimator\n",
    "        best_estimators.append(grid_search.best_estimator_)\n",
    "        \n",
    "        # Get feature importances if available\n",
    "        if hasattr(grid_search.best_estimator_, 'feature_importances_'):\n",
    "            importances = grid_search.best_estimator_.feature_importances_\n",
    "            all_importances.append(importances)\n",
    "        elif hasattr(grid_search.best_estimator_, 'coef_'):\n",
    "            # For linear models\n",
    "            importances = np.abs(grid_search.best_estimator_.coef_).mean(axis=0)\n",
    "            all_importances.append(importances)\n",
    "    \n",
    "    # Calculate stability scores if we have multiple runs\n",
    "    stability_scores = None\n",
    "    if n_runs > 1 and all_importances:\n",
    "        # Convert to numpy array for calculations\n",
    "        all_importances = np.array(all_importances)\n",
    "        \n",
    "        # Calculate stability as coefficient of variation (lower is more stable)\n",
    "        stability_scores = np.std(all_importances, axis=0) / (np.mean(all_importances, axis=0) + 1e-10)\n",
    "    \n",
    "    # Get the best model from the last run\n",
    "    best_estimator = best_estimators[-1]\n",
    "    # Get feature importances from the best model\n",
    "    feature_importances = None\n",
    "    if hasattr(best_estimator, 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_estimator.feature_importances_,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    elif hasattr(best_estimator, 'coef_'):\n",
    "        # For linear models\n",
    "        coef = best_estimator.coef_\n",
    "        if len(coef.shape) > 1:  # For multi-class\n",
    "            coef = np.abs(coef).mean(axis=0)\n",
    "        feature_importances = pd.Series(\n",
    "            coef,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    \n",
    "\n",
    "    print(feature_importances)\n",
    "    return {\n",
    "        'best_estimator': best_estimator,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features' : feature_importances.nlargest(min_feats).index,\n",
    "        'stability_scores': stability_scores,\n",
    "        'all_importances': all_importances\n",
    "    }\n",
    "\n",
    "def pca_wrapper():\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # The loadings are here!\n",
    "    loadings = pca.components_\n",
    "\n",
    "\n",
    "def lasso_wrapper(x_train, y_train):\n",
    "    # Define the parameter grid for logistic regression\n",
    "    param_grid = {\n",
    "        'Cs': np.logspace(-6, 1, 10),  # Regularization strength,  # Number of regularization values to try\n",
    "        'penalty': 'l1',\n",
    "        'solver': 'saga',\n",
    "        'cv': 5,\n",
    "        'scoring': 'average_precision',\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced',\n",
    "        'max_iter': 10000,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    # Create and fit LogisticRegressionCV\n",
    "    lr_cv = LogisticRegressionCV(**param_grid)\n",
    "    lr_cv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    print(f\"Best C: {lr_cv.C_[0]:.4f}\")\n",
    "    print(f\"Best penalty: {lr_cv.penalty}\")\n",
    "    print(f\"Best cross-validated score: {lr_cv.scores_[1].mean(axis=0).max():.4f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'importance': np.abs(lr_cv.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return {\n",
    "        \"best_model\": lr_cv,\n",
    "        \"feature_importance\": feature_importance,\n",
    "        \"selected_features\": x_train.columns[lr_cv.coef_[0] != 0].tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def sfs_wrapper(estimator, x_train, y_train, max_feats=None, use_predict_proba=True, min_feats=5):\n",
    "    \"\"\"\n",
    "    Performs Sequential Forward Selection (SFS) using RepeatedStratifiedKFold CV\n",
    "    to select the most stable and performant features.\n",
    "\n",
    "    Args:\n",
    "        estimator: The unfitted model (e.g., LogisticRegression).\n",
    "        x_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        max_feats (int): Maximum number of features to select.\n",
    "        min_feats (int): Minimum number of features to select (optional stopping point).\n",
    "\n",
    "    Returns:\n",
    "        list: The list of selected feature names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Define the Robust Cross-Validation Strategy ---\n",
    "    # Using 10 folds repeated 3 times is a common, stable choice.\n",
    "    # Setting random_state is CRUCIAL for reproducibility of the folds.\n",
    "    rskf = RepeatedStratifiedKFold(\n",
    "        n_splits=10, \n",
    "        n_repeats=3, \n",
    "        random_state=42 \n",
    "    )\n",
    "\n",
    "    # --- 2. Initialize the Sequential Feature Selector ---\n",
    "    # We use Forward Selection ('forward') to build the feature set.\n",
    "    # We use 'average_precision' as the scoring metric for imbalanced data.\n",
    "    ap_scorer = make_scorer(\n",
    "        average_precision_score, \n",
    "        needs_proba=True, # ðŸ”‘ This is the key argument for using predict_proba\n",
    "        greater_is_better=True\n",
    "    )\n",
    "\n",
    "    # --- 3. Initialize the Sequential Feature Selector ---\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=estimator,\n",
    "        n_features_to_select=\"auto\" if max_feats == None else max_feats,\n",
    "        direction='forward',\n",
    "        # ðŸ”‘ Pass the custom scorer object here:\n",
    "        scoring=ap_scorer, \n",
    "        cv=rskf,\n",
    "        n_jobs=-1,\n",
    "        # REMOVE the unsupported 'response_method' argument\n",
    "    )\n",
    "\n",
    "    # --- 3. Fit the Selector to the Data ---\n",
    "    # SFS internally performs the cross-validation using rskf at every step\n",
    "    # to evaluate which feature provides the best, most stable performance gain.\n",
    "    sfs.fit(x_train, y_train)\n",
    "\n",
    "    # --- 4. Extract and Return Results ---\n",
    "    selected_features_mask = sfs.get_support()\n",
    "    selected_feature_names = list(x_train.columns[selected_features_mask])\n",
    "  \n",
    "\n",
    "    return {\n",
    "        \"selected_features\" : selected_feature_names\n",
    "    }    \n",
    "\n",
    "def rfecv_wrapper(estimator, x_train, y_train, max_feats = 50, min_feats = 10):\n",
    "    # Ensure x_train is a DataFrame to access column names\n",
    "    # if not hasattr(x_train, 'columns'):\n",
    "    #     x_train = pd.DataFrame(x_train)\n",
    "    \n",
    "    rfecv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=5,\n",
    "        cv=StratifiedKFold(3),\n",
    "        scoring='average_precision',\n",
    "        n_jobs=-1, \n",
    "        min_features_to_select=min_feats,\n",
    "    )\n",
    "\n",
    "    rfecv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get the selected features by name\n",
    "\n",
    "\n",
    "    \n",
    "    # Get feature rankings with names\n",
    "# First, ensure the features are sorted by importance (best rank first)\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'ranking': rfecv.ranking_,\n",
    "        'support': rfecv.support_\n",
    "    }).sort_values('ranking')  # Sort by ranking (lower rank = more important)\n",
    "\n",
    "    # Keep only the top max_feats features\n",
    "    feature_ranking['support'] = feature_ranking.index < max_feats\n",
    "\n",
    "    # Update selected_features to only include the top max_feats features\n",
    "    selected_features = feature_ranking[feature_ranking['support']]['feature'].tolist()\n",
    "\n",
    "    # Print some information\n",
    "    print(f\"Selected top {len(selected_features)} features out of {len(feature_ranking)}\")\n",
    "    print(\"Selected features:\", selected_features)\n",
    "            \n",
    "    # Get cross-validation scores for each number of features\n",
    "    cv_scores = pd.DataFrame({\n",
    "        'n_features': range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n",
    "        'mean_score': rfecv.cv_results_['mean_test_score'],\n",
    "        'std_score': rfecv.cv_results_['std_test_score']\n",
    "    })\n",
    "    \n",
    "    # selected_features = [x_train.columns[col] for col in selected_features]\n",
    "\n",
    "    print(\"Selected features:\", selected_features)\n",
    "\n",
    "    return {\n",
    "        'selected_features': list(selected_features),\n",
    "        'feature_ranking': feature_ranking,\n",
    "        'cv_scores': cv_scores,\n",
    "        'optimal_n_features': rfecv.n_features_,\n",
    "        'rfecv': rfecv\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "pipeline_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if pipeline_path not in sys.path:\n",
    "    sys.path.append(pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_metrics(gs_cv_results, n_splits=4, title='Cross-Validation Metrics'):\n",
    "  \n",
    "    # Extract metrics for each fold\n",
    "    metrics = []\n",
    "    for i in range(1, n_splits):\n",
    "        fold_metrics = {\n",
    "            'Fold': i+1,\n",
    "            'Sensitivity': gs_cv_results[f'split{i}_test_sensitivity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Specificity': gs_cv_results[f'split{i}_test_specificity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Accuracy': gs_cv_results[f'split{i}_test_accuracy'][gs_cv_results['rank_test_accuracy'].argmin()]\n",
    "        }\n",
    "        metrics.append(fold_metrics)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df_metrics = pd.DataFrame(metrics).melt('Fold', var_name='Metric', value_name='Score')\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Fold', y='Score', hue='Metric', data=df_metrics)\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print mean and std of metrics\n",
    "    print(f\"Mean Sensitivity: {df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].mean():.3f} \"\n",
    "          f\"(Â±{df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Specificity: {df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].mean():.3f} \"\n",
    "          f\"(Â±{df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Accuracy: {df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].mean():.3f} \"\n",
    "          f\"(Â±{df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].std():.3f})\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_cv_metrics(gs.cv_results_, n_splits=4, title='Model Performance per Fold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import LinearRegression\n",
    "# from custom_models.svm_shap import SVMSHAP\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, average_precision_score, accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier #\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=3,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# scores = cross_validate(\n",
    "#     model,\n",
    "#     X,\n",
    "#     y,\n",
    "#     cv=cv,\n",
    "#     scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\"],\n",
    "#     n_jobs=-1,\n",
    "#     return_estimator=False,\n",
    "#     return_train_score=False\n",
    "# )\n",
    "\n",
    "# print(scores[\"test_roc_auc\"].mean(), scores[\"test_roc_auc\"].std())\n",
    "\n",
    "# XGBoost\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# SVM\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    svm_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "linear_svc_grid = GridSearchCV(\n",
    "    LinearSVC(random_state=42),\n",
    "    linear_svc_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extra Trees\n",
    "et_grid = GridSearchCV(\n",
    "    ExtraTreesClassifier(random_state=42),\n",
    "    et_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  # â† Most common choice\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=cv,\n",
    "    refit='average_precision',\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  # â† Most common choice\n",
    "    },\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "gs = GridSearchCV(\n",
    "  param_grid=lr_param_grid,\n",
    "  cv = cv,\n",
    "  estimator=LogisticRegression(random_state=42),\n",
    "  refit=\"accuracy\",\n",
    "  scoring={\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'average_precision': 'average_precision',\n",
    "    'accuracy': 'accuracy'\n",
    "  },\n",
    ")\n",
    "\n",
    "\n",
    "mlp_grid = GridSearchCV(\n",
    "    MLPClassifier(),\n",
    "    mlp_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,  # Note: MLP doesn't support n_jobs > 1\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "adaboost_grid = GridSearchCV(\n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),\n",
    "    adaboost_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grids = [\n",
    "    xgb_grid,        # XGBoost GridSearchCV\n",
    "    svm_grid,        # SVM GridSearchCV\n",
    "    linear_svc_grid, # Linear SVC GridSearchCV\n",
    "    et_grid,         # Extra Trees GridSearchCV\n",
    "    rf_grid, \n",
    "    mlp_grid,        # Random Forest GridSearchCV\n",
    "    gs               # Logistic Regression GridSearchCV\n",
    "]\n",
    "\n",
    "\n",
    "#======= FS WRAPPER ========\n",
    "\n",
    "\n",
    "    \n",
    "# current_gs = linear_svc_grid\n",
    "# current_gs.fit(x_tr, y_tr)\n",
    "# # Get the best estimator from GridSearchCV\n",
    "# best_lr = current_gs.best_estimator_\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = best_lr.predict(x_ts[feats])\n",
    "# # Generate and print the classification report\n",
    "# print(\"Best Parameters:\", current_gs.best_params_)\n",
    "# print(\"\\nClassification Report for Best Model:\")\n",
    "# print(classification_report(y_ts, y_pred))\n",
    "# print(confusion_matrix(y_ts,y_pred))\n",
    "# print(\"model accuracy:\", accuracy_score(y_ts, y_pred))\n",
    "\n",
    "# x_ts_filtered = x_ts[feats]\n",
    "\n",
    "# try:\n",
    "#     # Pass the filtered TEST FEATURES (x_ts[feats]) to predict_proba\n",
    "#     y_prob = best_lr.predict_proba(x_ts_filtered)\n",
    "    \n",
    "#     # NOTE: roc_auc_score requires probabilities for the positive class (column 1)\n",
    "#     # The output of predict_proba is usually (N_samples, 2), so we take all rows and column index 1\n",
    "#     model_proba = y_prob[:, 1]\n",
    "    \n",
    "#     print(\"model roc auc\", roc_auc_score(y_ts.to_numpy(), model_proba))\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(\"cannot print proba\")\n",
    "\n",
    "# print(current_gs.cv_results_ )\n",
    "# # plot_cv_metrics(current_gs.cv_results_, n_splits=3, title='Your Model Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d062eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score, accuracy_score, classification_report\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def save_model_results(grid, x_tr, y_tr, x_ts, y_ts, feats, \n",
    "                      feature_selection_name='', filter_algorithm_name='',\n",
    "                      output_file='model_results_clinical.xlsx'):\n",
    "    \"\"\"\n",
    "    Save model results including CV metrics (from GridSearchCV) and test set performance.\n",
    "    Expects a GridSearchCV instance. Will fit the grid if not already fitted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure grid is fitted (force surfacing of real errors)\n",
    "        if not hasattr(grid, 'best_estimator_'):\n",
    "            # If possible, flip error_score to raise to see root cause early\n",
    "            try:\n",
    "                grid.error_score = 'raise'\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(\"Fitting GridSearchCV...\")\n",
    "            grid.fit(x_tr[feats], y_tr)\n",
    "\n",
    "        best_estimator = grid.best_estimator_\n",
    "        cv_results = grid.cv_results_\n",
    "        best_params = grid.best_params_\n",
    "        best_idx = getattr(grid, 'best_index_', None)\n",
    "        model_name = type(best_estimator).__name__\n",
    "\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        run_id = f\"{model_name}_{feature_selection_name}_{filter_algorithm_name}_{timestamp}\"\n",
    "\n",
    "        # Extract CV metrics robustly\n",
    "        # Primary CV score comes from grid.best_score_ regardless of scoring name\n",
    "        primary_cv = getattr(grid, 'best_score_', None)\n",
    "\n",
    "        def _get_cv_stat(key_mean, key_std=None):\n",
    "            if key_mean in cv_results:\n",
    "                mean_val = cv_results[key_mean][best_idx] if best_idx is not None else np.nan\n",
    "                if key_std and key_std in cv_results:\n",
    "                    std_val = cv_results[key_std][best_idx]\n",
    "                    return f\"{mean_val:.4f} Â± {std_val:.4f}\"\n",
    "                return f\"{mean_val:.4f}\"\n",
    "            return 'N/A'\n",
    "\n",
    "        # Try to read explicit metric keys if you used a scoring dict when creating the grid\n",
    "        cv_metrics = {\n",
    "            'cv_primary_metric': f\"{primary_cv:.4f}\" if primary_cv is not None else 'N/A',\n",
    "            'cv_train_accuracy': _get_cv_stat('mean_train_accuracy', 'std_train_accuracy'),\n",
    "            'cv_test_accuracy':  _get_cv_stat('mean_test_accuracy',  'std_test_accuracy'),\n",
    "            'cv_train_roc_auc':  _get_cv_stat('mean_train_roc_auc',  'std_train_roc_auc'),\n",
    "            'cv_test_roc_auc':   _get_cv_stat('mean_test_roc_auc',   'std_test_roc_auc'),\n",
    "            'cv_train_ap':       _get_cv_stat('mean_train_average_precision', 'std_train_average_precision'),\n",
    "            'cv_test_ap':        _get_cv_stat('mean_test_average_precision',  'std_test_average_precision'),\n",
    "            # Fallback if you passed a single scoring like 'average_precision'\n",
    "            'cv_test_score':     _get_cv_stat('mean_test_score', 'std_test_score')\n",
    "        }\n",
    "\n",
    "        # Test set predictions\n",
    "        y_pred = best_estimator.predict(x_ts[feats])\n",
    "        y_prob = best_estimator.predict_proba(x_ts[feats])[:, 1] if hasattr(best_estimator, 'predict_proba') else None\n",
    "\n",
    "        test_metrics = {\n",
    "            'test_accuracy': accuracy_score(y_ts, y_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "            'test_average_precision': average_precision_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "        }\n",
    "\n",
    "        clf_report = classification_report(y_ts, y_pred, output_dict=True)\n",
    "\n",
    "        metrics_data = {\n",
    "            'Run_ID': run_id,\n",
    "            'Model': model_name,\n",
    "            'Feature_Selection': feature_selection_name,\n",
    "            'Filter_Algorithm': filter_algorithm_name,\n",
    "            'Num_Features': len(feats),\n",
    "            'Timestamp': timestamp,\n",
    "            'CV_Strategy': 'GridSearchCV',\n",
    "            **cv_metrics,\n",
    "            **{k: v for k, v in test_metrics.items()},\n",
    "            'Features_Used': str(feats),\n",
    "            'Model_Params': str(best_params)\n",
    "        }\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "        clf_df = pd.DataFrame(clf_report).transpose()\n",
    "        clf_df['Run_ID'] = run_id\n",
    "        clf_df = clf_df.reset_index().rename(columns={'index': 'class'})\n",
    "\n",
    "        # Save to Excel\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                try:\n",
    "                    existing_metrics = pd.read_excel(writer, sheet_name='Metrics')\n",
    "                    metrics_df = pd.concat([existing_metrics, metrics_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "                try:\n",
    "                    existing_clf = pd.read_excel(writer, sheet_name='Classification_Reports')\n",
    "                    clf_df = pd.concat([existing_clf, clf_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "        except Exception:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return metrics_df, clf_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc7b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialFeatureSelector\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# lasso_result = lasso_wrapper(\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     x_train=x_tr,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Use the selected features from Lasso as input to RFECV\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m rfecv_result \u001b[38;5;241m=\u001b[39m \u001b[43msfs_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAdaBoostClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or your preferred estimator\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use only Lasso-selected features\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_feats=10,\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_predict_proba\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# rfecv_result = rfecv_wrapper(\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     estimator=AdaBoostClassifier(random_state=42),\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     x_train = x_tr,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the final selected features from RFECV\u001b[39;00m\n\u001b[0;32m     30\u001b[0m selected_features_rfecv  \u001b[38;5;241m=\u001b[39m rfecv_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[16], line 243\u001b[0m, in \u001b[0;36msfs_wrapper\u001b[1;34m(estimator, x_train, y_train, max_feats, use_predict_proba, min_feats)\u001b[0m\n\u001b[0;32m    229\u001b[0m sfs \u001b[38;5;241m=\u001b[39m SequentialFeatureSelector(\n\u001b[0;32m    230\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    231\u001b[0m     n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_feats \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m max_feats,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# REMOVE the unsupported 'response_method' argument\u001b[39;00m\n\u001b[0;32m    238\u001b[0m )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# --- 3. Fit the Selector to the Data ---\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# SFS internally performs the cross-validation using rskf at every step\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# to evaluate which feature provides the best, most stable performance gain.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[43msfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# --- 4. Extract and Return Results ---\u001b[39;00m\n\u001b[0;32m    246\u001b[0m selected_features_mask \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mget_support()\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:283\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    281\u001b[0m     process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 283\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_best_new_feature_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:314\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, cv, current_mask, **params)\u001b[0m\n\u001b[0;32m    312\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[0;32m    313\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 314\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    323\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:677\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    675\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 677\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:399\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 399\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# First, run the lasso_wrapper to get the selected features\n",
    "from custom_models.svm_shap import SVMSHAP\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "# lasso_result = lasso_wrapper(\n",
    "    \n",
    "#     x_train=x_tr,\n",
    "#     y_train=y_tr\n",
    "# )\n",
    "\n",
    "# # Get the selected features from Lasso\n",
    "# selected_features_lasso = lasso_result[\"selected_features\"]\n",
    "\n",
    "# Use the selected features from Lasso as input to RFECV\n",
    "rfecv_result = sfs_wrapper(\n",
    "    estimator=AdaBoostClassifier(random_state=42),  # or your preferred estimator\n",
    "    x_train=x_tr,  # Use only Lasso-selected features\n",
    "    y_train=y_tr,\n",
    "    # max_feats=10,\n",
    "    use_predict_proba=True\n",
    ")\n",
    "\n",
    "# rfecv_result = rfecv_wrapper(\n",
    "#     estimator=AdaBoostClassifier(random_state=42),\n",
    "#     x_train = x_tr,\n",
    "#     y_train = y_tr, \n",
    "#     max_feats=10\n",
    "# )\n",
    "\n",
    "# Get the final selected features from RFECV\n",
    "selected_features_rfecv  = rfecv_result[\"selected_features\"]\n",
    "# print(\"Lasso selected features:\", selected_features_lasso)\n",
    "print(\"RFECV selected features:\", selected_features_rfecv)\n",
    "# # Get the selected features\n",
    "# selected_features = result[\"selected_features\"]\n",
    "metrics_df, clf_report = save_model_results(\n",
    "    grid=svm_grid,  # Pass the fitted GridSearchCV instance\n",
    "    x_tr=x_tr,\n",
    "    y_tr=y_tr,\n",
    "    x_ts=x_ts,\n",
    "    y_ts=y_ts,\n",
    "    feats=selected_features_rfecv,\n",
    "    feature_selection_name='None',  # or your feature selection method\n",
    "    filter_algorithm_name='MannWhitney',  # or your filter method\n",
    "    output_file='model_results_clinical.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real AUC: 0.5978571428571429 Â± 0.17000393902999508\n",
      "Permuted AUC: 0.49660714285714286 Â± 0.18332198867187818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -----------------------------\n",
    "# Replace these with your data\n",
    "# X = your feature matrix\n",
    "# y = your label vector\n",
    "# -----------------------------\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# ---- Real score ----\n",
    "real_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_tr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Real AUC:\", real_scores.mean(), \"Â±\", real_scores.std())\n",
    "\n",
    "# ---- Permuted score ----\n",
    "y_perm = np.random.permutation(y_tr)\n",
    "\n",
    "perm_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_perm,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Permuted AUC:\", perm_scores.mean(), \"Â±\", perm_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12e5dd",
   "metadata": {},
   "source": [
    "Model Tuning(?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b975781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the Variance Inflation Factor (VIF) for all numeric columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    # Use only numeric, non-null columns\n",
    "    X = df.select_dtypes(include=np.number).dropna(axis=1, how='any')\n",
    "\n",
    "    if X.empty:\n",
    "        return pd.DataFrame({'Feature': [], 'VIF': []})\n",
    "\n",
    "    # Add constant for intercept term\n",
    "    X_vif = sm.add_constant(X)\n",
    "    \n",
    "    features = X.columns\n",
    "    vif_list = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        # Calculate VIF. Index i+1 accounts for the added 'const' column at index 0.\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X_vif.values, i + 1)\n",
    "            vif_list.append(vif)\n",
    "        except Exception:\n",
    "            vif_list.append(np.nan) # Set to NaN if calculation fails (e.g., perfect collinearity)\n",
    "\n",
    "    vif_df = pd.DataFrame({'Feature': features, 'VIF': vif_list})\n",
    "    return vif_df.sort_values(by='VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "vif_df = calculate_vif(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cfcf3",
   "metadata": {},
   "source": [
    "RFE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin-bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
