{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c7f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ALL NECESSARY IMPORTS AND CLASSES =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import sklearn\n",
    "from typing import Union, Literal, TypeAlias\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ===== CleanFeatureSelector Class =====\n",
    "class CleanFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_thresh=0.3, variance_thresh=1e-6):\n",
    "        self.missing_thresh = missing_thresh\n",
    "        self.variance_thresh = variance_thresh\n",
    "        self.keep_features_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "        # Drop by missing %\n",
    "        keep_missing = X.isna().mean() < self.missing_thresh\n",
    "        X2 = X.loc[:, keep_missing]\n",
    "        # Drop by variance\n",
    "        var = X2.var()\n",
    "        keep_var = var > self.variance_thresh\n",
    "        self.keep_features_ = X2.columns[keep_var].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "        return X[self.keep_features_]\n",
    "\n",
    "# ===== train_test_split_patients Function =====\n",
    "def train_test_split_patients(dataframe: pd.DataFrame, identifier: str, endpoint: str, test_ratio: float = 0.3):\n",
    "    unique_patients = dataframe[identifier].unique()\n",
    "    test_patients = unique_patients[math.floor(1-test_ratio * len(unique_patients)):]\n",
    "    train_patients = unique_patients[: math.floor(1-test_ratio * len(unique_patients))]\n",
    "    train_mask = dataframe[identifier].isin(train_patients)\n",
    "    test_mask = dataframe[identifier].isin(test_patients)\n",
    "    x_train = dataframe[train_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    x_test = dataframe[test_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    y_train = dataframe[train_mask][endpoint]\n",
    "    y_test = dataframe[test_mask][endpoint]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# ===== preprocess_data_w_pipeline Function =====\n",
    "Processed: TypeAlias = Union[tuple[pd.DataFrame, pd.DataFrame], tuple[np.ndarray, np.ndarray]]\n",
    "def preprocess_data_w_pipeline(input_data: tuple[pd.DataFrame, pd.DataFrame], \n",
    "                               preprocess_pipe: sklearn.pipeline.Pipeline,\n",
    "                               keep_cols=True,\n",
    "                               output: Union[Literal[\"dataframe\"], Literal[\"ndarray\"]]=\"dataframe\") -> Processed:\n",
    "    if not isinstance(preprocess_pipe, sklearn.pipeline.Pipeline):\n",
    "        raise TypeError(\"preprocess_pipe argument must be instance of sklearn.pipeline.Pipeline!\")\n",
    "    if output not in [\"dataframe\", \"ndarray\"]:\n",
    "        raise ValueError(\"output must either be 'dataframe' or 'ndarray'!\")\n",
    "    if not isinstance(input_data, tuple) or len(input_data) != 2:\n",
    "        raise TypeError(\"input_data must be a tuple of (x_train, x_test) DataFrames.\")\n",
    "    \n",
    "    x_tr, x_ts = input_data\n",
    "    original_x_tr, original_x_ts = x_tr, x_ts\n",
    "    \n",
    "    try:\n",
    "        x_tr_index = x_tr.index\n",
    "        x_ts_index = x_ts.index\n",
    "        x_cols = x_tr.columns\n",
    "        x_tr = preprocess_pipe.fit_transform(x_tr)\n",
    "        x_ts = preprocess_pipe.transform(x_ts)\n",
    "        # rebuild DataFrames with only the kept columns\n",
    "        kept_cols = preprocess_pipe.named_steps['screening'].keep_features_\n",
    "        x_tr = pd.DataFrame(x_tr, columns=kept_cols, index=x_tr_index)\n",
    "        x_ts = pd.DataFrame(x_ts, columns=kept_cols, index=x_ts_index)\n",
    "        if output == \"ndarray\":\n",
    "            return x_tr.to_numpy(), x_ts.to_numpy()\n",
    "        return x_tr, x_ts\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return original_x_tr, original_x_ts\n",
    "\n",
    "# ===== BaseEnsembleClassifier Class =====\n",
    "class BaseEnsembleClassifier:\n",
    "    def __init__(self, base_estimators: list, final_estimator, pickle_file_path: str):\n",
    "        self.base_estimators = base_estimators\n",
    "        self.base_estimator_info = {}\n",
    "        self.final_estimator = final_estimator\n",
    "        self.classification_report = {}\n",
    "        self.final_estimator_dataset = pd.DataFrame([])\n",
    "        self.final_est_params = pd.DataFrame({})\n",
    "        self.pickle_file_path = pickle_file_path\n",
    "        # Automatically load pickle file if it exists\n",
    "        self.load_pickle_file()\n",
    "        \n",
    "    def load_pickle_file(self) -> bool:\n",
    "        \"\"\"Load weights/parameters from a pickle file and update the associated base estimators.\"\"\"\n",
    "        if os.path.exists(self.pickle_file_path):\n",
    "            try:\n",
    "                with open(self.pickle_file_path, 'rb') as f:\n",
    "                    weights_dict = pickle.load(f)\n",
    "                # Update weights of associated models by indexing the pickle dictionary\n",
    "                for idx, estimator in enumerate(self.base_estimators):\n",
    "                    if idx in weights_dict:\n",
    "                        self._update_estimator_weights(estimator, weights_dict[idx])\n",
    "                    elif hasattr(estimator, '__class__'):\n",
    "                        estimator_name = estimator.__class__.__name__\n",
    "                        if estimator_name in weights_dict:\n",
    "                            self._update_estimator_weights(estimator, weights_dict[estimator_name])\n",
    "                        elif str(idx) in weights_dict:\n",
    "                            self._update_estimator_weights(estimator, weights_dict[str(idx)])\n",
    "                self.base_estimator_info = weights_dict\n",
    "                print(f\"Successfully loaded weights from {self.pickle_file_path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading pickle file {self.pickle_file_path}: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Pickle file not found: {self.pickle_file_path}\")\n",
    "            return False\n",
    "    \n",
    "    def _update_estimator_weights(self, estimator, weights):\n",
    "        \"\"\"Update the weights/parameters of an estimator.\"\"\"\n",
    "        try:\n",
    "            if isinstance(weights, dict):\n",
    "                if hasattr(estimator, 'set_params'):\n",
    "                    estimator.set_params(**weights)\n",
    "                elif hasattr(estimator, 'coef_'):\n",
    "                    if 'coef_' in weights:\n",
    "                        estimator.coef_ = weights['coef_']\n",
    "                elif hasattr(estimator, 'feature_importances_'):\n",
    "                    if 'feature_importances_' in weights:\n",
    "                        estimator.feature_importances_ = weights['feature_importances_']\n",
    "            elif isinstance(weights, np.ndarray):\n",
    "                if hasattr(estimator, 'coef_'):\n",
    "                    estimator.coef_ = weights\n",
    "                elif hasattr(estimator, 'feature_importances_'):\n",
    "                    estimator.feature_importances_ = weights\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not update weights for {type(estimator).__name__}: {e}\")\n",
    "    \n",
    "    def generate_classification_report(self, y_test, y_pred):\n",
    "        \"\"\"Generates, prints, and stores the classification report.\"\"\"\n",
    "        self.classification_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        print(\"\\n=== Classification Report ===\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        return pd.DataFrame(self.classification_report).transpose()\n",
    "\n",
    "    def _construct_meta_estimator_dataset(self, X):\n",
    "        \"\"\"Construct meta-features dataset from base estimator predictions.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        meta_features_list = []\n",
    "        for estimator in self.base_estimators:\n",
    "            if hasattr(estimator, 'predict_proba'):\n",
    "                try:\n",
    "                    proba = estimator.predict_proba(X)\n",
    "                    if proba.shape[1] == 2:\n",
    "                        meta_features_list.append(proba[:, 1])\n",
    "                    else:\n",
    "                        meta_features_list.append(proba.flatten())\n",
    "                except:\n",
    "                    meta_features_list.append(estimator.predict(X))\n",
    "            else:\n",
    "                meta_features_list.append(estimator.predict(X))\n",
    "        meta_features = np.column_stack(meta_features_list)\n",
    "        return meta_features\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the ensemble model following sklearn conventions.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.values\n",
    "        # Step 1: Fit all base estimators\n",
    "        for estimator in self.base_estimators:\n",
    "            if sample_weight is not None and hasattr(estimator, 'fit'):\n",
    "                try:\n",
    "                    estimator.fit(X, y, sample_weight=sample_weight)\n",
    "                except TypeError:\n",
    "                    estimator.fit(X, y)\n",
    "            else:\n",
    "                estimator.fit(X, y)\n",
    "        # Step 2: Construct meta-features dataset\n",
    "        self.final_estimator_dataset = self._construct_meta_estimator_dataset(X)\n",
    "        # Step 3: Fit the final estimator\n",
    "        if sample_weight is not None and hasattr(self.final_estimator, 'fit'):\n",
    "            try:\n",
    "                self.final_estimator.fit(self.final_estimator_dataset, y, sample_weight=sample_weight)\n",
    "            except TypeError:\n",
    "                self.final_estimator.fit(self.final_estimator_dataset, y)\n",
    "        else:\n",
    "            self.final_estimator.fit(self.final_estimator_dataset, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        meta_features = self._construct_meta_estimator_dataset(X)\n",
    "        y_pred = self.final_estimator.predict(meta_features)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for samples in X.\"\"\"\n",
    "        if not hasattr(self.final_estimator, 'predict_proba'):\n",
    "            raise AttributeError(f\"Final estimator {type(self.final_estimator).__name__} does not support predict_proba\")\n",
    "        meta_features = self._construct_meta_estimator_dataset(X)\n",
    "        probabilities = self.final_estimator.predict_proba(meta_features)\n",
    "        return probabilities\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute decision function of samples in X.\"\"\"\n",
    "        if not hasattr(self.final_estimator, 'decision_function'):\n",
    "            raise AttributeError(f\"Final estimator {type(self.final_estimator).__name__} does not support decision_function\")\n",
    "        meta_features = self._construct_meta_estimator_dataset(X)\n",
    "        decision = self.final_estimator.decision_function(meta_features)\n",
    "        return decision\n",
    "\n",
    "# ===== StackingClassifier Class =====\n",
    "class StackingClassifier(BaseEnsembleClassifier):\n",
    "    def __init__(self, base_estimators, final_estimator, pickle_file_path: str = \"\"):\n",
    "        # If no pickle file path provided, use a dummy path that won't exist\n",
    "        if not pickle_file_path:\n",
    "            pickle_file_path = os.path.join(os.getcwd(), \"__dummy_pickle_path__.pkl\")\n",
    "        super().__init__(base_estimators, final_estimator, pickle_file_path)\n",
    "        \n",
    "    def fit_base_estimators(self, x_train: Union[pd.DataFrame, np.ndarray], y_train: Union[pd.DataFrame, np.ndarray]) -> None:\n",
    "        \"\"\"Fit all base models.\"\"\"\n",
    "        for estimator in self.base_estimators:\n",
    "            estimator.fit(x_train, y_train)\n",
    "\n",
    "    def fit_final_estimator(self, x_train: Union[pd.DataFrame, np.ndarray], y_train: Union[pd.DataFrame, np.ndarray]):\n",
    "        \"\"\"Fit the final estimator.\"\"\"\n",
    "        self.final_estimator.fit(x_train, y_train)\n",
    "        return self\n",
    "\n",
    "# ===== Import test cases (if available) =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool = [\n",
    "    {\n",
    "        \"estimator\": LogisticRegression,\n",
    "        \"params\": {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "            'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'],\n",
    "            'max_iter': [100, 200, 500],\n",
    "            'class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": SVC,\n",
    "        \"params\": {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'class_weight': [None, 'balanced'],\n",
    "            'probability': [True]  # Required for stacking\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": KNeighborsClassifier,\n",
    "        \"params\": {\n",
    "            'n_neighbors': [3, 5, 7, 10],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]  # 1: manhattan, 2: euclidean\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": XGBClassifier,\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'min_child_weight': [1, 3, 5]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# model_pool = [LogisticRegression, SVC, RandomForestClassifier, KNeighborsClassifier, XGBClassifier]\n",
    "\n",
    "\n",
    "model_pool = [\n",
    "    {\n",
    "        \"estimator\" : LogisticRegression, \n",
    "        \"params\" : \n",
    "        \n",
    "    }\n",
    "]\n",
    "for model in model_pool:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin-bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
