{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nrrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_features_dir = os.path.join(os.path.dirname(os.getcwd()),\"data\", \"radiomics_features_updated.csv\")\n",
    "radiomics_df= pd.read_csv(radiomics_features_dir)\n",
    "\n",
    "radiomics_df[\"TCIA_ID\"] = radiomics_df[\"Subject ID_x\"]\n",
    "radiomics_df = radiomics_df.drop(columns=[\"Subject ID_x\"])\n",
    "# len(radiomics_df[\"Subject ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad662296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diagnostic_cols(df):\n",
    "    copy = df.copy()\n",
    "    retained_cols = []\n",
    "    for column in copy.columns:\n",
    "        if \"diagnostics\" in column:\n",
    "            continue\n",
    "\n",
    "        retained_cols.append(column)\n",
    "\n",
    "    return copy[retained_cols]\n",
    "\n",
    "radiomics_df = remove_diagnostic_cols(radiomics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df = pd.read_csv(\"clinical_df_cleaned_fix.csv\")\n",
    "\n",
    "radiomics_df_merged = pd.merge(\n",
    "    left=radiomics_df,\n",
    "    right= clin_df[[\"TCIA_ID\", \"responder\"]],\n",
    "    on=\"TCIA_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Assuming 'radiomics_df_merged' is the original DataFrame containing all features and metadata\n",
    "\n",
    "# 1. Separate the metadata columns\n",
    "metadata_cols = [\"TCIA_ID\", \"responder\"] \n",
    "metadata = radiomics_df_merged[metadata_cols].copy()\n",
    "\n",
    "# 2. Drop metadata to isolate ONLY radiomics features\n",
    "radiomics_features = radiomics_df_merged.drop(columns=metadata_cols, errors='ignore').copy()\n",
    "\n",
    "# 3. Filter the features to keep ONLY those containing 'pv'\n",
    "radiomics_features_pv = radiomics_features\n",
    "\n",
    "# 4. Handle missing subjects (rows) by dropping any rows that are all NaN across PV features\n",
    "radiomics_df_final = radiomics_features_pv\n",
    "\n",
    "# 5. Concatenate the filtered PV features with the aligned metadata columns\n",
    "# NOTE: We use .loc[] to ensure the metadata only includes subjects that survived the dropna step (row alignment).\n",
    "metadata_aligned = metadata.loc[radiomics_df_final.index].drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "radiomics_df_final = pd.concat([radiomics_df_final, metadata_aligned], axis=1).reset_index(drop=True)\n",
    "\n",
    "# radiomics_df_final now contains ONLY PV features + TCIA_ID + responder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiphase_extraction_fixed_df = pd.read_csv(\"multiphase_radiomic_features_pv_aligned_v2_WAVELET.csv\")\n",
    "\n",
    "used_cols =  [col for col in multiphase_extraction_fixed_df.columns if (\"delayed\" not in col) and (\"diagnostics\" not in col)]\n",
    "\n",
    "multiphase_extraction_fixed_df = multiphase_extraction_fixed_df[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiphase_extraction_fixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22739ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiphase_extraction_fixed_df = pd.merge(\n",
    "    left=multiphase_extraction_fixed_df, \n",
    "    right = clin_df[[\"responder\", \"TCIA_ID\"]], \n",
    "    left_on=\"Subject ID\",\n",
    "    right_on=\"TCIA_ID\",\n",
    "    how = \"left\",\n",
    "    validate=\"one_to_one\"\n",
    ")\n",
    "\n",
    "multiphase_extraction_fixed_df = multiphase_extraction_fixed_df.drop(columns=[\"Subject ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = radiomics_df_final.columns.intersection(multiphase_extraction_fixed_df.columns)\n",
    "\n",
    "radiomics_df_final = pd.concat(\n",
    "    [radiomics_df_final[common_cols], multiphase_extraction_fixed_df[common_cols]],\n",
    "    axis = 0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final = radiomics_df_final.sort_values(\"TCIA_ID\").reset_index()\n",
    "\n",
    "drop_cases = (set(clin_df[\"TCIA_ID\"].unique())).difference(set(radiomics_df_final[\"TCIA_ID\"].unique()))\n",
    "clin_df = clin_df[clin_df[\"TCIA_ID\"].isin(drop_cases) == False]\n",
    "\n",
    "clin_df = clin_df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "def is_train_test_difference_significant(x_train, x_test, alpha=0.05):\n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    \n",
    "    # Flatten arrays if they are multi-dimensional\n",
    "    if x_train.ndim > 1:\n",
    "        x_train = x_train.ravel()\n",
    "    if x_test.ndim > 1:\n",
    "        x_test = x_test.ravel()\n",
    "    \n",
    "    # Perform the Wilcoxon rank-sum test\n",
    "    stat, p_value = ranksums(x_train, x_test)\n",
    "    \n",
    "    # Check if the difference is statistically significant\n",
    "    is_significant = p_value < alpha\n",
    "\n",
    "    \n",
    "    \n",
    "    return is_significant, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radiomics_df_final.to_csv(\"radiomics_df_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b30508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_rows = radiomics_df_final[\"responder\"].dropna().index\n",
    "\n",
    "# radiomics_df_final = radiomics_df_final.loc[non_empty_rows]\n",
    "\n",
    "# radiomics_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b96710",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "print(parent_path)\n",
    "data_df = pd.read_csv(os.path.join(parent_path, \"data\", \"combined_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047afca",
   "metadata": {},
   "source": [
    "A/B Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab7af9",
   "metadata": {},
   "source": [
    "combine endpoint and radiomic for initial test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tkinter import N\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def train_test_split_patients(\n",
    "#     dataframe: pd.DataFrame, \n",
    "#     identifier: str, \n",
    "#     endpoint: str, \n",
    "#     test_ratio: float = 0.2,\n",
    "#     random_state: int = 42\n",
    "# ):\n",
    "#     # Get unique patient IDs\n",
    "#     unique_patients = dataframe[identifier].unique()\n",
    "    \n",
    "#     # Split patients into train and test\n",
    "#     train_patients, test_patients = train_test_split(\n",
    "#         unique_patients, \n",
    "#         test_size=test_ratio,\n",
    "#         random_state=random_state,\n",
    "#         stratify=dataframe[endpoint]  # Optional: maintain class balance\n",
    "#     )\n",
    "    \n",
    "#     # Create masks\n",
    "#     train_mask = dataframe[identifier].isin(train_patients)\n",
    "#     test_mask = dataframe[identifier].isin(test_patients)\n",
    "#     cols_to_drop = [endpoint]\n",
    "\n",
    "#     if identifier != None:\n",
    "#         cols_to_drop.append(identifier)\n",
    "#     # Split the data\n",
    "#     x_train = dataframe[train_mask].drop(columns=cols_to_drop, axis=1)\n",
    "#     x_test = dataframe[test_mask].drop(columns=cols_to_drop, axis=1)\n",
    "#     y_train = dataframe[train_mask][endpoint]\n",
    "#     y_test = dataframe[test_mask][endpoint]\n",
    "\n",
    " \n",
    "#     return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67448bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.3\n",
    "train_test_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cddc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split_patients(\n",
    "    dataframe: pd.DataFrame, \n",
    "    identifier: str, \n",
    "    endpoint: str, \n",
    "    test_ratio: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into train and test sets at the patient level, \n",
    "    ensuring stratification based on the patient's final outcome (endpoint).\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input data.\n",
    "        identifier (str): The column name for unique patient IDs.\n",
    "        endpoint (str): The column name for the classification target/outcome.\n",
    "        test_ratio (float): The proportion of the data to include in the test split.\n",
    "        random_state (int): Controls the randomness of the split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Get unique patients and their associated endpoint ---\n",
    "    # We assume the 'endpoint' is the same for all rows belonging to one patient.\n",
    "    # We create a Series where the index is the patient ID and the value is the endpoint.\n",
    "    # patient_outcomes = dataframe.drop_duplicates(subset=[identifier]).set_index(identifier)[endpoint]\n",
    "    \n",
    "    # # Get the unique patient IDs (data to split) and their corresponding outcomes (labels to stratify by)\n",
    "    # unique_patients = patient_outcomes.index.to_numpy()\n",
    "    # stratify_labels = patient_outcomes.to_numpy()\n",
    "    \n",
    "    # --- 2. Split patients into train and test (Correct Stratification) ---\n",
    "    train_patients, test_patients = train_test_split(\n",
    "        dataframe[identifier], \n",
    "        test_size=test_ratio,\n",
    "        random_state=random_state,\n",
    "        # Now, stratify_labels has the correct length (N_patients) and order \n",
    "        # corresponding to the unique_patients array.\n",
    "        stratify=dataframe[endpoint] \n",
    "    )\n",
    "    \n",
    "    # --- 3. Create masks and split the full dataframe ---\n",
    "    train_mask = dataframe[identifier].isin(train_patients)\n",
    "    test_mask = dataframe[identifier].isin(test_patients)\n",
    "    \n",
    "    cols_to_drop = [endpoint]\n",
    "    if identifier is not None:\n",
    "        cols_to_drop.append(identifier)\n",
    "\n",
    "\n",
    "    # Split the data\n",
    "    x_train = dataframe[train_mask].drop(columns=cols_to_drop, axis=1)\n",
    "    x_test = dataframe[test_mask].drop(columns=cols_to_drop, axis=1)\n",
    "    y_train = dataframe[train_mask][endpoint]\n",
    "    y_test = dataframe[test_mask][endpoint]\n",
    "\n",
    "    print(f\"Train Set Patient Count: {len(train_patients)}\")\n",
    "    print(f\"Test Set Patient Count: {len(test_patients)}\")\n",
    "\n",
    "    print(f\"Train Set Endpoint Distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"Test Set Endpoint Distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute common classification metrics for binary classification.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "    - Accuracy\n",
    "    - F1 Score\n",
    "    - Precision\n",
    "    - Sensitivity (Recall)\n",
    "    - Specificity\n",
    "    - Confusion Matrix\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Sensitivity (Recall)': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Confusion Matrix': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7b0d5",
   "metadata": {},
   "source": [
    "Standardization of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MissingValueColumnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        self.keep_features_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        missing_frac = X.isna().mean()\n",
    "        self.keep_features_ = missing_frac[missing_frac <= self.threshold].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        # Only keep columns selected during fit\n",
    "        return X[self.keep_features_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e01e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CleanFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_thresh=0.3, variance_thresh=1e-6):\n",
    "        self.missing_thresh = missing_thresh\n",
    "        self.variance_thresh = variance_thresh\n",
    "        self.keep_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # print(\"from clean feature selector : X\")\n",
    "        # print(X)\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        # Drop by missing %\n",
    "        keep_missing = X.isna().mean() < self.missing_thresh\n",
    "        X2 = X.loc[:, keep_missing]\n",
    "\n",
    "        # Drop by variance\n",
    "        var = X2.var()\n",
    "        keep_var = var > self.variance_thresh\n",
    "\n",
    "        self.keep_cols_ = X2.columns[keep_var].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        print(X[self.keep_cols_])\n",
    "        return X[self.keep_cols_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "\n",
    "class FeatureFilterer:\n",
    "    def __init__(self, responder_idx, nonresponder_idx, filter_method_name = \"mannwhitney\", p_val_threshold=0.05):\n",
    "        self.p_val_threshold = p_val_threshold\n",
    "        self.responder_idx = responder_idx\n",
    "        self.nonresponder_idx = nonresponder_idx\n",
    "        self.filter_method_name = filter_method_name\n",
    "        self.kept_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # X is already a DataFrame here if CleanFeatureSelector keeps it that way\n",
    "        X_df = pd.DataFrame(X, index=self.responder_idx.union(self.nonresponder_idx))\n",
    "\n",
    "        x_responder = X_df.loc[self.responder_idx]\n",
    "        x_nonresponder = X_df.loc[self.nonresponder_idx]\n",
    "\n",
    "        self.significant_features = []\n",
    "\n",
    "        statistical_test = None\n",
    "\n",
    "        if self.filter_method_name == \"mannwhitney\":\n",
    "            statistical_test = mannwhitneyu\n",
    "\n",
    "        elif self.filter_method_name == \"kruskal\":\n",
    "            statistical_test = kruskal\n",
    "        \n",
    "        elif self.filter_method_name == \"wilcoxon\":\n",
    "            statistical_test = kruskal\n",
    "\n",
    "        for col in X_df.columns:\n",
    "            try:\n",
    "                _, p = statistical_test(\n",
    "                    x_responder[col].dropna(),\n",
    "                    x_nonresponder[col].dropna()\n",
    "                )\n",
    "                if p < self.p_val_threshold:\n",
    "                    self.significant_features.append(col)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        print(self.significant_features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        return X_df[self.significant_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # record input column names\n",
    "        self.columns_ = X.columns if hasattr(X, \"columns\") else self.columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.columns_, index=getattr(X, \"index\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# combined = pd.merge(\n",
    "#     left=radiomics_df,\n",
    "#     right= clin_df.drop(columns=['TNM']),\n",
    "#     on=\"Subject ID\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "\n",
    "identifier = radiomics_df_final[\"TCIA_ID\"]\n",
    "radiomics_df_final = radiomics_df_final.dropna(axis= 0, subset=[\"responder\"])\n",
    "\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split_patients(\n",
    "    radiomics_df_final, \n",
    "    random_state=train_test_seed,\n",
    "    identifier=\"TCIA_ID\",\n",
    "    endpoint=\"responder\", \n",
    "    test_ratio=train_test_ratio\n",
    ")\n",
    "x_tr_index = x_tr.index\n",
    "x_ts_index = x_ts.index\n",
    "x_cols = x_tr.columns\n",
    "\n",
    "responder_indices = y_tr[y_tr == 1].index\n",
    "nonresponder_indices = y_tr[y_tr == 0].index\n",
    "\n",
    "preprocess_pipe = Pipeline(steps=[\n",
    "    (\"screening\", CleanFeatureSelector()),\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# Transform\n",
    "x_tr_transformed = preprocess_pipe.fit_transform(x_tr)\n",
    "x_ts_transformed = preprocess_pipe.transform(x_ts)\n",
    "\n",
    "# Extract kept columns\n",
    "# screening_cols = preprocess_pipe.named_steps['filter'].keep_cols\n",
    "kept_cols = preprocess_pipe.named_steps['screening'].keep_cols_\n",
    "\n",
    "# Final aligned DataFrames\n",
    "x_tr = pd.DataFrame(x_tr_transformed, index=x_tr.index, columns=kept_cols)\n",
    "x_ts = pd.DataFrame(x_ts_transformed, index=x_ts.index, columns=kept_cols)\n",
    "\n",
    "# Now you can use x_tr_final and x_ts_final with their original indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc784ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_filter_pipe = Pipeline(steps=[\n",
    "    (\"filter\", FeatureFilterer(\n",
    "        p_val_threshold=0.2,\n",
    "        responder_idx=responder_indices,\n",
    "        nonresponder_idx=nonresponder_indices\n",
    "    )),\n",
    "    \n",
    "])\n",
    "\n",
    "# Fit on TRAINING DATA ONLY (after screening + impute)\n",
    "x_tr_filtered = feature_filter_pipe.fit_transform(x_tr)\n",
    "x_ts_filtered = feature_filter_pipe.transform(x_ts)\n",
    "\n",
    "# Extract feature names\n",
    "kept_cols = feature_filter_pipe.named_steps[\"filter\"].significant_features\n",
    "\n",
    "# Rebuild DataFrame\n",
    "x_tr = pd.DataFrame(x_tr_filtered, index=x_tr.index, columns=kept_cols)\n",
    "x_ts = pd.DataFrame(x_ts_filtered, index=x_ts.index, columns=kept_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdcc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "responder_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548d3f5",
   "metadata": {},
   "source": [
    "Univariate Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MRMRSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, K=50, show_progress=False):\n",
    "        self.K = K\n",
    "        self.show_progress = show_progress\n",
    "        self.selected_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # mrmr_classif automatically uses mutual information and redundancy internally\n",
    "        self.selected_features_ = mrmr_classif(\n",
    "            X, y,\n",
    "            K=self.K,\n",
    "            show_progress=self.show_progress\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure that the data type supports column selection\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X[self.selected_features_]\n",
    "        else:\n",
    "            raise TypeError(\"MRMRSelector expects a pandas DataFrame as input.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628275e9",
   "metadata": {},
   "source": [
    "### Clinical Filtering Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f76356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to standardize\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "x_tr_clin, x_ts_clin, y_tr_clin, y_ts_clin = train_test_split_patients(\n",
    "    random_state=train_test_seed,\n",
    "    dataframe=clin_df.drop(columns=[\"Interval_BL\"]),\n",
    "    identifier = \"TCIA_ID\",\n",
    "    endpoint = \"responder\", \n",
    "    test_ratio=train_test_ratio\n",
    ")\n",
    "cols_to_standardize = [\"age\", \"Tr_Size\", \"AFP\"]\n",
    "binary_cols = [col for col in x_tr_clin.columns if col not in cols_to_standardize]\n",
    "\n",
    "# Create separate pipelines for numeric and binary columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# For binary columns, we'll just use a simple imputer\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_standardize),\n",
    "        ('binary', 'passthrough', binary_cols)  # 'passthrough' means leave these columns as-is\n",
    "    ])\n",
    "\n",
    "# Transform the data\n",
    "x_tr_clin_transformed = preprocessor.fit_transform(x_tr_clin)\n",
    "x_ts_clin_transformed = preprocessor.transform(x_ts_clin)\n",
    "\n",
    "# Get feature names after transformation\n",
    "numeric_feature_names = preprocessor.named_transformers_['num'].get_feature_names_out(cols_to_standardize)\n",
    "all_feature_names = list(numeric_feature_names) + binary_cols\n",
    "\n",
    "# Create DataFrames with correct column names\n",
    "x_tr_clin = pd.DataFrame(x_tr_clin_transformed, columns=all_feature_names, index=x_tr_clin.index)\n",
    "x_ts_clin = pd.DataFrame(x_ts_clin_transformed, columns=all_feature_names, index=x_ts_clin.index)\n",
    "\n",
    "\n",
    "print(\"Training set shape:\", x_tr_clin.shape)\n",
    "print(\"Test set shape:\", x_ts_clin.shape)\n",
    "print(\"\\nColumns being scaled:\", cols_to_standardize)\n",
    "print(\"\\nBinary columns (not scaled):\", binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78057c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts_clin.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7afa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier #\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# logreg = LogisticRegression(\n",
    "#     max_iter= 5000, \n",
    "#     # C=10.0,\n",
    "#     class_weight= \"balanced\"\n",
    "# )\n",
    "\n",
    "lr_param_grid = {\n",
    "  \"C\" :[0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0],\n",
    "  \"penalty\" : [\"l1\", \"l2\"],\n",
    "  \"solver\" : [\"liblinear\", \"saga\"],\n",
    "  \"class_weight\" : [\n",
    "    \"balanced\",\n",
    "    {0:1, 1:2},\n",
    "    {0:1, 1:5},\n",
    "    {0:1, 1:10}\n",
    "  \n",
    "  ]\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum()]  # handles class imbalance\n",
    "}\n",
    "\n",
    "linear_svc_params = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "    'penalty': ['l1'],\n",
    "    'loss': ['squared_hinge'],\n",
    "    'dual': [False],\n",
    "    'class_weight': [None, 'balanced', {0:1, 1: 2}, {0:1, 1: 5}, {0:1, 1: 10}, {0:1, 1: 20}],\n",
    "    'max_iter': [5000, 10000, 20000],\n",
    "    'random_state': [42],\n",
    "    'intercept_scaling': [0.5, 1.0, 2.0, 3.0]\n",
    "}\n",
    "# Support Vector Machine\n",
    "svm_params = {\n",
    "    'C': np.logspace(-2, 1, 10),\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'class_weight': ['balanced', {0:1, 1: 5}, {0: 1, 1: 10}]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(100, 50), (100,50,25), (128,64), (128, 64, 32), (64,32), (64,32,16)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': np.logspace(2, 4, 10),\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 500, 1000],\n",
    "    'early_stopping': [True],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "adaboost_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'estimator': [\n",
    "        DecisionTreeClassifier(max_depth=1, class_weight='balanced'), # Decision Stump\n",
    "        DecisionTreeClassifier(max_depth=2, class_weight='balanced'), \n",
    "        DecisionTreeClassifier(max_depth=3, class_weight='balanced')\n",
    "    ],\n",
    "    # AdaBoost does not have a native 'scale_pos_weight' parameter. \n",
    "    # Imbalance is typically handled by adjusting the class_weight of the base estimator \n",
    "    # or relying on the boosting mechanism itself.\n",
    "    # The DecisionTreeClassifier base estimator must handle the class_weight.\n",
    "    # Note: GridSearchCV handles the combination.\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 40], # Main complexity parameter for leaf-wise growth\n",
    "    'max_depth': [-1, 5, 8], # -1 means no limit\n",
    "    'min_child_samples': [20],\n",
    "    'subsample': [0.7, 0.9], # Row subsampling\n",
    "    'colsample_bytree': [0.7, 0.9], # Feature subsampling\n",
    "    'reg_alpha': [0, 0.1, 0.5], # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 0.5], # L2 regularization\n",
    "    # Handling Imbalance\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum(), 5, 10, 20]\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15, 21],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski'],\n",
    "    'p': [1, 2],  # Manhattan vs Euclidean\n",
    "    'leaf_size': [20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_wrapper(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sampling_method=None,\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    if sampling_method is None:\n",
    "        return x_train, y_train\n",
    "\n",
    "    try:\n",
    "        # Get column names before sampling\n",
    "        feature_columns = x_train.columns\n",
    "        target_name = y_train.name if hasattr(y_train, 'name') else 'target'\n",
    "        \n",
    "        # Apply sampling\n",
    "        sampler = sampling_method\n",
    "        x_tr_res, y_tr_res = sampler.fit_resample(x_train, y_train)\n",
    "        \n",
    "        # Convert back to DataFrame/Series with original column names\n",
    "        x_tr_res = pd.DataFrame(x_tr_res, columns=feature_columns)\n",
    "        y_tr_res = pd.Series(y_tr_res, name=target_name)\n",
    "        \n",
    "        return x_tr_res, y_tr_res\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sampling: {str(e)}\")\n",
    "        return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(1.5, 2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def sparse_pca_wrapper(x_train, y_train, estimator, param_grid=None,\n",
    "                      n_components=None, min_feats=5, alpha=1, ridge_alpha=0.01, \n",
    "                      max_iter=1000, n_runs=5, cv=5, scoring='accuracy', \n",
    "                      random_state=42, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Sparse PCA wrapper with integrated cross-validation for feature selection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : array-like of shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : array-like of shape (n_samples,)\n",
    "        Target values\n",
    "    estimator : estimator object\n",
    "        A scikit-learn estimator that implements 'fit' and 'predict'\n",
    "    param_grid : dict, optional\n",
    "        Dictionary with parameters names as keys and lists of parameter settings to try\n",
    "    n_components : int, default=None\n",
    "        Number of sparse components to extract\n",
    "    alpha : float, default=1\n",
    "        Sparsity controlling parameter\n",
    "    ridge_alpha : float, default=0.01\n",
    "        Amount of ridge shrinkage\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    n_runs : int, default=5\n",
    "        Number of runs for stability analysis\n",
    "    cv : int, cross-validation generator, default=5\n",
    "        Determines cross-validation splitting strategy\n",
    "    scoring : str, callable, default='accuracy'\n",
    "        Scoring metric\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict:\n",
    "        Dictionary containing:\n",
    "        - 'best_estimator': Best model from GridSearchCV\n",
    "        - 'best_params': Best parameters from GridSearchCV\n",
    "        - 'cv_results': Cross-validation results\n",
    "        - 'feature_importances': Feature importances if available\n",
    "        - 'stability_scores': Stability of feature selection across runs\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedKFold\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    if param_grid is None:\n",
    "        param_grid = {}  # Default empty param grid\n",
    "    \n",
    "    # Store results from all runs\n",
    "    all_importances = []\n",
    "    best_estimators = []\n",
    "    \n",
    "    # Run multiple times for stability analysis\n",
    "    for run in tqdm(range(n_runs), desc=\"Running stability analysis\"):\n",
    "        # Set random state for this run\n",
    "        current_seed = random_state + run if random_state is not None else None\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=current_seed)\n",
    "        \n",
    "        # Set up GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv_splitter,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        \n",
    "        # Store best estimator\n",
    "        best_estimators.append(grid_search.best_estimator_)\n",
    "        \n",
    "        # Get feature importances if available\n",
    "        if hasattr(grid_search.best_estimator_, 'feature_importances_'):\n",
    "            importances = grid_search.best_estimator_.feature_importances_\n",
    "            all_importances.append(importances)\n",
    "        elif hasattr(grid_search.best_estimator_, 'coef_'):\n",
    "            # For linear models\n",
    "            importances = np.abs(grid_search.best_estimator_.coef_).mean(axis=0)\n",
    "            all_importances.append(importances)\n",
    "    \n",
    "    # Calculate stability scores if we have multiple runs\n",
    "    stability_scores = None\n",
    "    if n_runs > 1 and all_importances:\n",
    "        # Convert to numpy array for calculations\n",
    "        all_importances = np.array(all_importances)\n",
    "        \n",
    "        # Calculate stability as coefficient of variation (lower is more stable)\n",
    "        stability_scores = np.std(all_importances, axis=0) / (np.mean(all_importances, axis=0) + 1e-10)\n",
    "    \n",
    "    # Get the best model from the last run\n",
    "    best_estimator = best_estimators[-1]\n",
    "    # Get feature importances from the best model\n",
    "    feature_importances = None\n",
    "    if hasattr(best_estimator, 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_estimator.feature_importances_,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    elif hasattr(best_estimator, 'coef_'):\n",
    "        # For linear models\n",
    "        coef = best_estimator.coef_\n",
    "        if len(coef.shape) > 1:  # For multi-class\n",
    "            coef = np.abs(coef).mean(axis=0)\n",
    "        feature_importances = pd.Series(\n",
    "            coef,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    \n",
    "\n",
    "    print(feature_importances)\n",
    "    return {\n",
    "        'best_estimator': best_estimator,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features' : feature_importances.nlargest(min_feats).index,\n",
    "        'stability_scores': stability_scores,\n",
    "        'all_importances': all_importances\n",
    "    }\n",
    "\n",
    "def pca_wrapper():\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # The loadings are here!\n",
    "    loadings = pca.components_\n",
    "\n",
    "\n",
    "def lasso_wrapper(x_train, y_train, max_feature = 100,  mlp_setting = False):\n",
    "    # Define the parameter grid for logistic regression\n",
    "    print(\"original column length\", len(x_train.columns))\n",
    "    regularization_grid = None\n",
    "\n",
    "    if not mlp_setting:\n",
    "        regularization_grid = np.logspace(-6, 1, 10)\n",
    "    else:\n",
    "        regularization_grid = np.logspace(2.5, 3.5, 10)\n",
    "\n",
    "        \n",
    "    param_grid = {\n",
    "        'Cs': regularization_grid,  # Regularization strength,  # Number of regularization values to try\n",
    "        'penalty': 'l1',\n",
    "        'solver': 'saga',\n",
    "        'cv': 5,\n",
    "        'scoring': 'average_precision',\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced',\n",
    "        'max_iter': 10000,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    # Create and fit LogisticRegressionCV\n",
    "    lr_cv = LogisticRegressionCV(**param_grid)\n",
    "    lr_cv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    print(f\"Best C: {lr_cv.C_[0]:.4f}\")\n",
    "    print(f\"Best penalty: {lr_cv.penalty}\")\n",
    "    print(f\"Best cross-validated score: {lr_cv.scores_[1].mean(axis=0).max():.4f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'importance': np.abs(lr_cv.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    selected_top_n_features = feature_importance.nlargest(max_feature, 'importance')[\"feature\"].tolist()\n",
    "\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "\n",
    "    return {\n",
    "        \"best_model\": lr_cv,\n",
    "        \"feature_importance\": feature_importance,\n",
    "        \"selected_features\": selected_top_n_features\n",
    "    }\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def sfs_wrapper(estimator, x_train, y_train, max_feats=None, min_feats=10):\n",
    "    \"\"\"\n",
    "    Performs Sequential Forward Selection (SFS) using RepeatedStratifiedKFold CV\n",
    "    to select the most stable and performant features.\n",
    "\n",
    "    Args:\n",
    "        estimator: The unfitted model (e.g., LogisticRegression).\n",
    "        x_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        max_feats (int): Maximum number of features to select.\n",
    "        min_feats (int): Minimum number of features to select (optional stopping point).\n",
    "\n",
    "    Returns:\n",
    "        list: The list of selected feature names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Define the Robust Cross-Validation Strategy ---\n",
    "    # Using 10 folds repeated 3 times is a common, stable choice.\n",
    "    # Setting random_state is CRUCIAL for reproducibility of the folds.\n",
    "    rskf = RepeatedStratifiedKFold(\n",
    "        n_splits=10, \n",
    "        n_repeats=3, \n",
    "        random_state=42 \n",
    "    )\n",
    "\n",
    "    # --- 2. Initialize the Sequential Feature Selector ---\n",
    "    # We use Forward Selection ('forward') to build the feature set.\n",
    "    # We use 'average_precision' as the scoring metric for imbalanced data.\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=clone(estimator), # Clone ensures a clean estimator for each run\n",
    "        n_features_to_select=\"auto\" if max_feats == None else max_feats,\n",
    "        direction='forward',\n",
    "        scoring='average_precision',\n",
    "        cv=rskf, # Pass the robust CV object here!\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # --- 3. Fit the Selector to the Data ---\n",
    "    # SFS internally performs the cross-validation using rskf at every step\n",
    "    # to evaluate which feature provides the best, most stable performance gain.\n",
    "    sfs.fit(x_train, y_train)\n",
    "\n",
    "    # --- 4. Extract and Return Results ---\n",
    "    selected_features_mask = sfs.get_support()\n",
    "    selected_feature_names = list(x_train.columns[selected_features_mask])\n",
    "  \n",
    "\n",
    "    return {\n",
    "        \"selected_features\" : selected_feature_names\n",
    "    }    \n",
    "def rfecv_wrapper(estimator, x_train, y_train, max_feats = 50, min_feats = 10):\n",
    "    # Ensure x_train is a DataFrame to access column names\n",
    "    # if not hasattr(x_train, 'columns'):\n",
    "    #     x_train = pd.DataFrame(x_train)\n",
    "    \n",
    "    rfecv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=5,\n",
    "        cv=StratifiedKFold(3),\n",
    "        scoring='average_precision',\n",
    "        n_jobs=-1, \n",
    "        min_features_to_select=min_feats,\n",
    "    )\n",
    "\n",
    "    rfecv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get the selected features by name\n",
    "\n",
    "\n",
    "    \n",
    "    # Get feature rankings with names\n",
    "# First, ensure the features are sorted by importance (best rank first)\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'ranking': rfecv.ranking_,\n",
    "        'support': rfecv.support_\n",
    "    }).sort_values('ranking')  # Sort by ranking (lower rank = more important)\n",
    "\n",
    "    # Keep only the top max_feats features\n",
    "\n",
    "    if max_feats != None:\n",
    "        feature_ranking['support'] = feature_ranking.index < max_feats\n",
    "\n",
    "    # Update selected_features to only include the top max_feats features\n",
    "    selected_features = feature_ranking[feature_ranking['support']]['feature'].tolist()\n",
    "\n",
    "    # Print some information\n",
    "    print(f\"Selected top {len(selected_features)} features out of {len(feature_ranking)}\")\n",
    "    print(\"Selected features:\", selected_features)\n",
    "            \n",
    "    # Get cross-validation scores for each number of features\n",
    "    cv_scores = pd.DataFrame({\n",
    "        'n_features': range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n",
    "        'mean_score': rfecv.cv_results_['mean_test_score'],\n",
    "        'std_score': rfecv.cv_results_['std_test_score']\n",
    "    })\n",
    "    \n",
    "    # selected_features = [x_train.columns[col] for col in selected_features]\n",
    "\n",
    "    print(\"Selected features:\", selected_features)\n",
    "\n",
    "    return {\n",
    "        'selected_features': list(selected_features),\n",
    "        'feature_ranking': feature_ranking,\n",
    "        'cv_scores': cv_scores,\n",
    "        'optimal_n_features': rfecv.n_features_,\n",
    "        'rfecv': rfecv\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "pipeline_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if pipeline_path not in sys.path:\n",
    "    sys.path.append(pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_metrics(gs_cv_results, n_splits=4, title='Cross-Validation Metrics'):\n",
    "  \n",
    "    # Extract metrics for each fold\n",
    "    metrics = []\n",
    "    for i in range(1, n_splits):\n",
    "        fold_metrics = {\n",
    "            'Fold': i+1,\n",
    "            'Sensitivity': gs_cv_results[f'split{i}_test_sensitivity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Specificity': gs_cv_results[f'split{i}_test_specificity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Accuracy': gs_cv_results[f'split{i}_test_accuracy'][gs_cv_results['rank_test_accuracy'].argmin()]\n",
    "        }\n",
    "        metrics.append(fold_metrics)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df_metrics = pd.DataFrame(metrics).melt('Fold', var_name='Metric', value_name='Score')\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Fold', y='Score', hue='Metric', data=df_metrics)\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print mean and std of metrics\n",
    "    print(f\"Mean Sensitivity: {df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Specificity: {df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Accuracy: {df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].std():.3f})\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_cv_metrics(gs.cv_results_, n_splits=4, title='Model Performance per Fold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from statistics import LinearRegression\n",
    "# from custom_models.svm_shap import SVMSHAP\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, average_precision_score, accuracy_score, roc_auc_score, recall_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=3,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# scores = cross_validate(\n",
    "#     model,\n",
    "#     X,\n",
    "#     y,\n",
    "#     cv=cv,\n",
    "#     scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\"],\n",
    "#     n_jobs=-1,\n",
    "#     return_estimator=False,\n",
    "#     return_train_score=False\n",
    "# )\n",
    "\n",
    "# print(scores[\"test_roc_auc\"].mean(), scores[\"test_roc_auc\"].std())\n",
    "\n",
    "\n",
    "sensitivity_scorer = make_scorer(\n",
    "\n",
    "    recall_score,\n",
    "    greater_is_better=True, \n",
    "    pos_label = 1\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# SVM\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    svm_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        'f1': 'f1_weighted'  #  Most common choice\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    refit=\"average_precision\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "linear_svc_grid= GridSearchCV(\n",
    "    LinearSVC(random_state=42),\n",
    "    linear_svc_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision' : 'precision',\n",
    "        'recall' : 'recall',\n",
    "        'f1': 'f1_weighted'  #  Most common choice\n",
    "    },    \n",
    "    refit=\"average_precision\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extra Trees\n",
    "et_grid = GridSearchCV(\n",
    "    ExtraTreesClassifier(random_state=42),\n",
    "    et_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  #  Most common choice\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=cv,\n",
    "    refit='average_precision',\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  #  Most common choice\n",
    "    },\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "gs = GridSearchCV(\n",
    "  param_grid=lr_param_grid,\n",
    "  cv = cv,\n",
    "  estimator=LogisticRegression(random_state=42),\n",
    "  refit=\"accuracy\",\n",
    "  scoring={\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'average_precision': 'average_precision',\n",
    "    'accuracy': 'accuracy'\n",
    "  },\n",
    ")\n",
    "\n",
    "# catboost_grid = GridSearchCV(\n",
    "#     CatBoostClassifier(random_seed=42, verbose=0), # verbose=0 suppresses training logs\n",
    "#     catboost_params,\n",
    "#     cv=cv,\n",
    "#     scoring='average_precision',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "adaboost_grid = GridSearchCV(\n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),\n",
    "    adaboost_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "    },    \n",
    "    refit='average_precision',  # Specify which metric to use for refitting\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "lgbm_grid = GridSearchCV(\n",
    "    LGBMClassifier(random_state=42, n_jobs=-1, objective='binary', metric='binary_logloss'),\n",
    "    lgbm_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsClassifier(n_neighbors = 3),\n",
    "    knn_params, \n",
    "    cv= cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "    },\n",
    "    refit=\"average_precision\",\n",
    "    n_jobs=-1,  # Note: MLP doesn't support n_jobs > 1\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "mlp_grid = GridSearchCV(\n",
    "    MLPClassifier(),\n",
    "    mlp_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,  # Note: MLP doesn't support n_jobs > 1\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_estimator_params = {\n",
    "    \"rf\": rf_grid,\n",
    "    \"logreg\": gs,\n",
    "    \"svm\" : svm_grid,\n",
    "    \"ada\": adaboost_grid,\n",
    "}\n",
    "\n",
    "\n",
    "voting_params = {\n",
    "    f\"{key}__{param}\": value \n",
    "    for key, estimator in voting_estimator_params.items()\n",
    "    for param, value in estimator.param_grid.items()    \n",
    "}\n",
    "\n",
    "voting_params[\"voting\"] = [\"hard\", \"soft\"]\n",
    "\n",
    "print(voting_params)\n",
    "\n",
    "voting_grid = GridSearchCV(\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            # (\"rf\", RandomForestClassifier(random_state=42)),\n",
    "            (\"logreg\", LogisticRegression(random_state=42)),\n",
    "            (\"svm\", SVC(random_state=42)),\n",
    "            (\"ada\", AdaBoostClassifier(random_state=42))\n",
    "        ]),\n",
    "    param_grid=voting_params, cv=cv, verbose=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "stacking_params = {\n",
    "    f\"{key}__{param}\": value \n",
    "    for key, estimator in voting_estimator_params.items()\n",
    "    for param, value in estimator.param_grid.items()    \n",
    "}\n",
    "    \n",
    "# stacking_grid = \n",
    "\n",
    "grids = [\n",
    "    xgb_grid,        # XGBoost GridSearchCV\n",
    "    svm_grid,        # SVM GridSearchCV\n",
    "    linear_svc_grid, # Linear SVC GridSearchCV\n",
    "    et_grid,         # Extra Trees GridSearchCV\n",
    "    rf_grid, \n",
    "    mlp_grid,        # Random Forest GridSearchCV\n",
    "    gs               # Logistic Regression GridSearchCV\n",
    "]\n",
    "\n",
    "\n",
    "#======= FS WRAPPER ========\n",
    "\n",
    "# current_gs = linear_svc_grid\n",
    "# current_gs.fit(x_tr, y_tr)\n",
    "# # Get the best estimator from GridSearchCV\n",
    "# best_lr = current_gs.best_estimator_\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = best_lr.predict(x_ts[feats])\n",
    "# # Generate and print the classification report\n",
    "# print(\"Best Parameters:\", current_gs.best_params_)\n",
    "# print(\"\\nClassification Report for Best Model:\")\n",
    "# print(classification_report(y_ts, y_pred))\n",
    "# print(confusion_matrix(y_ts,y_pred))\n",
    "# print(\"model accuracy:\", accuracy_score(y_ts, y_pred))\n",
    "\n",
    "# x_ts_filtered = x_ts[feats]\n",
    "\n",
    "# try:\n",
    "#     # Pass the filtered TEST FEATURES (x_ts[feats]) to predict_proba\n",
    "#     y_prob = best_lr.predict_proba(x_ts_filtered)\n",
    "    \n",
    "#     # NOTE: roc_auc_score requires probabilities for the positive class (column 1)\n",
    "#     # The output of predict_proba is usually (N_samples, 2), so we take all rows and column index 1\n",
    "#     model_proba = y_prob[:, 1]\n",
    "    \n",
    "#     print(\"model roc auc\", roc_auc_score(y_ts.to_numpy(), model_proba))\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(\"cannot print proba\")\n",
    "\n",
    "# print(current_gs.cv_results_ )\n",
    "# # plot_cv_metrics(current_gs.cv_results_, n_splits=3, title='Your Model Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d062eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score, accuracy_score, classification_report\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def save_model_results(grid, x_tr, y_tr, x_ts, y_ts, feats, \n",
    "                      feature_selection_name='', filter_algorithm_name='',\n",
    "                      use_decision_function = False, \n",
    "                      output_file='model_results.xlsx', has_clin_data = True):\n",
    "    \"\"\"\n",
    "    Save model results including CV metrics (from GridSearchCV) and test set performance.\n",
    "    Expects a GridSearchCV instance. Will fit the grid if not already fitted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure grid is fitted (force surfacing of real errors)\n",
    "        if not hasattr(grid, 'best_estimator_'):\n",
    "            # If possible, flip error_score to raise to see root cause early\n",
    "            try:\n",
    "                grid.error_score = 'raise'\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(\"Fitting GridSearchCV...\")\n",
    "            grid.fit(x_tr[feats], y_tr)\n",
    "\n",
    "        best_estimator = grid.best_estimator_\n",
    "        cv_results = grid.cv_results_\n",
    "        best_params = grid.best_params_\n",
    "        best_idx = getattr(grid, 'best_index_', None)\n",
    "        model_name = type(best_estimator).__name__\n",
    "\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        run_id = f\"{model_name}_{feature_selection_name}_{filter_algorithm_name}_{timestamp}\"\n",
    "\n",
    "        # Extract CV metrics robustly\n",
    "        # Primary CV score comes from grid.best_score_ regardless of scoring name\n",
    "        primary_cv = getattr(grid, 'best_score_', None)\n",
    "\n",
    "        def _get_cv_stat(key_mean, key_std=None):\n",
    "            if key_mean in cv_results:\n",
    "                mean_val = cv_results[key_mean][best_idx] if best_idx is not None else np.nan\n",
    "                if key_std and key_std in cv_results:\n",
    "                    std_val = cv_results[key_std][best_idx]\n",
    "                    return f\"{mean_val:.4f}  {std_val:.4f}\"\n",
    "                return f\"{mean_val:.4f}\"\n",
    "            return 'N/A'\n",
    "\n",
    "        # Try to read explicit metric keys if you used a scoring dict when creating the grid\n",
    "        cv_metrics = {\n",
    "            'cv_primary_metric': f\"{primary_cv:.4f}\" if primary_cv is not None else 'N/A',\n",
    "            'cv_train_accuracy': _get_cv_stat('mean_train_accuracy', 'std_train_accuracy'),\n",
    "            'cv_test_accuracy':  _get_cv_stat('mean_test_accuracy',  'std_test_accuracy'),\n",
    "            'cv_train_roc_auc':  _get_cv_stat('mean_train_roc_auc',  'std_train_roc_auc'),\n",
    "            'cv_test_roc_auc':   _get_cv_stat('mean_test_roc_auc',   'std_test_roc_auc'),\n",
    "            'cv_train_ap':       _get_cv_stat('mean_train_average_precision', 'std_train_average_precision'),\n",
    "            'cv_test_ap':        _get_cv_stat('mean_test_average_precision',  'std_test_average_precision'),\n",
    "            # Fallback if you passed a single scoring like 'average_precision'\n",
    "            'cv_test_score':     _get_cv_stat('mean_test_score', 'std_test_score')\n",
    "        }\n",
    "\n",
    "        # Test set predictions\n",
    "        y_pred = best_estimator.predict(x_ts[feats])\n",
    "        if use_decision_function ==False:\n",
    "            y_prob = best_estimator.predict_proba(x_ts[feats])[:, 1] if hasattr(best_estimator, 'predict_proba') else None\n",
    "\n",
    "        else:\n",
    "            y_prob = best_estimator.decision_function(x_ts[feats])\n",
    "\n",
    "        test_metrics = {\n",
    "            'test_accuracy': accuracy_score(y_ts, y_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "            'test_average_precision': average_precision_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "        }\n",
    "\n",
    "        clf_report = classification_report(y_ts, y_pred, output_dict=True)\n",
    "\n",
    "        metrics_data = {\n",
    "            'Run_ID': run_id,\n",
    "            'Feature Type': \"Combined\" if has_clin_data else \"Radiomic only\",\n",
    "\n",
    "            'Model': model_name,\n",
    "            'Feature_Selection': feature_selection_name,\n",
    "            'Filter_Algorithm': filter_algorithm_name,\n",
    "            'Num_Features': len(feats),\n",
    "            'Timestamp': timestamp,\n",
    "            'CV_Strategy': 'GridSearchCV',\n",
    "            **cv_metrics,\n",
    "            **{k: v for k, v in test_metrics.items()},\n",
    "            'Features_Used': str(feats),\n",
    "            'Model_Params': str(best_params)\n",
    "        }\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "\n",
    "        clf_df = pd.DataFrame(clf_report).transpose()\n",
    "        clf_df['Run_ID'] = run_id\n",
    "        clf_df = clf_df.reset_index().rename(columns={'index': 'class'})\n",
    "\n",
    "        # Save to Excel\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                try:\n",
    "                    existing_metrics = pd.read_excel(writer, sheet_name='Metrics')\n",
    "                    metrics_df = pd.concat([existing_metrics, metrics_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "                try:\n",
    "                    existing_clf = pd.read_excel(writer, sheet_name='Classification_Reports')\n",
    "                    clf_df = pd.concat([existing_clf, clf_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "        except Exception:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return metrics_df, clf_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab54437",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemo_columns = [col for col in clin_df.columns if col.startswith('chemo_')]\n",
    "\n",
    "x_ts_clin[chemo_columns].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306e470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2757c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, run the lasso_wrapper to get the selected features\n",
    "from custom_models.svm_shap import SVMSHAP\n",
    "from boruta import boruta_py\n",
    "\n",
    "lasso_result = lasso_wrapper(\n",
    "    \n",
    "    x_train=x_tr,\n",
    "    y_train=y_tr,\n",
    "    max_feature=50,\n",
    "    mlp_setting=False\n",
    ")\n",
    "\n",
    "# Get the selected features from Lasso\n",
    "selected_features_lasso = lasso_result[\"selected_features\"]\n",
    "\n",
    "# Use the selected features from Lasso as input to RFECV\n",
    "# rfecv_result = rfecv_wrapper(\n",
    "#     estimator= LogisticRegression(random_state=42),  # or your preferred estimator\n",
    "#     x_train=x_tr[selected_features_lasso],  # Use only Lasso-selected features\n",
    "#     y_train=y_tr,\n",
    "#     max_feats=15\n",
    "# )\n",
    "\n",
    "# sfscv_result = sfs_wrapper(\n",
    "#     estimator=LinearRegression(random_state=42),  # or your preferred estimator\n",
    "#     x_train=x_tr[selected_features_lasso],  # Use only Lasso-selected features\n",
    "#     y_train=y_tr,\n",
    "#     max_feats=\n",
    "# )\n",
    "\n",
    "# rfecv_result_clin = rfecv_wrapper(\n",
    "#     estimator=LogisticRegression(random_state=42),  # or your preferred estimator\n",
    "#     x_train=x_tr_clin,  # Use only Lasso-selected features\n",
    "#     y_train=y_tr_clin,\n",
    "#     max_feats=4\n",
    "# )\n",
    "\n",
    "# selected_features_sfs_rad = rfecv_result[\"selected_features\"]\n",
    "\n",
    "# feature_ranking_rfe = rfecv_result[\"feature_ranking\"]\n",
    "\n",
    "# selected_features_sfs_rad = list(feature_ranking_rfe['feature'].values)\n",
    "# selected_features_sfs_clin = rfecv_result_clin[\"selected_features\"]\n",
    "\n",
    "radiomics_df_train = x_tr[selected_features_lasso]\n",
    "# radiomics_df_train_clin = x_tr_clin[selected_features_sfs_clin]\n",
    "\n",
    "# radiomics_df_train = x_tr[selected_features_lasso]\n",
    "# radiomics_df_train_clin = x_tr_clin\n",
    "# For training data\n",
    "# Reset indices to ensure they're sequential numbers\n",
    "radiomics_df_train = radiomics_df_train.reset_index(drop=True)\n",
    "# radiomics_df_train_clin = radiomics_df_train_clin.reset_index(drop=True)\n",
    "x_tr_clin = x_tr_clin.reset_index(drop=True)\n",
    "\n",
    "# Concatenate training data\n",
    "tr_final = pd.concat([\n",
    "    radiomics_df_train,\n",
    "    # radiomics_df_train_clin,\n",
    "    x_tr_clin[chemo_columns]\n",
    "], axis=1)\n",
    "\n",
    "# For test data\n",
    "# Reset indices to ensure they're sequential numbers\n",
    "x_ts = x_ts.reset_index(drop=True)\n",
    "x_ts_clin = x_ts_clin.reset_index(drop=True)\n",
    "# feature_ranking_rfe = rfecv_result[\"feature_ranking\"]\n",
    "\n",
    "# Get the test data features\n",
    "test_radiomics = x_ts[selected_features_lasso].reset_index(drop=True)\n",
    "# test_clin = x_ts_clin[selected_features_sfs_clin].reset_index(drop=True)\n",
    "test_chemo = x_ts_clin[chemo_columns].reset_index(drop=True)\n",
    "\n",
    "# Concatenate test data\n",
    "ts_final = pd.concat([\n",
    "    test_radiomics,\n",
    "    # test_clin,\n",
    "    test_chemo\n",
    "], axis=1)\n",
    "\n",
    "# # Get the final selected features from RFECV\n",
    "# selected_features_rfecv = rfecv_result[\"selected_features\"]\n",
    "\n",
    "\n",
    "# print(\"RFECV selected features:\", selected_features_rfecv)\n",
    "# # Get the selected features\n",
    "# selected_features = result[\"selected_features\"]\n",
    "metrics_df, clf_report = save_model_results(\n",
    "    grid=linear_svc_grid,  # Pass the fitted GridSearchCV instance\n",
    "    x_tr=tr_final,\n",
    "    y_tr=y_tr,\n",
    "    x_ts=ts_final,\n",
    "    y_ts=y_ts,\n",
    "    feats=tr_final.columns,\n",
    "    has_clin_data=False,\n",
    "    use_decision_function=True,\n",
    "    feature_selection_name='Lasso_RFECV',  # or your feature selection method\n",
    "    filter_algorithm_name='MannWhitney',  # or your filter method\n",
    "    output_file='model_results_combined.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get top 20 most important features\n",
    "top_features = lasso_result[\"feature_importance\"].sort_values(\"importance\", ascending=False).head(21)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.barh(top_features['feature'].to_list(), top_features['importance'], color='skyblue')\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 21 Most Important Features from Lasso Model')\n",
    "plt.gca().invert_yaxis()  # Show highest importance at the top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = top_features['feature'].to_list()\n",
    "\n",
    "\n",
    "top_features['phase'] = top_features['feature'].apply(lambda x: x.split(\"_\")[0])\n",
    "top_features['Region'] = top_features['feature'].apply(lambda x: x.split(\"_\")[1])\n",
    "top_features['is_wavelet'] = (top_features['feature'].apply(lambda x: \"wavelet\" in x.split(\"_\")[2])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba52513",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_pvt = top_features.pivot_table(index='feature', columns='Region', values='importance', aggfunc='count')\n",
    "\n",
    "phase_pvt\n",
    "\n",
    "phases = ['Liver', 'Mass']\n",
    "counts = [9 ,11]\n",
    "plt.bar(\n",
    "    phases,\n",
    "    counts,\n",
    "    color='lightblue',\n",
    "    label='Train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "top_feats_encoded = ohe.fit_transform(top_features[['Region', 'phase']])\n",
    "\n",
    "feature_names = ohe.get_feature_names_out(input_features=['Region', 'phase'])\n",
    "# Create DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(top_feats_encoded, columns=feature_names, index=top_features.index)\n",
    "encoded_df = pd.concat([encoded_df, top_features['is_wavelet']], axis=1)\n",
    "sns.heatmap(encoded_df.corr())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac27d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "for region in top_features[\"Region\"].unique():\n",
    "    table = pd.crosstab(\n",
    "        encoded_df[f\"Region_{region}\"],\n",
    "        encoded_df[[\"phase_arterial\", \"phase_pre-contrast\", \"phase_pv\"]].idxmax(axis=1)\n",
    "    )\n",
    "\n",
    "    chi2, p, dof, expected = chi2_contingency(table)\n",
    "\n",
    "    print(f\"p value of association between region {region} and phases : {p} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "##ORIGINAL \n",
    "# features = ['arterial_Mass_wavelet-LHH_glcm_SumEntropy',\n",
    "#        'pre-contrast_Liver_wavelet-LHH_glcm_Imc2',\n",
    "#        'pv_Liver_original_shape_Maximum2DDiameterColumn',\n",
    "#        'arterial_Liver_original_shape_Maximum2DDiameterColumn',\n",
    "#        'pv_Mass_wavelet-LLH_glszm_SmallAreaLowGrayLevelEmphasis',\n",
    "#        'pre-contrast_Mass_wavelet-HHH_glszm_LargeAreaLowGrayLevelEmphasis',\n",
    "#        'pre-contrast_Mass_wavelet-HHH_glszm_SizeZoneNonUniformityNormalized',\n",
    "#        'arterial_Mass_wavelet-HHL_firstorder_Median',\n",
    "#        'pv_Liver_wavelet-LHL_firstorder_Maximum',\n",
    "#        'pre-contrast_Mass_original_shape_Sphericity',\n",
    "#        'chemo_Cisplatin_doxorubicin_Mitomycin-C',\n",
    "#        'chemo_doxorubicin_LC_beads']\n",
    "\n",
    "\n",
    "\n",
    "# features = ['arterial_Mass_wavelet-HHL_gldm_SmallDependenceLowGrayLevelEmphasis',\n",
    "#        'pre-contrast_Mass_wavelet-HHL_firstorder_Median',\n",
    "#        'pre-contrast_Mass_wavelet-HHH_glszm_SizeZoneNonUniformityNormalized',\n",
    "#        'pre-contrast_Mass_wavelet-LHH_ngtdm_Contrast',\n",
    "#        'pv_Liver_wavelet-HLL_glszm_SizeZoneNonUniformityNormalized',\n",
    "#        'pre-contrast_Liver_wavelet-HHL_glszm_GrayLevelVariance',\n",
    "#        'pre-contrast_Mass_wavelet-LLH_firstorder_Kurtosis',\n",
    "#        'pv_Mass_wavelet-HHH_gldm_SmallDependenceLowGrayLevelEmphasis',\n",
    "#        'arterial_Mass_wavelet-LLL_glszm_ZonePercentage',\n",
    "#        'arterial_Mass_wavelet-HLH_glszm_SizeZoneNonUniformityNormalized',\n",
    "#        'pre-contrast_Mass_wavelet-HHH_ngtdm_Busyness',\n",
    "#        'pre-contrast_Mass_wavelet-LLH_glcm_Idmn',\n",
    "#        'pv_Mass_wavelet-LLL_glszm_LowGrayLevelZoneEmphasis',\n",
    "#        'arterial_Mass_wavelet-LLH_firstorder_Median',\n",
    "#        'arterial_Mass_wavelet-LHL_firstorder_Median',\n",
    "#        'chemo_Cisplatin_doxorubicin_Mitomycin-C',\n",
    "#        'chemo_doxorubicin_LC_beads']\n",
    "\n",
    "features = selected_features_lasso\n",
    "\n",
    "# Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1a79b",
   "metadata": {},
   "source": [
    "## Graphs for Reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_curve, roc_auc_score,\n",
    "    average_precision_score, accuracy_score, precision_recall_curve,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import  CalibratedClassifierCV\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "# --- Prepare data and model ---\n",
    "df = radiomics_df_final.copy()\n",
    "df = pd.merge(df, clin_df[[*chemo_columns, \"TCIA_ID\"]], on=\"TCIA_ID\", validate=\"one_to_one\")\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split_patients(\n",
    "    df, identifier=\"TCIA_ID\", endpoint=\"responder\", test_ratio=train_test_ratio, random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "x_cols = X_tr.columns\n",
    "X_tr = pd.DataFrame(pipeline.fit_transform(X_tr), columns=x_cols)\n",
    "X_ts = pd.DataFrame(pipeline.transform(X_ts), columns=x_cols)\n",
    "\n",
    "used_features = features + chemo_columns  # Creates a new list\n",
    "\n",
    "\n",
    "adas = SMOTE(random_state=42)\n",
    "X_tr, y_tr = adas.fit_resample(X_tr[used_features], y_tr)\n",
    "\n",
    "current_model_grid = linear_svc_grid\n",
    "# Fit the grid search on the training set\n",
    "\n",
    "current_model_grid.fit(X_tr[used_features], y_tr)\n",
    "good_estimator_params = current_model_grid.best_estimator_\n",
    "\n",
    "# threshold_tuner = TunedThresholdClassifierCV(good_estimator_params, cv=\"prefit\", refit=False, random_state=42, response_method=\"decision_function\", scoring = \"f1\")\n",
    "# threshold_tuner.fit(X_tr[pruned_features], y_tr)\n",
    "\n",
    "# optimal_threshold = threshold_tuner.best_threshold_\n",
    "\n",
    "optimal_threshold = 0.5\n",
    "\n",
    "print(f\"optimal decision function threshold: {optimal_threshold}\")\n",
    "\n",
    "# --- Predict probabilities ---\n",
    "y_prob_ts = good_estimator_params.decision_function(X_ts[used_features])\n",
    "\n",
    "# --- ROC Curve and AUC ---\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_ts, y_prob_ts)\n",
    "roc_auc = roc_auc_score(y_ts, y_prob_ts)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "roc_display.plot()\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, pad=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve and optimal F1 threshold ---\n",
    "precision, recall, thresholds = precision_recall_curve(y_ts, y_prob_ts)\n",
    "fscore = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1])\n",
    "\n",
    "# ix = np.argmax(fscore)\n",
    "# optimal_threshold = thresholds[ix]\n",
    "\n",
    "# cal_svc = CalibratedClassifierCV(good_estimator_params, method='sigmoid', cv='prefit')\n",
    "# cal_svc.fit(X_tr[features], y_tr)\n",
    "# y_prob_ts = cal_svc.predict_proba(X_ts[features])[:, 1]  # now 0-1 probabilities\n",
    "\n",
    "# # --- Precision-Recall and F1 ---\n",
    "# precision, recall, thresholds = precision_recall_curve(y_ts, y_prob_ts)\n",
    "# fscore = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1])\n",
    "# ix = np.argmax(fscore)\n",
    "# optimal_threshold = thresholds[ix]\n",
    "\n",
    "y_pred = (y_prob_ts > optimal_threshold).astype(int)\n",
    "average_precision = average_precision_score(y_ts, y_prob_ts)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "pr_display = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=average_precision)\n",
    "pr_display.plot()\n",
    "plt.scatter(recall, precision, marker='o', color='red')\n",
    "plt.title('Precision-Recall Curve', fontsize=14, pad=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Metrics ---\n",
    "tn, fp, fn, tp = confusion_matrix(y_ts, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_ts, y_pred)\n",
    "recall = recall_score(y_ts, y_pred)\n",
    "f1 = f1_score(y_ts, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Model Performance Metrics':^50}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Accuracy:':<25} {accuracy_score(y_ts, y_pred):.4f}\")\n",
    "print(f\"{'Precision:':<25} {precision:.4f}\")\n",
    "print(f\"{'Recall:':<25} {recall:.4f}\")\n",
    "print(f\"{'F1 Score:':<25} {f1:.4f}\")\n",
    "print(f\"{'Specificity:':<25} {specificity:.4f}\")\n",
    "print(f\"{'ROC AUC:':<25} {roc_auc:.4f}\")\n",
    "print(f\"{'Average Precision:':<25} {average_precision:.4f}\")\n",
    "print(\"=\"*50)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ec210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_curve, roc_auc_score,\n",
    "    average_precision_score, accuracy_score, precision_recall_curve,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    ")\n",
    "\n",
    "def update_model_results(model_name, num_features, y_true, y_pred, y_prob, output_file=\"model_optimum_features.xlsx\"):\n",
    "    \"\"\"\n",
    "    Update or append model results to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        num_features (int): Number of features used\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        y_prob: Predicted probabilities (for ROC AUC and Average Precision)\n",
    "        output_file (str): Path to the output Excel file\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'name': model_name,\n",
    "        'number_of_features': num_features,\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'specificity': (confusion_matrix(y_true, y_pred)[0, 0] / \n",
    "                       (confusion_matrix(y_true, y_pred)[0, 0] + confusion_matrix(y_true, y_pred)[0, 1])),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob),\n",
    "        'average_precision': average_precision_score(y_true, y_prob)\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(output_file):\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        \n",
    "        # If model name exists, update the row\n",
    "        if model_name in existing_df['name'].values:\n",
    "            existing_df = existing_df[existing_df['name'] != model_name]\n",
    "            metrics_df = pd.concat([existing_df, metrics_df], ignore_index=True)\n",
    "        else:\n",
    "            metrics_df = pd.concat([existing_df, metrics_df], ignore_index=True)\n",
    "    \n",
    "    # Save to Excel\n",
    "    metrics_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_curve, roc_auc_score,\n",
    "    average_precision_score, accuracy_score, precision_recall_curve,\n",
    "    RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.calibration import  CalibratedClassifierCV\n",
    "# --- Prepare data and model ---\n",
    "df = radiomics_df_final.copy()\n",
    "df = pd.merge(df, clin_df[[*chemo_columns, \"TCIA_ID\"]], on=\"TCIA_ID\", validate=\"one_to_one\")\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split_patients(\n",
    "    df, identifier=\"TCIA_ID\", endpoint=\"responder\", test_ratio=train_test_ratio, random_state=train_test_seed\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "x_cols = X_tr.columns\n",
    "X_tr = pd.DataFrame(pipeline.fit_transform(X_tr), columns=x_cols)\n",
    "X_ts = pd.DataFrame(pipeline.transform(X_ts), columns=x_cols)\n",
    "\n",
    "current_model_grid = et_grid\n",
    "\n",
    "\n",
    "for feat_count in np.linspace(start=3 , stop=50, num=10,dtype=int):\n",
    "    # Fit the grid search on the training set\n",
    "    pruned_features = features[:feat_count] + chemo_columns  # Creates a new list\n",
    "\n",
    "    print(pruned_features)\n",
    "\n",
    "    current_model_grid.fit(X_tr[pruned_features], y_tr)\n",
    "    good_estimator_params = current_model_grid.best_estimator_\n",
    "\n",
    "    # --- Predict probabilities ---\n",
    "    y_prob_ts = good_estimator_params.predict_proba(X_ts[pruned_features])[:,1]\n",
    "\n",
    "    # --- ROC Curve and AUC ---\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_ts, y_prob_ts)\n",
    "    roc_auc = roc_auc_score(y_ts, y_prob_ts)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "    roc_display.plot()\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, pad=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Precision-Recall Curve and optimal F1 threshold ---\n",
    "    precision, recall, thresholds = precision_recall_curve(y_ts, y_prob_ts)\n",
    "    fscore = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1])\n",
    "\n",
    "    # ix = np.argmax(fscore)\n",
    "    # optimal_threshold = thresholds[ix]\n",
    "\n",
    "    # cal_svc = CalibratedClassifierCV(good_estimator_params, method='sigmoid', cv='prefit')\n",
    "    # cal_svc.fit(X_tr[features], y_tr)\n",
    "    # y_prob_ts = cal_svc.predict_proba(X_ts[features])[:, 1]  # now 0-1 probabilities\n",
    "\n",
    "    # # --- Precision-Recall and F1 ---\n",
    "    # precision, recall, thresholds = precision_recall_curve(y_ts, y_prob_ts)\n",
    "    # fscore = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1])\n",
    "    # ix = np.argmax(fscore)\n",
    "    # optimal_threshold = thresholds[ix]\n",
    "\n",
    "    y_pred = good_estimator_params.predict(X_ts[pruned_features])\n",
    "    average_precision = average_precision_score(y_ts, y_prob_ts)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pr_display = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=average_precision)\n",
    "    pr_display.plot()\n",
    "    plt.scatter(recall, precision, marker='o', color='red')\n",
    "    plt.title('Precision-Recall Curve', fontsize=14, pad=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Metrics ---\n",
    "    tn, fp, fn, tp = confusion_matrix(y_ts, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_ts, y_pred)\n",
    "    recall = recall_score(y_ts, y_pred)\n",
    "    f1 = f1_score(y_ts, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model Performance Metrics of {feat_count} features:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Accuracy:':<25} {accuracy_score(y_ts, y_pred):.4f}\")\n",
    "    print(f\"{'Precision:':<25} {precision:.4f}\")\n",
    "    print(f\"{'Recall:':<25} {recall:.4f}\")\n",
    "    print(f\"{'F1 Score:':<25} {f1:.4f}\")\n",
    "    print(f\"{'Specificity:':<25} {specificity:.4f}\")\n",
    "    print(f\"{'ROC AUC:':<25} {roc_auc:.4f}\")\n",
    "    print(f\"{'Average Precision:':<25} {average_precision:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # After calculating all metrics, update the results file\n",
    "    update_model_results(\n",
    "        model_name=current_model_grid.best_estimator_.__class__.__name__,  # or whatever your model name is\n",
    "        num_features=feat_count,\n",
    "        y_true=y_ts,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob_ts,\n",
    "        output_file=\"model_optimum_features.xlsx\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ca2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pruned_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36072fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good_estimator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9366c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_feats = selected_features_lasso[:23] + chemo_columns\n",
    "\n",
    "print(used_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813eea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_seeds = [ 8,32,47,70,48,54,55,94,43,86] #generated from np random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, average_precision_score, accuracy_score, precision_recall_curve\n",
    "\n",
    "\n",
    "split_history = {\n",
    "    'accuracy': [],\n",
    "    'roc': [],\n",
    "    'auc_prc': [],\n",
    "    'f1': [],\n",
    "    'precision': [],\n",
    "    'sensitivity' : [],\n",
    "    'recall': [],\n",
    "    'specificity': []\n",
    "}\n",
    "\n",
    "\n",
    "used_feats = selected_features_lasso[:23] + chemo_columns\n",
    "\n",
    "\n",
    "\n",
    "# 6. --- UNBIASED THRESHOLD DETERMINATION (ON TRAINING DATA) ---\n",
    "# Find the optimal threshold based ONLY on the training data (y_tr_split, y_prob_tr).\n",
    "# youden_j_train, optimal_threshold = calculate_youden_j_and_threshold(y_tr, y_prob_tr)\n",
    "\n",
    "# 7. --- UNBIASED PREDICTIONS (ON TEST DATA) ---\n",
    "# Apply the training-derived threshold to the test set scores.\n",
    "\n",
    "current_model_grid = linear_svc_grid\n",
    "\n",
    "for rand_state in random_test_seeds:\n",
    "    # Split the data\n",
    "    X_tr, X_ts, y_tr, y_ts = train_test_split_patients(\n",
    "        df, \n",
    "        identifier=\"TCIA_ID\", \n",
    "        endpoint=\"responder\",  \n",
    "        test_ratio=0.3, \n",
    "        random_state=rand_state,\n",
    "    )\n",
    "\n",
    "    # Create and fit pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Transform features\n",
    "    x_cols = X_tr.columns\n",
    "    X_tr = pipeline.fit_transform(X_tr)\n",
    "    X_ts = pipeline.transform(X_ts)\n",
    "    X_tr = pd.DataFrame(X_tr, columns=x_cols)\n",
    "    X_ts = pd.DataFrame(X_ts, columns=x_cols)\n",
    "    current_model_grid.fit(X_tr[used_feats], y_tr)    \n",
    "    threshold_tuner = TunedThresholdClassifierCV(current_model_grid.best_estimator_, cv=\"prefit\", refit=False, random_state=42, response_method=\"decision_function\", scoring=\"f1\")\n",
    "    threshold_tuner.fit(X_tr[used_feats], y_tr)\n",
    "\n",
    "    optimal_threshold = threshold_tuner.best_threshold_\n",
    "\n",
    "    # y_prob_ts = good_estimator_params.decision_function(X_ts[features])\n",
    "    # Fit grid search and get best estimator\n",
    "\n",
    "    est = current_model_grid.best_estimator_\n",
    "    # Make predictions\n",
    "    y_prob = est.decision_function(X_ts[used_feats])\n",
    "    # y_prob_ts = cal_svc.predict_proba(X_ts[features])[:, 1] \n",
    "    y_pred = (y_prob > optimal_threshold).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_ts, y_pred).ravel()\n",
    "\n",
    "# Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc = roc_auc_score(y_ts, y_prob)\n",
    "    auc = average_precision_score(y_ts, y_prob)\n",
    "    accuracy = accuracy_score(y_ts, y_pred)\n",
    "    f1 = f1_score(y_ts, y_pred, pos_label=1)\n",
    "    precision = precision_score(y_ts, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_ts, y_pred, pos_label=1)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_ts, y_pred).ravel()\n",
    "\n",
    "    print(classification_report(y_ts, y_pred))\n",
    "    print(f\"ROC AUC score: {roc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"recall : {recall}\")\n",
    "    print(f\"f1 score : {f1}\")\n",
    "\n",
    "# Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Store metrics\n",
    "    split_history['roc'].append(roc)\n",
    "    split_history['auc_prc'].append(auc)\n",
    "    split_history['accuracy'].append(accuracy)\n",
    "    split_history['f1'].append(f1)\n",
    "    split_history['precision'].append(precision)\n",
    "    split_history['recall'].append(recall)\n",
    "    split_history['specificity'].append(specificity)\n",
    "\n",
    "    # Print iteration results\n",
    "    print(f\"\\n--- Random State: {rand_state} ---\")\n",
    "    print(classification_report(y_ts, y_pred))\n",
    "    print(f\"ROC AUC score: {roc:.4f}\")\n",
    "    print(f\"Average precision score: {auc:.4f}\")\n",
    " \n",
    "\n",
    "\n",
    "# Calculate and print averages\n",
    "# Calculate averages using dictionary comprehension\n",
    "averages = {metric: np.mean(values) for metric, values in split_history.items()}\n",
    "\n",
    "# Print formatted averages\n",
    "print(\"\\n--- Final Averages ---\")\n",
    "for metric, avg in averages.items():\n",
    "    print(f\"{metric.upper()}: {avg:.4f}\")\n",
    "\n",
    "# If you also want standard deviations\n",
    "stds = {metric: np.std(values) for metric, values in split_history.items()}\n",
    "print(\"\\n--- Standard Deviations ---\")\n",
    "for metric, std in stds.items():\n",
    "    print(f\"{metric.upper()}: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96653993",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0, 10000, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73dc8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_monte_carlo_results(output_file, model_name, split_history):\n",
    "    try:\n",
    "        output_df = pd.read_excel(output_file)\n",
    "        if output_df.empty:\n",
    "            output_df = pd.DataFrame()\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        output_df = pd.DataFrame()\n",
    "\n",
    "    # Create a new row with model_name and all metrics from split_history\n",
    "    new_entry = {\n",
    "        'model_name': model_name,\n",
    "        **{key: [np.mean(split_history[key])] for key in split_history}\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    new_df = pd.DataFrame(new_entry)\n",
    "    \n",
    "    # Check if model_name already exists in the output file\n",
    "    if not output_df.empty and 'model_name' in output_df.columns:\n",
    "        # Remove the existing entry if it exists\n",
    "        output_df = output_df[output_df['model_name'] != model_name]\n",
    "    \n",
    "    # Concatenate the updated DataFrame with the new entry\n",
    "    output_df = pd.concat([output_df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "results = append_monte_carlo_results(\n",
    "    output_file='monte_carlo_results_optimized.xlsx',\n",
    "    model_name=current_model_grid.best_estimator_.__class__.__name__,\n",
    "    split_history=split_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b626c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428b2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(split_history, title='Model Performance Across Different Random States', \n",
    "                    dsafdsds     figsize=(12, 4), dpi=100, save_path=None, show_std=False,\n",
    "                         colors=None, line_style='-', mean_line_style='--', \n",
    "                         grid_alpha=0.3, legend_loc='best', y_padding=0.1):\n",
    "    \"\"\"\n",
    "    Plot model performance metrics across different random states.\n",
    "    Skips any empty metrics in the input.\n",
    "    \"\"\"\n",
    "    # Filter out empty metrics\n",
    "    valid_metrics = {k: v for k, v in split_history.items() if len(v) > 0}\n",
    "    \n",
    "    if not valid_metrics:\n",
    "        print(\"Warning: No valid metrics to plot!\")\n",
    "        return None\n",
    "    \n",
    "    # Default colors if none provided\n",
    "    if colors is None:\n",
    "        colors = {\n",
    "            'accuracy': '#1f77b4', 'roc_auc': '#ff7f0e', 'auc_prc': '#2ca02c',\n",
    "            'f1': '#d62728', 'precision': '#9467bd', 'recall': '#8c564b',\n",
    "            'loss': '#e377c2', 'val_loss': '#7f7f7f'\n",
    "        }\n",
    "    \n",
    "    n_metrics = len(valid_metrics)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=(figsize[0], figsize[1] * n_metrics), dpi=dpi)\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "    \n",
    "    # If there's only one metric, axes won't be an array\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each valid metric\n",
    "    for i, (metric, values) in enumerate(valid_metrics.items()):\n",
    "        ax = axes[i]\n",
    "        values = np.array(values)\n",
    "        x = range(len(values))\n",
    "        \n",
    "        # Get color for this metric\n",
    "        color = colors.get(metric, f'C{i}')\n",
    "        \n",
    "        # Plot the metric values\n",
    "        ax.plot(x, values, line_style, color=color, label=metric.upper(), alpha=0.8)\n",
    "        \n",
    "        # Calculate and plot mean and std\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        \n",
    "        # Plot mean line\n",
    "        ax.axhline(y=mean_val, color=color, linestyle=mean_line_style, \n",
    "                  label=f'Mean: {mean_val:.3f}  {std_val:.3f}', alpha=0.8)\n",
    "        \n",
    "        # Plot std as shaded area if enabled\n",
    "        if show_std and len(values) > 1:\n",
    "            ax.fill_between(x, \n",
    "                          mean_val - std_val, \n",
    "                          mean_val + std_val, \n",
    "                          color=color, \n",
    "                          alpha=0.1)\n",
    "        \n",
    "        # Customize subplot\n",
    "        ax.set_title(metric.upper().replace('_', ' ').title())\n",
    "        ax.set_xlabel('Random State')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.legend(loc=legend_loc)\n",
    "        ax.grid(True, alpha=grid_alpha)\n",
    "        \n",
    "        # Set y-axis limits\n",
    "        y_min, y_max = np.min(values), np.max(values)\n",
    "        y_range = y_max - y_min\n",
    "        y_range = y_range if y_range > 0 else 0.1  # Handle case where min == max\n",
    "        ax.set_ylim([y_min - y_padding * y_range, y_max + y_padding * y_range])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=dpi)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = plot_model_performance(\n",
    "    split_history,\n",
    "    title='Model Performance Across Different Random States',\n",
    "    # colors={\n",
    "    #     'accuracy': 'blue',\n",
    "    #     'f1': 'red',\n",
    "    #     'precision': 'green',\n",
    "    #     'recall': 'purple'\n",
    "    # },\n",
    "    show_std=True,\n",
    "    save_path='model_performance.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad08ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_tr contains your LASSO-selected features and y_tr contains your class labels.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA# ----------------------------------------------------\n",
    "# A. Perform Linear Discriminant Analysis (LDA)\n",
    "# ----------------------------------------------------\n",
    "# LDA is supervised and will find the single best axis (LD1) that separates the classes.\n",
    "lda = LDA(n_components=1)\n",
    "X_lda = lda.fit_transform(X_tr, y_tr)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Visualize the 1D LDA Projection\n",
    "# ----------------------------------------------------\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "# For a 1D plot, we set the y-axis to a constant (0) to create a line\n",
    "# and then use a small random jitter (noise) to prevent points from stacking up exactly\n",
    "# on top of each other, which helps visualize the density.\n",
    "import numpy as np\n",
    "y_jitter = np.random.normal(0, 0.05, X_lda.shape[0])\n",
    "\n",
    "# Plotting Class 0 (NonResponder)\n",
    "subset_0 = X_lda[y_tr == 0]\n",
    "plt.scatter(\n",
    "    subset_0, y_jitter[y_tr == 0],\n",
    "    c='#0077b6', # A distinct blue color\n",
    "    label='0 (NonResponder)',\n",
    "    alpha=0.7,\n",
    "    s=70,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "\n",
    "# Plotting Class 1 (Responder)\n",
    "subset_1 = X_lda[y_tr == 1]\n",
    "plt.scatter(\n",
    "    subset_1, y_jitter[y_tr == 1] + 0.05, # Shift Class 1 slightly up for better viewing\n",
    "    c='#d90429', # A distinct red color\n",
    "    label='1 (Responder)',\n",
    "    alpha=0.7,\n",
    "    s=70,\n",
    "    edgecolors='k',\n",
    "    linewidths=0.5\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# C. Final Plot Aesthetics\n",
    "# ----------------------------------------------------\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5) # Add a vertical line at the origin\n",
    "plt.title('LDA Projection (LD1) Showing Optimal Class Separation')\n",
    "plt.xlabel('Linear Discriminant 1 (LD1) - The Optimal Separating Axis')\n",
    "plt.yticks([]) # Hide the Y-axis ticks, as it's a 1D projection\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19474fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = linear_svc_grid.best_estimator_\n",
    "\n",
    "y_pred = estimator.predict(x_ts[features])\n",
    "\n",
    "print(classification_report(y_ts, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate patient IDs in radiomics_df_final:\", radiomics_df_final['TCIA_ID'].duplicated().sum())\n",
    "print(\"Duplicate patient IDs in clin_df:\", clin_df['TCIA_ID'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -----------------------------\n",
    "# Replace these with your data\n",
    "# X = your feature matrix\n",
    "# y = your label vector\n",
    "# -----------------------------\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# ---- Real score ----\n",
    "real_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_tr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Real AUC:\", real_scores.mean(), \"\", real_scores.std())\n",
    "\n",
    "# ---- Permuted score ----\n",
    "y_perm = np.random.permutation(y_tr)\n",
    "\n",
    "perm_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_perm,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Permuted AUC:\", perm_scores.mean(), \"\", perm_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b559dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12e5dd",
   "metadata": {},
   "source": [
    "Model Tuning(?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cfcf3",
   "metadata": {},
   "source": [
    "RFE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin-bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
