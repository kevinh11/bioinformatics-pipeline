{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2699dea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnrrd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# module imports\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze \u001b[38;5;28;01mas\u001b[39;00m ana\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ecat, imagestats, mriutils, orientations, streamlines, viewers\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nifti1 \u001b[38;5;28;01mas\u001b[39;00m ni1\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spm2analyze \u001b[38;5;28;01mas\u001b[39;00m spm2\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\imagestats.py:13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Functions for computing image statistics\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimageclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spatial_axes_first\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_nonzero_voxels\u001b[39m(img):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Count number of non-zero voxels\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\imageclasses.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyzeImage\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrikhead\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AFNIImage\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Image\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfreesurfer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MGHImage\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgifti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GiftiImage\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\cifti2\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# vi: set ft=python sts=4 ts=4 sw=4 et:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"CIFTI-2 format IO\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m.. currentmodule:: nibabel.cifti2\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m   cifti2_axes\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     CIFTI_BRAIN_STRUCTURES,\n\u001b[0;32m     22\u001b[0m     CIFTI_MODEL_TYPES,\n\u001b[0;32m     23\u001b[0m     Cifti2BrainModel,\n\u001b[0;32m     24\u001b[0m     Cifti2Header,\n\u001b[0;32m     25\u001b[0m     Cifti2HeaderError,\n\u001b[0;32m     26\u001b[0m     Cifti2Image,\n\u001b[0;32m     27\u001b[0m     Cifti2Label,\n\u001b[0;32m     28\u001b[0m     Cifti2LabelTable,\n\u001b[0;32m     29\u001b[0m     Cifti2Matrix,\n\u001b[0;32m     30\u001b[0m     Cifti2MatrixIndicesMap,\n\u001b[0;32m     31\u001b[0m     Cifti2MetaData,\n\u001b[0;32m     32\u001b[0m     Cifti2NamedMap,\n\u001b[0;32m     33\u001b[0m     Cifti2Parcel,\n\u001b[0;32m     34\u001b[0m     Cifti2Surface,\n\u001b[0;32m     35\u001b[0m     Cifti2TransformationMatrixVoxelIndicesIJKtoXYZ,\n\u001b[0;32m     36\u001b[0m     Cifti2VertexIndices,\n\u001b[0;32m     37\u001b[0m     Cifti2Vertices,\n\u001b[0;32m     38\u001b[0m     Cifti2Volume,\n\u001b[0;32m     39\u001b[0m     Cifti2VoxelIndicesIJK,\n\u001b[0;32m     40\u001b[0m     load,\n\u001b[0;32m     41\u001b[0m     save,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifti2_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axis, BrainModelAxis, LabelAxis, ParcelsAxis, ScalarAxis, SeriesAxis\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_cifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cifti2Extension\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\cifti2\\cifti2.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataobj_images\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataobjImage\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilebasedimages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileBasedHeader, SerializableImage\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti1Extensions\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnifti2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nifti2Header, Nifti2Image\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvolumeutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Recoder, make_dt_codes\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\nifti1.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m     DicomDataset \u001b[38;5;241m=\u001b[39m pdcm\u001b[38;5;241m.\u001b[39mDataset\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     pdcm, have_dicom, _ \u001b[38;5;241m=\u001b[39m \u001b[43moptional_package\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpydicom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m have_dicom:\n\u001b[0;32m     50\u001b[0m         DicomDataset \u001b[38;5;241m=\u001b[39m pdcm\u001b[38;5;241m.\u001b[39mDataset\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\nibabel\\optpkg.py:104\u001b[0m, in \u001b[0;36moptional_package\u001b[1;34m(name, trip_msg, min_version)\u001b[0m\n\u001b[0;32m    102\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfromlist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc_:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Could fail due to some ImportError or for some other reason\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# e.g. h5py might have been checking file system to support UTF-8\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# etc.  We should not blow if they blow\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     exc \u001b[38;5;241m=\u001b[39m exc_  \u001b[38;5;66;03m# So it is accessible outside of the code block\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\__init__.py:31\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2018 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"pydicom package -- easily handle DICOM files.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m   See Quick Start below.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataelem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataElement\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, FileDataset, FileMetaDataset\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\dataelem.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, TYPE_CHECKING, NamedTuple\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config  \u001b[38;5;66;03m# don't import datetime_conversion directly\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatadict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     dictionary_has_tag,\n\u001b[0;32m     21\u001b[0m     dictionary_description,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     repeater_has_tag,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\config.py:407\u001b[0m\n\u001b[0;32m    398\u001b[0m show_file_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03mIf ``True`` (default), the 'str' and 'repr' methods\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03mof :class:`~pydicom.dataset.Dataset` begin with a separate section\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m.. versionadded:: 2.0\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrle_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrle_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixel_data_handlers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpillow_handler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpillow_handler\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixel_data_handlers\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn_and_log\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     apply_color_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_color_lut,\n\u001b[0;32m      8\u001b[0m     apply_modality_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_modality_lut,\n\u001b[0;32m      9\u001b[0m     apply_voi_lut \u001b[38;5;28;01mas\u001b[39;00m _apply_voi_lut,\n\u001b[0;32m     10\u001b[0m     apply_voi \u001b[38;5;28;01mas\u001b[39;00m _apply_voi,\n\u001b[0;32m     11\u001b[0m     apply_windowing \u001b[38;5;28;01mas\u001b[39;00m _apply_windowing,\n\u001b[0;32m     12\u001b[0m     convert_color_space \u001b[38;5;28;01mas\u001b[39;00m _convert_color_space,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     expand_ybr422 \u001b[38;5;28;01mas\u001b[39;00m _expand_ybr422,\n\u001b[0;32m     16\u001b[0m     pack_bits \u001b[38;5;28;01mas\u001b[39;00m _pack_bits,\n\u001b[0;32m     17\u001b[0m     unpack_bits \u001b[38;5;28;01mas\u001b[39;00m _unpack_bits,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     21\u001b[0m _DEPRECATED \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_color_lut\u001b[39m\u001b[38;5;124m\"\u001b[39m: _apply_color_lut,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_modality_lut\u001b[39m\u001b[38;5;124m\"\u001b[39m: _apply_modality_lut,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munpack_bits\u001b[39m\u001b[38;5;124m\"\u001b[39m: _unpack_bits,\n\u001b[0;32m     31\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2024 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_decoder\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_encoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     apply_color_lut,\n\u001b[0;32m      7\u001b[0m     apply_icc_profile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     create_icc_transform,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     as_pixel_options,\n\u001b[0;32m     19\u001b[0m     compress,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     unpack_bits,\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\encoders\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2008-2024 pydicom authors. See LICENSE file for details.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpixels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     RLELosslessEncoder,\n\u001b[0;32m      5\u001b[0m     JPEGLSLosslessEncoder,\n\u001b[0;32m      6\u001b[0m     JPEGLSNearLosslessEncoder,\n\u001b[0;32m      7\u001b[0m     JPEG2000LosslessEncoder,\n\u001b[0;32m      8\u001b[0m     JPEG2000Encoder,\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\encoders\\base.py:783\u001b[0m\n\u001b[0;32m    774\u001b[0m RLELosslessEncoder\u001b[38;5;241m.\u001b[39madd_plugins(\n\u001b[0;32m    775\u001b[0m     [\n\u001b[0;32m    776\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdcm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpydicom.pixels.encoders.gdcm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencode_pixel_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m     ],\n\u001b[0;32m    780\u001b[0m )\n\u001b[0;32m    782\u001b[0m JPEGLSLosslessEncoder \u001b[38;5;241m=\u001b[39m Encoder(JPEGLSLossless)\n\u001b[1;32m--> 783\u001b[0m \u001b[43mJPEGLSLosslessEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_plugin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyjpegls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpydicom.pixels.encoders.pyjpegls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_encode_frame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    787\u001b[0m JPEGLSNearLosslessEncoder \u001b[38;5;241m=\u001b[39m Encoder(JPEGLSNearLossless)\n\u001b[0;32m    788\u001b[0m JPEGLSNearLosslessEncoder\u001b[38;5;241m.\u001b[39madd_plugin(\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyjpegls\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpydicom.pixels.encoders.pyjpegls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_encode_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    790\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\site-packages\\pydicom\\pixels\\common.py:83\u001b[0m, in \u001b[0;36mCoderBase.add_plugin\u001b[1;34m(self, label, import_path)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_available \u001b[38;5;129;01mor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unavailable:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already has a plugin named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimport_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# `is_available(UID)` is required for plugins\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mis_available(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUID):\n",
      "File \u001b[1;32mc:\\Users\\Kevin Hadinata\\.conda\\envs\\kevin-bioinformatics\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1532\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1506\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1605\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nrrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_features_dir = os.path.join(os.path.dirname(os.getcwd()),\"data\", \"radiomics_features_updated.csv\")\n",
    "radiomics_df= pd.read_csv(radiomics_features_dir)\n",
    "\n",
    "radiomics_df[\"TCIA_ID\"] = radiomics_df[\"Subject ID_x\"]\n",
    "radiomics_df = radiomics_df.drop(columns=[\"Subject ID_x\"])\n",
    "# len(radiomics_df[\"Subject ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f64be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in radiomics_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad662296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diagnostic_cols(df):\n",
    "    copy = df.copy()\n",
    "    retained_cols = []\n",
    "    for column in copy.columns:\n",
    "        if \"diagnostics\" in column:\n",
    "            continue\n",
    "\n",
    "        retained_cols.append(column)\n",
    "\n",
    "    return copy[retained_cols]\n",
    "\n",
    "radiomics_df = remove_diagnostic_cols(radiomics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f757bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()), \"data\",  \"clinical_df_cleaned.csv\"))\n",
    "clin_df[\"responder\"].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_merged = pd.merge(\n",
    "    left=radiomics_df,\n",
    "    right= clin_df[[\"TCIA_ID\", \"responder\"]],\n",
    "    on=\"TCIA_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Assuming 'radiomics_df_merged' is the original DataFrame containing all features and metadata\n",
    "\n",
    "# 1. Separate the metadata columns\n",
    "metadata_cols = [\"TCIA_ID\", \"responder\"] \n",
    "metadata = radiomics_df_merged[metadata_cols].copy()\n",
    "\n",
    "# 2. Drop metadata to isolate ONLY radiomics features\n",
    "radiomics_features = radiomics_df_merged.drop(columns=metadata_cols, errors='ignore').copy()\n",
    "\n",
    "# 3. Filter the features to keep ONLY those containing 'pv'\n",
    "radiomics_features_pv = radiomics_features\n",
    "\n",
    "# 4. Handle missing subjects (rows) by dropping any rows that are all NaN across PV features\n",
    "radiomics_df_final = radiomics_features_pv\n",
    "\n",
    "# 5. Concatenate the filtered PV features with the aligned metadata columns\n",
    "# NOTE: We use .loc[] to ensure the metadata only includes subjects that survived the dropna step (row alignment).\n",
    "metadata_aligned = metadata.loc[radiomics_df_final.index].drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "radiomics_df_final = pd.concat([radiomics_df_final, metadata_aligned], axis=1).reset_index(drop=True)\n",
    "\n",
    "# radiomics_df_final now contains ONLY PV features + TCIA_ID + responder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_multiphase_df = pd.read_csv(\"non_multiphase_joined_fixed.csv\")\n",
    "\n",
    "non_multiphase_patients = non_multiphase_df[\"Subject ID\"].unique()\n",
    "\n",
    "non_multiphase_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radiomics_df_final = radiomics_df_final[radiomics_df_final[\"TCIA_ID\"].isin(non_multiphase_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiphase_extraction_fixed_df = pd.read_csv(\"multiphase_radiomic_features_pv_aligned_v2_WAVELET.csv\")\n",
    "\n",
    "used_cols =  [col for col in multiphase_extraction_fixed_df.columns if (\"delayed\" not in col) and (\"diagnostics\" not in col)]\n",
    "\n",
    "multiphase_extraction_fixed_df = multiphase_extraction_fixed_df[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiphase_extraction_fixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22739ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df = pd.read_csv(\"clinical_df_cleaned.csv\")\n",
    "multiphase_extraction_fixed_df = pd.merge(\n",
    "    left=multiphase_extraction_fixed_df, \n",
    "    right = clin_df[[\"responder\", \"TCIA_ID\"]], \n",
    "    left_on=\"Subject ID\",\n",
    "    right_on=\"TCIA_ID\",\n",
    "    how = \"left\",\n",
    ")\n",
    "\n",
    "multiphase_extraction_fixed_df = multiphase_extraction_fixed_df.drop(columns=[\"Subject ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = radiomics_df_final.columns.intersection(multiphase_extraction_fixed_df.columns)\n",
    "\n",
    "radiomics_df_final = pd.concat(\n",
    "    [radiomics_df_final[common_cols], multiphase_extraction_fixed_df[common_cols]],\n",
    "    axis = 0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final = radiomics_df_final.sort_values(\"TCIA_ID\").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85792959",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "def is_train_test_difference_significant(x_train, x_test, alpha=0.05):\n",
    "\n",
    "    x_train = np.asarray(x_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    \n",
    "    # Flatten arrays if they are multi-dimensional\n",
    "    if x_train.ndim > 1:\n",
    "        x_train = x_train.ravel()\n",
    "    if x_test.ndim > 1:\n",
    "        x_test = x_test.ravel()\n",
    "    \n",
    "    # Perform the Wilcoxon rank-sum test\n",
    "    stat, p_value = ranksums(x_train, x_test)\n",
    "    \n",
    "    # Check if the difference is statistically significant\n",
    "    is_significant = p_value < alpha\n",
    "\n",
    "    \n",
    "    \n",
    "    return is_significant, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radiomics_df_final.to_csv(\"radiomics_df_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b30508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_rows = radiomics_df_final[\"responder\"].dropna().index\n",
    "\n",
    "# radiomics_df_final = radiomics_df_final.loc[non_empty_rows]\n",
    "\n",
    "# radiomics_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b96710",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "print(parent_path)\n",
    "data_df = pd.read_csv(os.path.join(parent_path, \"data\", \"combined_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047afca",
   "metadata": {},
   "source": [
    "A/B Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab7af9",
   "metadata": {},
   "source": [
    "combine endpoint and radiomic for initial test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_split_patients(\n",
    "    dataframe: pd.DataFrame, \n",
    "    identifier: str, \n",
    "    endpoint: str, \n",
    "    test_ratio: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    # Get unique patient IDs\n",
    "    unique_patients = dataframe[identifier].unique()\n",
    "    \n",
    "    # Split patients into train and test\n",
    "    train_patients, test_patients = train_test_split(\n",
    "        unique_patients, \n",
    "        test_size=test_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=dataframe[endpoint]  # Optional: maintain class balance\n",
    "    )\n",
    "    \n",
    "    # Create masks\n",
    "    train_mask = dataframe[identifier].isin(train_patients)\n",
    "    test_mask = dataframe[identifier].isin(test_patients)\n",
    "    \n",
    "    # Split the data\n",
    "    x_train = dataframe[train_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    x_test = dataframe[test_mask].drop(columns=[endpoint, identifier], axis=1)\n",
    "    y_train = dataframe[train_mask][endpoint]\n",
    "    y_test = dataframe[test_mask][endpoint]\n",
    "\n",
    " \n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute common classification metrics for binary classification.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "    - Accuracy\n",
    "    - F1 Score\n",
    "    - Precision\n",
    "    - Sensitivity (Recall)\n",
    "    - Specificity\n",
    "    - Confusion Matrix\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Sensitivity (Recall)': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Confusion Matrix': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7b0d5",
   "metadata": {},
   "source": [
    "Standardization of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MissingValueColumnFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.3):\n",
    "        self.threshold = threshold\n",
    "        self.keep_features_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        missing_frac = X.isna().mean()\n",
    "        self.keep_features_ = missing_frac[missing_frac <= self.threshold].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        # Only keep columns selected during fit\n",
    "        return X[self.keep_features_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e01e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CleanFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, missing_thresh=0.3, variance_thresh=1e-6):\n",
    "        self.missing_thresh = missing_thresh\n",
    "        self.variance_thresh = variance_thresh\n",
    "        self.keep_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # print(\"from clean feature selector : X\")\n",
    "        # print(X)\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        # Drop by missing %\n",
    "        keep_missing = X.isna().mean() < self.missing_thresh\n",
    "        X2 = X.loc[:, keep_missing]\n",
    "\n",
    "        # Drop by variance\n",
    "        var = X2.var()\n",
    "        keep_var = var > self.variance_thresh\n",
    "\n",
    "        self.keep_cols_ = X2.columns[keep_var].tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=X.columns if hasattr(X, \"columns\") else None)\n",
    "\n",
    "        print(X[self.keep_cols_])\n",
    "        return X[self.keep_cols_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "\n",
    "class FeatureFilterer:\n",
    "    def __init__(self, responder_idx, nonresponder_idx, filter_method_name = \"mannwhitney\", p_val_threshold=0.05):\n",
    "        self.p_val_threshold = p_val_threshold\n",
    "        self.responder_idx = responder_idx\n",
    "        self.nonresponder_idx = nonresponder_idx\n",
    "        self.filter_method_name = filter_method_name\n",
    "        self.kept_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # X is already a DataFrame here if CleanFeatureSelector keeps it that way\n",
    "        X_df = pd.DataFrame(X, index=self.responder_idx.union(self.nonresponder_idx))\n",
    "\n",
    "        x_responder = X_df.loc[self.responder_idx]\n",
    "        x_nonresponder = X_df.loc[self.nonresponder_idx]\n",
    "\n",
    "        self.significant_features = []\n",
    "\n",
    "        statistical_test = None\n",
    "\n",
    "        if self.filter_method_name == \"mannwhitney\":\n",
    "            statistical_test = mannwhitneyu\n",
    "\n",
    "        elif self.filter_method_name == \"kruskal\":\n",
    "            statistical_test = kruskal\n",
    "        \n",
    "        elif self.filter_method_name == \"wilcoxon\":\n",
    "            statistical_test = kruskal\n",
    "\n",
    "        for col in X_df.columns:\n",
    "            try:\n",
    "                _, p = statistical_test(\n",
    "                    x_responder[col].dropna(),\n",
    "                    x_nonresponder[col].dropna()\n",
    "                )\n",
    "                if p < self.p_val_threshold:\n",
    "                    self.significant_features.append(col)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        print(self.significant_features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        return X_df[self.significant_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # record input column names\n",
    "        self.columns_ = X.columns if hasattr(X, \"columns\") else self.columns\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(X, columns=self.columns_, index=getattr(X, \"index\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# combined = pd.merge(\n",
    "#     left=radiomics_df,\n",
    "#     right= clin_df.drop(columns=['TNM']),\n",
    "#     on=\"Subject ID\",\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "\n",
    "identifier = radiomics_df_final[\"TCIA_ID\"]\n",
    "\n",
    "\n",
    "radiomics_df_final = radiomics_df_final.dropna(axis= 0, subset=[\"responder\"])\n",
    "\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split_patients(\n",
    "    radiomics_df_final, \n",
    "    identifier=\"TCIA_ID\",\n",
    "    endpoint=\"responder\", \n",
    "    test_ratio=0.3\n",
    ")\n",
    "x_tr_index = x_tr.index\n",
    "x_ts_index = x_ts.index\n",
    "x_cols = x_tr.columns\n",
    "    \n",
    "responder_indices = y_tr[y_tr == 1].index\n",
    "nonresponder_indices = y_tr[y_tr == 0].index\n",
    "\n",
    "preprocess_pipe = Pipeline(steps=[\n",
    "    (\"screening\", CleanFeatureSelector()),\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# Transform\n",
    "x_tr_transformed = preprocess_pipe.fit_transform(x_tr)\n",
    "x_ts_transformed = preprocess_pipe.transform(x_ts)\n",
    "\n",
    "# Extract kept columns\n",
    "# screening_cols = preprocess_pipe.named_steps['filter'].keep_cols\n",
    "kept_cols = preprocess_pipe.named_steps['screening'].keep_cols_\n",
    "\n",
    "# Final aligned DataFrames\n",
    "x_tr = pd.DataFrame(x_tr_transformed, index=x_tr.index, columns=kept_cols)\n",
    "x_ts = pd.DataFrame(x_ts_transformed, index=x_ts.index, columns=kept_cols)\n",
    "\n",
    "# Now you can use x_tr_final and x_ts_final with their original indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dcb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(radiomics_df_final.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_filter_pipe = Pipeline(steps=[\n",
    "    (\"filter\", FeatureFilterer(\n",
    "        p_val_threshold=0.2,\n",
    "        responder_idx=responder_indices,\n",
    "        nonresponder_idx=nonresponder_indices\n",
    "    )),\n",
    "    \n",
    "])\n",
    "\n",
    "# Fit on TRAINING DATA ONLY (after screening + impute)\n",
    "x_tr_filtered = feature_filter_pipe.fit_transform(x_tr)\n",
    "x_ts_filtered = feature_filter_pipe.transform(x_ts)\n",
    "\n",
    "# Extract feature names\n",
    "kept_cols = feature_filter_pipe.named_steps[\"filter\"].significant_features\n",
    "\n",
    "# Rebuild DataFrame\n",
    "x_tr = pd.DataFrame(x_tr_filtered, index=x_tr.index, columns=kept_cols)\n",
    "x_ts = pd.DataFrame(x_ts_filtered, index=x_ts.index, columns=kept_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548d3f5",
   "metadata": {},
   "source": [
    "Univariate Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrmr import mrmr_classif\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MRMRSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, K=50, show_progress=False):\n",
    "        self.K = K\n",
    "        self.show_progress = show_progress\n",
    "        self.selected_features_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # mrmr_classif automatically uses mutual information and redundancy internally\n",
    "        self.selected_features_ = mrmr_classif(\n",
    "            X, y,\n",
    "            K=self.K,\n",
    "            show_progress=self.show_progress\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure that the data type supports column selection\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X[self.selected_features_]\n",
    "        else:\n",
    "            raise TypeError(\"MRMRSelector expects a pandas DataFrame as input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7afa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier #\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# logreg = LogisticRegression(\n",
    "#     max_iter= 5000, \n",
    "#     # C=10.0,\n",
    "#     class_weight= \"balanced\"\n",
    "# )\n",
    "\n",
    "lr_param_grid = {\n",
    "  \"C\" :[0.01, 0.1, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0],\n",
    "  \"penalty\" : [\"l1\", \"l2\"],\n",
    "  \"solver\" : [\"liblinear\", \"saga\"],\n",
    "  \"class_weight\" : [\n",
    "    \"balanced\",\n",
    "    {0:1, 1:2},\n",
    "    {0:1, 1:5},\n",
    "    {0:1, 1:10}\n",
    "  \n",
    "  ]\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum()]  # handles class imbalance\n",
    "}\n",
    "\n",
    "linear_svc_params = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 5, 10, 20, 100],\n",
    "    'penalty': ['l1'],\n",
    "    'loss': ['squared_hinge'],\n",
    "    'dual': [False],\n",
    "    'class_weight': [None, 'balanced', {0:1, 1: 2}, {0:1, 1: 5}, {0:1, 1: 10}, {0:1, 1: 20}],\n",
    "    'max_iter': [5000, 10000, 20000],\n",
    "    'random_state': [42],\n",
    "    'intercept_scaling': [0.5, 1.0, 2.0, 3.0]\n",
    "}\n",
    "# Support Vector Machine\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'class_weight': ['balanced', {0:1, 1: 5}, {0: 1, 1: 10}]\n",
    "}\n",
    "\n",
    "# Extra Trees\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', 'balanced_subsample',  {0:1, 1:10},   {0:1, 1:20}]\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (64,), (128,), (64, 64), (128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': np.logspace(2, 4, 10),\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 500, 1000],\n",
    "    'early_stopping': [True],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "adaboost_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'estimator': [\n",
    "        DecisionTreeClassifier(max_depth=1), # Decision Stump\n",
    "        DecisionTreeClassifier(max_depth=2), \n",
    "        DecisionTreeClassifier(max_depth=3)\n",
    "    ],\n",
    "    # AdaBoost does not have a native 'scale_pos_weight' parameter. \n",
    "    # Imbalance is typically handled by adjusting the class_weight of the base estimator \n",
    "    # or relying on the boosting mechanism itself.\n",
    "    # The DecisionTreeClassifier base estimator must handle the class_weight.\n",
    "    # Note: GridSearchCV handles the combination.\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [20, 31, 40], # Main complexity parameter for leaf-wise growth\n",
    "    'max_depth': [-1, 5, 8], # -1 means no limit\n",
    "    'min_child_samples': [20, 50, 100],\n",
    "    'subsample': [0.7, 0.9, 1.0], # Row subsampling\n",
    "    'colsample_bytree': [0.7, 0.9, 1.0], # Feature subsampling\n",
    "    'reg_alpha': [0, 0.1, 0.5], # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 0.5], # L2 regularization\n",
    "    # Handling Imbalance\n",
    "    'scale_pos_weight': [1, (y_tr == 0).sum() / (y_tr == 1).sum(), 5, 10, 20]\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23942b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voting_estimator_params = [rf_params, svm_params, lr_param_grid, ]\n",
    "\n",
    "voting_grid = {\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_wrapper(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sampling_method=None,\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    if sampling_method is None:\n",
    "        return x_train, y_train\n",
    "\n",
    "    try:\n",
    "        # Get column names before sampling\n",
    "        feature_columns = x_train.columns\n",
    "        target_name = y_train.name if hasattr(y_train, 'name') else 'target'\n",
    "        \n",
    "        # Apply sampling\n",
    "        sampler = sampling_method\n",
    "        x_tr_res, y_tr_res = sampler.fit_resample(x_train, y_train)\n",
    "        \n",
    "        # Convert back to DataFrame/Series with original column names\n",
    "        x_tr_res = pd.DataFrame(x_tr_res, columns=feature_columns)\n",
    "        y_tr_res = pd.Series(y_tr_res, name=target_name)\n",
    "        \n",
    "        return x_tr_res, y_tr_res\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sampling: {str(e)}\")\n",
    "        return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.logspace(1.5, 2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def sparse_pca_wrapper(x_train, y_train, estimator, param_grid=None,\n",
    "                      n_components=None, min_feats=5, alpha=1, ridge_alpha=0.01, \n",
    "                      max_iter=1000, n_runs=5, cv=5, scoring='accuracy', \n",
    "                      random_state=42, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Sparse PCA wrapper with integrated cross-validation for feature selection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : array-like of shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : array-like of shape (n_samples,)\n",
    "        Target values\n",
    "    estimator : estimator object\n",
    "        A scikit-learn estimator that implements 'fit' and 'predict'\n",
    "    param_grid : dict, optional\n",
    "        Dictionary with parameters names as keys and lists of parameter settings to try\n",
    "    n_components : int, default=None\n",
    "        Number of sparse components to extract\n",
    "    alpha : float, default=1\n",
    "        Sparsity controlling parameter\n",
    "    ridge_alpha : float, default=0.01\n",
    "        Amount of ridge shrinkage\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    n_runs : int, default=5\n",
    "        Number of runs for stability analysis\n",
    "    cv : int, cross-validation generator, default=5\n",
    "        Determines cross-validation splitting strategy\n",
    "    scoring : str, callable, default='accuracy'\n",
    "        Scoring metric\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    n_jobs : int, default=-1\n",
    "        Number of jobs to run in parallel\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict:\n",
    "        Dictionary containing:\n",
    "        - 'best_estimator': Best model from GridSearchCV\n",
    "        - 'best_params': Best parameters from GridSearchCV\n",
    "        - 'cv_results': Cross-validation results\n",
    "        - 'feature_importances': Feature importances if available\n",
    "        - 'stability_scores': Stability of feature selection across runs\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedKFold\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    if param_grid is None:\n",
    "        param_grid = {}  # Default empty param grid\n",
    "    \n",
    "    # Store results from all runs\n",
    "    all_importances = []\n",
    "    best_estimators = []\n",
    "    \n",
    "    # Run multiple times for stability analysis\n",
    "    for run in tqdm(range(n_runs), desc=\"Running stability analysis\"):\n",
    "        # Set random state for this run\n",
    "        current_seed = random_state + run if random_state is not None else None\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        cv_splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=current_seed)\n",
    "        \n",
    "        # Set up GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv_splitter,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        \n",
    "        # Store best estimator\n",
    "        best_estimators.append(grid_search.best_estimator_)\n",
    "        \n",
    "        # Get feature importances if available\n",
    "        if hasattr(grid_search.best_estimator_, 'feature_importances_'):\n",
    "            importances = grid_search.best_estimator_.feature_importances_\n",
    "            all_importances.append(importances)\n",
    "        elif hasattr(grid_search.best_estimator_, 'coef_'):\n",
    "            # For linear models\n",
    "            importances = np.abs(grid_search.best_estimator_.coef_).mean(axis=0)\n",
    "            all_importances.append(importances)\n",
    "    \n",
    "    # Calculate stability scores if we have multiple runs\n",
    "    stability_scores = None\n",
    "    if n_runs > 1 and all_importances:\n",
    "        # Convert to numpy array for calculations\n",
    "        all_importances = np.array(all_importances)\n",
    "        \n",
    "        # Calculate stability as coefficient of variation (lower is more stable)\n",
    "        stability_scores = np.std(all_importances, axis=0) / (np.mean(all_importances, axis=0) + 1e-10)\n",
    "    \n",
    "    # Get the best model from the last run\n",
    "    best_estimator = best_estimators[-1]\n",
    "    # Get feature importances from the best model\n",
    "    feature_importances = None\n",
    "    if hasattr(best_estimator, 'feature_importances_'):\n",
    "        feature_importances = pd.Series(\n",
    "            best_estimator.feature_importances_,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    elif hasattr(best_estimator, 'coef_'):\n",
    "        # For linear models\n",
    "        coef = best_estimator.coef_\n",
    "        if len(coef.shape) > 1:  # For multi-class\n",
    "            coef = np.abs(coef).mean(axis=0)\n",
    "        feature_importances = pd.Series(\n",
    "            coef,\n",
    "            index=x_train.columns if hasattr(x_train, 'columns') else range(x_train.shape[1])\n",
    "        ).sort_values(ascending=False)\n",
    "    \n",
    "\n",
    "    print(feature_importances)\n",
    "    return {\n",
    "        'best_estimator': best_estimator,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_results': grid_search.cv_results_,\n",
    "        'feature_importances': feature_importances,\n",
    "        'selected_features' : feature_importances.nlargest(min_feats).index,\n",
    "        'stability_scores': stability_scores,\n",
    "        'all_importances': all_importances\n",
    "    }\n",
    "\n",
    "def pca_wrapper():\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # The loadings are here!\n",
    "    loadings = pca.components_\n",
    "\n",
    "\n",
    "def lasso_wrapper(x_train, y_train, max_feature = 100,  mlp_setting = False):\n",
    "    # Define the parameter grid for logistic regression\n",
    "    print(\"original column length\", len(x_train.columns))\n",
    "    regularization_grid = None\n",
    "\n",
    "    if not mlp_setting:\n",
    "        regularization_grid = np.logspace(-6, 1, 10)\n",
    "    else:\n",
    "        regularization_grid = np.logspace(1, 3.5, 10)\n",
    "\n",
    "        \n",
    "    param_grid = {\n",
    "        'Cs': regularization_grid,  # Regularization strength,  # Number of regularization values to try\n",
    "        'penalty': 'l1',\n",
    "        'solver': 'saga',\n",
    "        'cv': 5,\n",
    "        'scoring': 'average_precision',\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced',\n",
    "        'max_iter': 10000,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    # Create and fit LogisticRegressionCV\n",
    "    lr_cv = LogisticRegressionCV(**param_grid)\n",
    "    lr_cv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    print(f\"Best C: {lr_cv.C_[0]:.4f}\")\n",
    "    print(f\"Best penalty: {lr_cv.penalty}\")\n",
    "    print(f\"Best cross-validated score: {lr_cv.scores_[1].mean(axis=0).max():.4f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'importance': np.abs(lr_cv.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    selected_top_n_features = feature_importance.nlargest(max_feature, 'importance')[\"feature\"].tolist()\n",
    "\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"best_model\": lr_cv,\n",
    "        \"feature_importance\": feature_importance,\n",
    "        \"selected_features\": selected_top_n_features\n",
    "    }\n",
    "\n",
    "\n",
    "def rfecv_wrapper(estimator, x_train, y_train, max_feats = 50, min_feats = 10):\n",
    "    # Ensure x_train is a DataFrame to access column names\n",
    "    # if not hasattr(x_train, 'columns'):\n",
    "    #     x_train = pd.DataFrame(x_train)\n",
    "    \n",
    "    rfecv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=5,\n",
    "        cv=StratifiedKFold(3),\n",
    "        scoring='average_precision',\n",
    "        n_jobs=-1, \n",
    "        min_features_to_select=min_feats,\n",
    "    )\n",
    "\n",
    "    rfecv.fit(x_train, y_train)\n",
    "    \n",
    "    # Get the selected features by name\n",
    "\n",
    "\n",
    "    \n",
    "    # Get feature rankings with names\n",
    "# First, ensure the features are sorted by importance (best rank first)\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        'feature': x_train.columns,\n",
    "        'ranking': rfecv.ranking_,\n",
    "        'support': rfecv.support_\n",
    "    }).sort_values('ranking')  # Sort by ranking (lower rank = more important)\n",
    "\n",
    "    # Keep only the top max_feats features\n",
    "    feature_ranking['support'] = feature_ranking.index < max_feats\n",
    "\n",
    "    # Update selected_features to only include the top max_feats features\n",
    "    selected_features = feature_ranking[feature_ranking['support']]['feature'].tolist()\n",
    "\n",
    "    # Print some information\n",
    "    print(f\"Selected top {len(selected_features)} features out of {len(feature_ranking)}\")\n",
    "    print(\"Selected features:\", selected_features)\n",
    "            \n",
    "    # Get cross-validation scores for each number of features\n",
    "    cv_scores = pd.DataFrame({\n",
    "        'n_features': range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n",
    "        'mean_score': rfecv.cv_results_['mean_test_score'],\n",
    "        'std_score': rfecv.cv_results_['std_test_score']\n",
    "    })\n",
    "    \n",
    "    # selected_features = [x_train.columns[col] for col in selected_features]\n",
    "\n",
    "    print(\"Selected features:\", selected_features)\n",
    "\n",
    "    return {\n",
    "        'selected_features': list(selected_features),\n",
    "        'feature_ranking': feature_ranking,\n",
    "        'cv_scores': cv_scores,\n",
    "        'optimal_n_features': rfecv.n_features_,\n",
    "        'rfecv': rfecv\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "pipeline_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if pipeline_path not in sys.path:\n",
    "    sys.path.append(pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_metrics(gs_cv_results, n_splits=4, title='Cross-Validation Metrics'):\n",
    "  \n",
    "    # Extract metrics for each fold\n",
    "    metrics = []\n",
    "    for i in range(1, n_splits):\n",
    "        fold_metrics = {\n",
    "            'Fold': i+1,\n",
    "            'Sensitivity': gs_cv_results[f'split{i}_test_sensitivity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Specificity': gs_cv_results[f'split{i}_test_specificity'][gs_cv_results['rank_test_accuracy'].argmin()],\n",
    "            'Accuracy': gs_cv_results[f'split{i}_test_accuracy'][gs_cv_results['rank_test_accuracy'].argmin()]\n",
    "        }\n",
    "        metrics.append(fold_metrics)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df_metrics = pd.DataFrame(metrics).melt('Fold', var_name='Metric', value_name='Score')\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Fold', y='Score', hue='Metric', data=df_metrics)\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print mean and std of metrics\n",
    "    print(f\"Mean Sensitivity: {df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Sensitivity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Specificity: {df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Specificity']['Score'].std():.3f})\")\n",
    "    print(f\"Mean Accuracy: {df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].mean():.3f} \"\n",
    "          f\"({df_metrics[df_metrics['Metric'] == 'Accuracy']['Score'].std():.3f})\")\n",
    "\n",
    "# Example usage:\n",
    "# plot_cv_metrics(gs.cv_results_, n_splits=4, title='Model Performance per Fold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import LinearRegression\n",
    "# from custom_models.svm_shap import SVMSHAP\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, average_precision_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=3,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# scores = cross_validate(\n",
    "#     model,\n",
    "#     X,\n",
    "#     y,\n",
    "#     cv=cv,\n",
    "#     scoring=[\"roc_auc\", \"accuracy\", \"precision\", \"recall\"],\n",
    "#     n_jobs=-1,\n",
    "#     return_estimator=False,\n",
    "#     return_train_score=False\n",
    "# )\n",
    "\n",
    "# print(scores[\"test_roc_auc\"].mean(), scores[\"test_roc_auc\"].std())\n",
    "\n",
    "# XGBoost\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# SVM\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    svm_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "linear_svc_grid = GridSearchCV(\n",
    "    LinearSVC(random_state=42),\n",
    "    linear_svc_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extra Trees\n",
    "et_grid = GridSearchCV(\n",
    "    ExtraTreesClassifier(random_state=42),\n",
    "    et_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  #  Most common choice\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_params,\n",
    "    cv=cv,\n",
    "    refit='average_precision',\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "        # 'f1': 'f1_weighted'  #  Most common choice\n",
    "    },\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "gs = GridSearchCV(\n",
    "  param_grid=lr_param_grid,\n",
    "  cv = cv,\n",
    "  estimator=LogisticRegression(random_state=42),\n",
    "  refit=\"accuracy\",\n",
    "  scoring={\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'average_precision': 'average_precision',\n",
    "    'accuracy': 'accuracy'\n",
    "  },\n",
    ")\n",
    "\n",
    "# catboost_grid = GridSearchCV(\n",
    "#     CatBoostClassifier(random_seed=42, verbose=0), # verbose=0 suppresses training logs\n",
    "#     catboost_params,\n",
    "#     cv=cv,\n",
    "#     scoring='average_precision',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "adaboost_grid = GridSearchCV(\n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),\n",
    "    adaboost_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "lgbm_grid = GridSearchCV(\n",
    "    LGBMClassifier(random_state=42, n_jobs=-1, objective='binary', metric='binary_logloss'),\n",
    "    lgbm_params,\n",
    "    cv=cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "mlp_grid = GridSearchCV(\n",
    "    MLPClassifier(),\n",
    "    mlp_params,\n",
    "    cv=cv,\n",
    "    scoring={\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'average_precision': 'average_precision',\n",
    "        'accuracy': 'accuracy',\n",
    "    },\n",
    "    refit='average_precision',\n",
    "    n_jobs=-1,  # Note: MLP doesn't support n_jobs > 1\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "grids = [\n",
    "    xgb_grid,        # XGBoost GridSearchCV\n",
    "    svm_grid,        # SVM GridSearchCV\n",
    "    linear_svc_grid, # Linear SVC GridSearchCV\n",
    "    et_grid,         # Extra Trees GridSearchCV\n",
    "    rf_grid, \n",
    "    mlp_grid,        # Random Forest GridSearchCV\n",
    "    gs               # Logistic Regression GridSearchCV\n",
    "]\n",
    "\n",
    "\n",
    "#======= FS WRAPPER ========\n",
    "\n",
    "\n",
    "    \n",
    "# current_gs = linear_svc_grid\n",
    "# current_gs.fit(x_tr, y_tr)\n",
    "# # Get the best estimator from GridSearchCV\n",
    "# best_lr = current_gs.best_estimator_\n",
    "\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = best_lr.predict(x_ts[feats])\n",
    "# # Generate and print the classification report\n",
    "# print(\"Best Parameters:\", current_gs.best_params_)\n",
    "# print(\"\\nClassification Report for Best Model:\")\n",
    "# print(classification_report(y_ts, y_pred))\n",
    "# print(confusion_matrix(y_ts,y_pred))\n",
    "# print(\"model accuracy:\", accuracy_score(y_ts, y_pred))\n",
    "\n",
    "# x_ts_filtered = x_ts[feats]\n",
    "\n",
    "# try:\n",
    "#     # Pass the filtered TEST FEATURES (x_ts[feats]) to predict_proba\n",
    "#     y_prob = best_lr.predict_proba(x_ts_filtered)\n",
    "    \n",
    "#     # NOTE: roc_auc_score requires probabilities for the positive class (column 1)\n",
    "#     # The output of predict_proba is usually (N_samples, 2), so we take all rows and column index 1\n",
    "#     model_proba = y_prob[:, 1]\n",
    "    \n",
    "#     print(\"model roc auc\", roc_auc_score(y_ts.to_numpy(), model_proba))\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(\"cannot print proba\")\n",
    "\n",
    "# print(current_gs.cv_results_ )\n",
    "# # plot_cv_metrics(current_gs.cv_results_, n_splits=3, title='Your Model Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b915db",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiomics_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df = pd.read_csv(\"clinical_df_cleaned.csv\")\n",
    "\n",
    "(set(clin_df[\"TCIA_ID\"].unique())).difference(set(radiomics_df_final[\"TCIA_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83528571",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "radiomics_df_final.to_csv(\"radiomics_df_veryfinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all duplicated rows in clin_df\n",
    "duplicated_rows = clin_df[clin_df.duplicated(subset=[\"TCIA_ID\"])]\n",
    "\n",
    "# Sort by all columns to group duplicates together\n",
    "duplicated_rows = duplicated_rows.sort_values(by=duplicated_rows.columns.tolist())\n",
    "\n",
    "# Display the duplicated rows\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680751b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d062eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, roc_auc_score, average_precision_score, accuracy_score, classification_report\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def save_model_results(grid, x_tr, y_tr, x_ts, y_ts, feats, \n",
    "                      feature_selection_name='', filter_algorithm_name='',\n",
    "                      output_file='model_results.xlsx'):\n",
    "    \"\"\"\n",
    "    Save model results including CV metrics (from GridSearchCV) and test set performance.\n",
    "    Expects a GridSearchCV instance. Will fit the grid if not already fitted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure grid is fitted (force surfacing of real errors)\n",
    "        if not hasattr(grid, 'best_estimator_'):\n",
    "            # If possible, flip error_score to raise to see root cause early\n",
    "            try:\n",
    "                grid.error_score = 'raise'\n",
    "            except Exception:\n",
    "                pass\n",
    "            print(\"Fitting GridSearchCV...\")\n",
    "            grid.fit(x_tr[feats], y_tr)\n",
    "\n",
    "        best_estimator = grid.best_estimator_\n",
    "        cv_results = grid.cv_results_\n",
    "        best_params = grid.best_params_\n",
    "        best_idx = getattr(grid, 'best_index_', None)\n",
    "        model_name = type(best_estimator).__name__\n",
    "\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        run_id = f\"{model_name}_{feature_selection_name}_{filter_algorithm_name}_{timestamp}\"\n",
    "\n",
    "        # Extract CV metrics robustly\n",
    "        # Primary CV score comes from grid.best_score_ regardless of scoring name\n",
    "        primary_cv = getattr(grid, 'best_score_', None)\n",
    "\n",
    "        def _get_cv_stat(key_mean, key_std=None):\n",
    "            if key_mean in cv_results:\n",
    "                mean_val = cv_results[key_mean][best_idx] if best_idx is not None else np.nan\n",
    "                if key_std and key_std in cv_results:\n",
    "                    std_val = cv_results[key_std][best_idx]\n",
    "                    return f\"{mean_val:.4f}  {std_val:.4f}\"\n",
    "                return f\"{mean_val:.4f}\"\n",
    "            return 'N/A'\n",
    "\n",
    "        # Try to read explicit metric keys if you used a scoring dict when creating the grid\n",
    "        cv_metrics = {\n",
    "            'cv_primary_metric': f\"{primary_cv:.4f}\" if primary_cv is not None else 'N/A',\n",
    "            'cv_train_accuracy': _get_cv_stat('mean_train_accuracy', 'std_train_accuracy'),\n",
    "            'cv_test_accuracy':  _get_cv_stat('mean_test_accuracy',  'std_test_accuracy'),\n",
    "            'cv_train_roc_auc':  _get_cv_stat('mean_train_roc_auc',  'std_train_roc_auc'),\n",
    "            'cv_test_roc_auc':   _get_cv_stat('mean_test_roc_auc',   'std_test_roc_auc'),\n",
    "            'cv_train_ap':       _get_cv_stat('mean_train_average_precision', 'std_train_average_precision'),\n",
    "            'cv_test_ap':        _get_cv_stat('mean_test_average_precision',  'std_test_average_precision'),\n",
    "            # Fallback if you passed a single scoring like 'average_precision'\n",
    "            'cv_test_score':     _get_cv_stat('mean_test_score', 'std_test_score')\n",
    "        }\n",
    "\n",
    "        # Test set predictions\n",
    "        y_pred = best_estimator.predict(x_ts[feats])\n",
    "        y_prob = best_estimator.predict_proba(x_ts[feats])[:, 1] if hasattr(best_estimator, 'predict_proba') else None\n",
    "\n",
    "        test_metrics = {\n",
    "            'test_accuracy': accuracy_score(y_ts, y_pred),\n",
    "            'test_roc_auc': roc_auc_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "            'test_average_precision': average_precision_score(y_ts, y_prob) if y_prob is not None else None,\n",
    "        }\n",
    "\n",
    "        clf_report = classification_report(y_ts, y_pred, output_dict=True)\n",
    "\n",
    "        metrics_data = {\n",
    "            'Run_ID': run_id,\n",
    "            'Model': model_name,\n",
    "            'Feature_Selection': feature_selection_name,\n",
    "            'Filter_Algorithm': filter_algorithm_name,\n",
    "            'Num_Features': len(feats),\n",
    "            'Timestamp': timestamp,\n",
    "            'CV_Strategy': 'GridSearchCV',\n",
    "            **cv_metrics,\n",
    "            **{k: v for k, v in test_metrics.items()},\n",
    "            'Features_Used': str(feats),\n",
    "            'Model_Params': str(best_params)\n",
    "        }\n",
    "        metrics_df = pd.DataFrame([metrics_data])\n",
    "\n",
    "        clf_df = pd.DataFrame(clf_report).transpose()\n",
    "        clf_df['Run_ID'] = run_id\n",
    "        clf_df = clf_df.reset_index().rename(columns={'index': 'class'})\n",
    "\n",
    "        # Save to Excel\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                try:\n",
    "                    existing_metrics = pd.read_excel(writer, sheet_name='Metrics')\n",
    "                    metrics_df = pd.concat([existing_metrics, metrics_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "                try:\n",
    "                    existing_clf = pd.read_excel(writer, sheet_name='Classification_Reports')\n",
    "                    clf_df = pd.concat([existing_clf, clf_df], ignore_index=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "        except Exception:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "                clf_df.to_excel(writer, sheet_name='Classification_Reports', index=False)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return metrics_df, clf_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, run the lasso_wrapper to get the selected features\n",
    "from custom_models.svm_shap import SVMSHAP\n",
    "\n",
    "lasso_result = lasso_wrapper(\n",
    "    \n",
    "    x_train=x_tr,\n",
    "    y_train=y_tr,\n",
    "    max_feature=200,\n",
    "    mlp_setting=True\n",
    ")\n",
    "\n",
    "# Get the selected features from Lasso\n",
    "selected_features_lasso = lasso_result[\"selected_features\"]\n",
    "\n",
    "# Use the selected features from Lasso as input to RFECV\n",
    "rfecv_result = rfecv_wrapper(\n",
    "    estimator=AdaBoostClassifier(random_state=42),  # or your preferred estimator\n",
    "    x_train=x_tr[selected_features_lasso],  # Use only Lasso-selected features\n",
    "    y_train=y_tr,\n",
    "    max_feats=20\n",
    ")\n",
    "\n",
    "# # Get the final selected features from RFECV\n",
    "selected_features_rfecv = rfecv_result[\"selected_features\"]\n",
    "\n",
    "print(\"Lasso selected features:\", len(selected_features_lasso))\n",
    "# print(\"RFECV selected features:\", selected_features_rfecv)\n",
    "# # Get the selected features\n",
    "# selected_features = result[\"selected_features\"]\n",
    "\n",
    "metrics_df, clf_report = save_model_results(\n",
    "    grid=adaboost_grid,  # Pass the fitted GridSearchCV instance\n",
    "    x_tr=x_tr,\n",
    "    y_tr=y_tr,\n",
    "    x_ts=x_ts,\n",
    "    y_ts=y_ts,\n",
    "    feats=selected_features_rfecv,\n",
    "    feature_selection_name='Lasso',  # or your feature selection method\n",
    "    filter_algorithm_name='MannWhitney',  # or your filter method\n",
    "    output_file='model_results.xlsx'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -----------------------------\n",
    "# Replace these with your data\n",
    "# X = your feature matrix\n",
    "# y = your label vector\n",
    "# -----------------------------\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# ---- Real score ----\n",
    "real_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_tr,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Real AUC:\", real_scores.mean(), \"\", real_scores.std())\n",
    "\n",
    "# ---- Permuted score ----\n",
    "y_perm = np.random.permutation(y_tr)\n",
    "\n",
    "perm_scores = cross_val_score(\n",
    "    clf,\n",
    "    x_tr,\n",
    "    y_perm,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Permuted AUC:\", perm_scores.mean(), \"\", perm_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b559dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12e5dd",
   "metadata": {},
   "source": [
    "Model Tuning(?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b975781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the Variance Inflation Factor (VIF) for all numeric columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    # Use only numeric, non-null columns\n",
    "    X = df.select_dtypes(include=np.number).dropna(axis=1, how='any')\n",
    "\n",
    "    if X.empty:\n",
    "        return pd.DataFrame({'Feature': [], 'VIF': []})\n",
    "\n",
    "    # Add constant for intercept term\n",
    "    X_vif = sm.add_constant(X)\n",
    "    \n",
    "    features = X.columns\n",
    "    vif_list = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        # Calculate VIF. Index i+1 accounts for the added 'const' column at index 0.\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X_vif.values, i + 1)\n",
    "            vif_list.append(vif)\n",
    "        except Exception:\n",
    "            vif_list.append(np.nan) # Set to NaN if calculation fails (e.g., perfect collinearity)\n",
    "\n",
    "    vif_df = pd.DataFrame({'Feature': features, 'VIF': vif_list})\n",
    "    return vif_df.sort_values(by='VIF', ascending=False).reset_index(drop=True)\n",
    "\n",
    "vif_df = calculate_vif(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cfcf3",
   "metadata": {},
   "source": [
    "RFE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin-bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
